{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53a4f92e-e4a5-43ee-a9c5-2d9adce436e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-04 15:56:46.313623: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-04 15:56:46.423613: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-04 15:56:46.452378: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-04 15:56:46.978382: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-04 15:56:46.978465: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-04 15:56:46.978475: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os       # using operating system dependent functionality (folders)\n",
    "import pandas as pd # data analysis and manipulation\n",
    "import numpy as np    # numerical computing (manipulating and performing operations on arrays of data)\n",
    "import copy     # Can Copy and Deepcopy files so original file is untouched.\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../eegyolk') # path to helper functions\n",
    "import helper_functions as hf # library useful for eeg and erp data cleaning\n",
    "import epod_helper\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Reshape\n",
    "from tensorflow.keras.layers import Conv1D,BatchNormalization,LeakyReLU,MaxPool1D,\\\n",
    "GlobalAveragePooling1D,Dense,Dropout,AveragePooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ca9d4b3-1280-46a2-8485-194fd53ecca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a309da3ca847518114e3040b080f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Pick sensors', options=('df_mmr_ch_complete.csv', 'df_mmr_ch_literature.csv', 'df_mmâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv = widgets.RadioButtons(\n",
    "    options=['df_mmr_ch_complete.csv', 'df_mmr_ch_literature.csv', 'df_mmr_ch_ttest.csv', 'df_mmr_ch_connectivity.csv'],\n",
    "    description='Pick sensors',\n",
    "    disabled=False\n",
    ")\n",
    "display(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01fe422a-3064-4d02-b413-d0a62f7ec71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df_mmr_ch_connectivity.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv = str(csv.value)\n",
    "csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1865e6cd-a97e-45e1-89e6-8667c7239e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "213113e7-f789-4d5d-a431-c71e0821467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if csv=='df_mmr_ch_ttest.csv' or csv=='df_mmr_ch_connectivity.csv':\n",
    "    df = df[30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f30bd362-7010-4930-8f2c-2e27f76560cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df.pop('Group_AccToParents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ad6caef-fc7e-4fb4-ba89-7d12649c356d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-04 15:56:53.214494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 15:56:53.244989: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-11-04 15:56:53.245011: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-11-04 15:56:53.245955: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(71, 12), dtype=float64, numpy=\n",
       "array([[ 2.46140191e-06,  3.07044957e-06,  2.90702506e-06,\n",
       "        -1.13344019e-03, -5.09929920e-03, -3.77269113e-03,\n",
       "        -5.75850338e-06, -1.04288893e-05, -1.06293191e-05,\n",
       "         5.34943940e-06,  2.52098560e-06,  2.66417785e-06],\n",
       "       [ 3.71632675e-06,  3.77844905e-06,  2.51025444e-06,\n",
       "        -4.08954197e-03, -3.09213806e-03, -2.99302833e-03,\n",
       "        -1.33109771e-05, -1.18979795e-05, -6.80227552e-06,\n",
       "         3.96621936e-06,  6.03767186e-06,  4.07571364e-06],\n",
       "       [ 1.75308135e-06,  3.07163941e-06,  3.14130774e-06,\n",
       "         1.82362908e-03,  7.68792356e-03, -2.84698162e-03,\n",
       "        -3.89510410e-06, -2.57922395e-06, -1.46261913e-05,\n",
       "         4.78485637e-06,  1.04068740e-05,  4.89379033e-06],\n",
       "       [ 1.93972498e-06,  1.55490124e-06,  2.42582514e-06,\n",
       "         3.21763191e-03,  4.57422231e-04,  3.57127658e-03,\n",
       "        -4.01728259e-06, -3.84789389e-06, -3.50903282e-06,\n",
       "         5.55127421e-06,  3.65452297e-06,  6.59219499e-06],\n",
       "       [ 2.56565195e-06,  2.28107486e-06,  1.96772886e-06,\n",
       "        -3.45003487e-03, -1.07489027e-03, -4.01162988e-03,\n",
       "        -6.39258979e-06, -5.71828972e-06, -6.70878680e-06,\n",
       "         3.85691656e-06,  4.13208541e-06,  2.39601849e-06],\n",
       "       [ 2.98682332e-06,  3.04383786e-06,  2.31462430e-06,\n",
       "         1.08027926e-04,  2.91646615e-03,  5.41501853e-03,\n",
       "        -6.71753306e-06, -5.69087425e-06, -1.90512625e-06,\n",
       "         7.31799151e-06,  8.72716024e-06,  7.48204717e-06],\n",
       "       [ 3.71044004e-06,  3.04136072e-06,  1.86791252e-06,\n",
       "        -9.97746671e-03, -7.24125631e-03, -3.38475873e-03,\n",
       "        -1.38296027e-05, -9.93322483e-06, -5.56131497e-06,\n",
       "         5.89408363e-06,  6.03042470e-06,  2.45094974e-06],\n",
       "       [ 4.17323490e-06,  3.76524112e-06,  2.17912707e-06,\n",
       "         7.98507354e-03,  9.27198888e-03,  6.89992127e-04,\n",
       "        -3.46828737e-06, -2.18199705e-06, -4.00553615e-06,\n",
       "         1.44405772e-05,  1.18420483e-05,  5.72968593e-06],\n",
       "       [ 2.27678441e-06,  2.61210183e-06,  2.63537327e-06,\n",
       "        -3.84493072e-04, -4.05698863e-04,  4.21173318e-03,\n",
       "        -7.02016568e-06, -8.47473199e-06, -4.77246976e-06,\n",
       "         6.27570002e-06,  4.84778755e-06,  8.48452493e-06],\n",
       "       [ 5.90272434e-06,  5.14368349e-06,  3.73692212e-06,\n",
       "         1.23205813e-02,  9.92995980e-03,  6.80729807e-03,\n",
       "        -3.21388753e-06, -3.54399100e-06, -1.84396642e-06,\n",
       "         1.72889818e-05,  1.51811094e-05,  1.08236344e-05],\n",
       "       [ 2.82367416e-06,  5.81841418e-06,  2.50847420e-06,\n",
       "        -2.15690834e-03, -1.14236401e-02, -1.46631054e-03,\n",
       "        -8.86036095e-06, -1.68360531e-05, -5.45931987e-06,\n",
       "         5.99542591e-06,  3.52772702e-06,  3.68595385e-06],\n",
       "       [ 3.63226592e-06,  3.40605717e-06,  3.20463749e-06,\n",
       "        -8.32300374e-03, -6.22744612e-03, -5.92176457e-03,\n",
       "        -1.30417807e-05, -1.12042211e-05, -1.07012152e-05,\n",
       "         2.87104272e-06,  3.16881416e-06,  5.28056780e-06],\n",
       "       [ 9.31402989e-06,  8.57650155e-06,  3.52030475e-06,\n",
       "         1.02948700e-02,  2.81866096e-03,  1.33252805e-03,\n",
       "        -7.12244146e-06, -9.67617203e-06, -4.56008528e-06,\n",
       "         2.34939351e-05,  2.06554800e-05,  9.20707092e-06],\n",
       "       [ 1.42358699e-06,  1.28200023e-06,  1.18394526e-06,\n",
       "        -7.35747743e-04,  8.78943048e-04,  2.20621210e-03,\n",
       "        -4.39803854e-06, -3.28307043e-06, -1.81202333e-06,\n",
       "         2.35505461e-06,  3.01162952e-06,  3.96797560e-06],\n",
       "       [ 2.02100008e-06,  2.29478553e-06,  2.10329493e-06,\n",
       "        -2.00012700e-03, -3.33674973e-03, -4.93145290e-03,\n",
       "        -5.31336478e-06, -6.36109673e-06, -6.49026950e-06,\n",
       "         3.70423786e-06,  3.53509699e-06,  1.53708007e-06],\n",
       "       [ 4.00089817e-06,  4.13137952e-06,  2.28643586e-06,\n",
       "         5.20194499e-03,  5.98189100e-03,  3.22444569e-03,\n",
       "        -4.08347811e-06, -3.91614764e-06, -3.06624280e-06,\n",
       "         1.16424664e-05,  1.15645093e-05,  6.69371806e-06],\n",
       "       [ 1.75710015e-06,  1.97334933e-06,  2.00740531e-06,\n",
       "        -3.50747958e-03, -4.59608304e-03, -5.64987996e-03,\n",
       "        -5.58960889e-06, -6.04230849e-06, -6.35989417e-06,\n",
       "         2.85435365e-06,  2.04283389e-06,  1.93224946e-06],\n",
       "       [ 2.44771861e-06,  2.37655299e-06,  2.55053571e-06,\n",
       "         7.16089839e-03,  3.73736142e-03,  6.18828669e-03,\n",
       "        -2.91561245e-06, -3.81283269e-06, -2.09563731e-06,\n",
       "         7.64946647e-06,  6.61573969e-06,  9.03442228e-06],\n",
       "       [ 2.25828637e-06,  1.70863159e-06,  1.36148996e-06,\n",
       "         1.84076183e-03,  9.38085554e-04, -3.00382114e-03,\n",
       "        -2.79712448e-06, -2.25706642e-06, -4.53843316e-06,\n",
       "         6.58147821e-06,  5.63076162e-06,  2.52889360e-06],\n",
       "       [ 3.46342115e-06,  1.60788761e-06,  2.15190292e-06,\n",
       "        -6.27286747e-03,  1.66533842e-03,  3.30826698e-03,\n",
       "        -1.07028727e-05, -3.45544496e-06, -1.82280335e-06,\n",
       "         1.58875303e-06,  4.71540094e-06,  7.44619647e-06],\n",
       "       [ 2.81630316e-06,  4.06095851e-06,  2.61482993e-06,\n",
       "        -6.84828649e-04,  1.27872564e-02, -3.56898929e-03,\n",
       "        -7.29708316e-06, -8.26739440e-06, -6.60549250e-06,\n",
       "         4.69275847e-06,  1.34005499e-05,  3.25283792e-06],\n",
       "       [ 3.02686876e-06,  2.43966272e-06,  2.58311494e-06,\n",
       "        -6.49313015e-03, -5.08078747e-03, -2.61257813e-03,\n",
       "        -9.09222901e-06, -7.90051735e-06, -6.89592199e-06,\n",
       "         2.86100710e-06,  2.90832848e-06,  2.95802638e-06],\n",
       "       [ 6.37322842e-06,  5.51353447e-06,  2.57937639e-06,\n",
       "         1.99015048e-02,  1.36785918e-02,  6.50002135e-03,\n",
       "        -7.65559835e-06, -8.99113818e-06, -3.99248546e-06,\n",
       "         2.07055183e-05,  1.46780795e-05,  7.48820634e-06],\n",
       "       [ 2.44062488e-06,  3.83424027e-06,  2.35339101e-06,\n",
       "        -5.54774038e-03, -1.19004272e-02, -6.10037194e-03,\n",
       "        -8.44961514e-06, -1.12331662e-05, -9.81656424e-06,\n",
       "         3.22451671e-06,  2.56854558e-06,  2.65439329e-06],\n",
       "       [ 1.63792341e-06,  1.31670978e-06,  2.12587004e-06,\n",
       "        -2.97896432e-03, -6.55671441e-05, -4.82221758e-03,\n",
       "        -5.61298773e-06, -3.33184803e-06, -7.03611324e-06,\n",
       "         2.81519767e-06,  3.63352362e-06,  1.76785343e-06],\n",
       "       [ 3.50584494e-06,  2.78274269e-06,  2.10250749e-06,\n",
       "         9.87785379e-03,  4.55421009e-03,  5.82146419e-03,\n",
       "        -3.00719767e-06, -4.01822191e-06, -1.52105470e-06,\n",
       "         1.24304123e-05,  9.93603823e-06,  7.51901149e-06],\n",
       "       [ 4.11225294e-06,  4.19857880e-06,  3.50090482e-06,\n",
       "        -3.41740908e-03, -4.99956165e-03, -2.97786777e-03,\n",
       "        -8.07910390e-06, -1.05032926e-05, -8.76416476e-06,\n",
       "         7.04271350e-06,  5.91712307e-06,  5.68036339e-06],\n",
       "       [ 2.45228349e-06,  2.84777203e-06,  3.08803362e-06,\n",
       "         2.64342857e-03,  2.23767852e-03,  1.14410308e-02,\n",
       "        -4.84721195e-06, -5.99346679e-06, -1.72806355e-06,\n",
       "         6.97642970e-06,  7.65579569e-06,  9.25103489e-06],\n",
       "       [ 2.16342758e-06,  1.98681334e-06,  2.66038491e-06,\n",
       "        -7.58689992e-04, -2.65338938e-03, -3.52779816e-03,\n",
       "        -5.12023652e-06, -5.75447970e-06, -8.81074970e-06,\n",
       "         3.99889245e-06,  3.84378859e-06,  3.12082449e-06],\n",
       "       [ 1.89709321e-06,  1.77351015e-06,  1.46651297e-06,\n",
       "        -2.65478695e-03, -3.38002575e-03, -7.66323076e-04,\n",
       "        -6.00338515e-06, -5.94876693e-06, -3.24326550e-06,\n",
       "         2.74952215e-06,  2.46948535e-06,  2.93081008e-06],\n",
       "       [ 4.28820580e-06,  4.47990466e-06,  2.53990010e-06,\n",
       "        -4.23608332e-03, -4.97165334e-03, -4.51528894e-03,\n",
       "        -1.00043910e-05, -1.07713461e-05, -6.94579206e-06,\n",
       "         1.08451747e-05,  1.05054660e-05,  2.47988290e-06],\n",
       "       [ 2.86200562e-06,  2.91152086e-06,  1.06481036e-06,\n",
       "         1.47693346e-03,  3.38618264e-03, -1.68641988e-04,\n",
       "        -9.95675547e-06, -7.53889460e-06, -3.59177127e-06,\n",
       "         4.71593528e-06,  6.55552248e-06,  2.29664215e-06],\n",
       "       [ 8.21884140e-06,  5.71664442e-06,  3.84815372e-06,\n",
       "         2.41320250e-02,  1.12957060e-02,  1.14611018e-02,\n",
       "        -3.33384106e-06, -4.71469544e-06, -2.56636884e-06,\n",
       "         2.67440520e-05,  1.85224042e-05,  1.30370523e-05],\n",
       "       [ 3.06678940e-06,  2.90306452e-06,  2.26053610e-06,\n",
       "         9.49227566e-03,  2.52972658e-03,  2.18316965e-03,\n",
       "        -2.41522399e-06, -7.50180387e-06, -4.11143367e-06,\n",
       "         9.93233547e-06,  7.21160841e-06,  6.00736380e-06],\n",
       "       [ 2.69799188e-06,  3.56426260e-06,  2.66696367e-06,\n",
       "        -1.50562340e-04, -2.66668944e-03, -3.44939352e-03,\n",
       "        -6.72246368e-06, -9.59438402e-06, -7.35521363e-06,\n",
       "         5.01579400e-06,  5.81576114e-06,  3.66707343e-06],\n",
       "       [ 2.41953099e-06,  2.37359409e-06,  1.31331071e-06,\n",
       "         1.51817972e-03,  3.21792620e-03, -1.28455137e-03,\n",
       "        -5.53228371e-06, -3.79818966e-06, -3.44973149e-06,\n",
       "         6.12954839e-06,  6.61553384e-06,  2.81564410e-06],\n",
       "       [ 2.52058543e-06,  2.61354742e-06,  1.73794378e-06,\n",
       "         6.48197788e-03,  6.13099691e-03, -1.51263465e-03,\n",
       "        -1.25512231e-06, -2.78005839e-06, -4.19349779e-06,\n",
       "         8.55637605e-06,  7.80917097e-06,  3.02116498e-06],\n",
       "       [ 1.75084022e-06,  2.32556788e-06,  1.27910948e-06,\n",
       "        -2.97287877e-03, -5.56649032e-03, -3.24474694e-04,\n",
       "        -5.94629270e-06, -8.83909997e-06, -4.03558294e-06,\n",
       "         1.76589919e-06,  3.21188508e-06,  2.16118981e-06],\n",
       "       [ 6.89431265e-06,  6.77096923e-06,  6.01859189e-06,\n",
       "        -1.21464213e-02, -1.44406824e-02, -1.68664406e-02,\n",
       "        -2.34810496e-05, -2.27236656e-05, -1.92368782e-05,\n",
       "         4.83120593e-06,  4.44626325e-06,  6.18864080e-06],\n",
       "       [ 4.56251130e-06,  3.12111013e-06,  1.31412214e-06,\n",
       "        -1.22760516e-02, -5.92418510e-03,  1.24472705e-03,\n",
       "        -1.47761519e-05, -1.04397091e-05, -2.25150542e-06,\n",
       "         3.15826971e-06,  1.57167664e-06,  3.12794076e-06],\n",
       "       [ 3.11504079e-06,  2.70156953e-06,  4.31542379e-06,\n",
       "        -5.12972480e-04, -1.92618454e-03,  6.14261789e-03,\n",
       "        -5.58488441e-06, -6.16015521e-06, -3.75334364e-06,\n",
       "         6.87572923e-06,  4.11414489e-06,  1.11654297e-05],\n",
       "       [ 2.21126200e-06,  2.25691886e-06,  1.78166444e-06,\n",
       "         1.19438227e-05,  6.04428309e-04,  3.07840239e-03,\n",
       "        -6.22193700e-06, -5.56595721e-06, -2.45580453e-06,\n",
       "         4.33622851e-06,  4.83559382e-06,  6.36427426e-06],\n",
       "       [ 3.95369678e-06,  4.62681510e-06,  3.62704330e-06,\n",
       "         8.06368734e-03,  1.21466864e-02,  3.19915672e-03,\n",
       "        -5.66986322e-06, -4.60800865e-06, -5.14021825e-06,\n",
       "         1.32015379e-05,  1.54339837e-05,  1.35319752e-05],\n",
       "       [ 4.82259266e-06,  5.25582215e-06,  3.15423221e-06,\n",
       "         1.06614152e-02,  1.47478794e-02,  2.55715998e-04,\n",
       "        -7.25719873e-06, -5.65028343e-06, -7.78943297e-06,\n",
       "         1.47491844e-05,  1.71525715e-05,  4.92095675e-06],\n",
       "       [ 4.94940191e-06,  3.97356467e-06,  3.83145194e-06,\n",
       "        -1.42603085e-02, -1.03168009e-02, -1.14190320e-02,\n",
       "        -1.85129260e-05, -1.39164607e-05, -1.34696062e-05,\n",
       "         7.10502386e-06,  5.63918066e-06,  4.76338944e-06],\n",
       "       [ 2.73527892e-06,  2.33054422e-06,  1.55261043e-06,\n",
       "        -1.93568279e-03, -1.32154715e-03,  2.00100224e-03,\n",
       "        -8.23570877e-06, -8.06184797e-06, -2.56606826e-06,\n",
       "         6.07217751e-06,  4.16132399e-06,  5.50819603e-06],\n",
       "       [ 3.68593139e-06,  4.13155367e-06,  2.91509778e-06,\n",
       "         3.54612526e-03, -2.24295276e-03, -6.28750547e-03,\n",
       "        -5.28282491e-06, -8.81565195e-06, -1.06398266e-05,\n",
       "         1.04309035e-05,  6.89497513e-06,  3.21235475e-06],\n",
       "       [ 2.84933118e-06,  2.78988973e-06,  2.15277403e-06,\n",
       "         2.27855951e-03,  5.45960292e-03, -4.89114792e-03,\n",
       "        -4.34139847e-06, -3.14352789e-06, -7.43417243e-06,\n",
       "         9.22285339e-06,  9.43013350e-06,  3.66279749e-06],\n",
       "       [ 3.00260313e-06,  2.90771468e-06,  2.39207387e-06,\n",
       "         4.12671468e-03,  1.76816551e-03,  4.30779410e-03,\n",
       "        -3.27082698e-06, -5.36140182e-06, -2.19755072e-06,\n",
       "         8.49942730e-06,  7.05245256e-06,  6.83119770e-06],\n",
       "       [ 3.38137557e-06,  2.02260627e-06,  2.89984060e-06,\n",
       "         7.25291675e-03,  2.00990199e-03,  7.37389408e-04,\n",
       "        -1.06141875e-05, -7.68714907e-06, -6.95347407e-06,\n",
       "         1.29601602e-05,  8.24025871e-06,  5.98373713e-06],\n",
       "       [ 2.79124373e-06,  2.77432756e-06,  2.53700608e-06,\n",
       "         1.43941074e-03, -4.67800567e-03, -3.68269010e-03,\n",
       "        -5.55539810e-06, -9.55693875e-06, -6.74374332e-06,\n",
       "         6.36275233e-06,  2.13785240e-06,  2.16154890e-06],\n",
       "       [ 6.58302781e-06,  4.52160728e-06,  3.15473840e-06,\n",
       "         1.79619043e-02,  1.14102133e-02,  5.17023452e-03,\n",
       "        -4.11339544e-06, -3.99885179e-06, -3.22963871e-06,\n",
       "         2.52661364e-05,  1.50730562e-05,  1.03877296e-05],\n",
       "       [ 6.22826256e-06,  6.54385408e-06,  4.80124973e-06,\n",
       "         1.04494945e-02,  1.29467188e-02,  4.03963710e-03,\n",
       "        -7.34546278e-06, -6.92350372e-06, -8.64955391e-06,\n",
       "         1.71743543e-05,  1.62826740e-05,  1.25734036e-05],\n",
       "       [ 1.89352090e-06,  2.41146007e-06,  1.46282381e-06,\n",
       "         1.15128418e-03,  6.21132570e-03,  9.01999346e-04,\n",
       "        -4.46298061e-06, -3.07736778e-06, -2.98670504e-06,\n",
       "         5.66226594e-06,  7.98894511e-06,  4.11891340e-06],\n",
       "       [ 2.73807779e-06,  2.64038508e-06,  2.43772374e-06,\n",
       "        -1.45117316e-03, -3.73757313e-03,  1.73847086e-03,\n",
       "        -8.10958011e-06, -8.16310412e-06, -3.52605591e-06,\n",
       "         5.68860270e-06,  5.24292857e-06,  6.49756962e-06],\n",
       "       [ 1.76468583e-06,  2.04757494e-06,  1.64991531e-06,\n",
       "        -1.95379374e-03, -9.21408879e-04,  2.40973541e-03,\n",
       "        -7.31122630e-06, -7.35685757e-06, -2.20440881e-06,\n",
       "         3.45329069e-06,  4.57247510e-06,  4.79119711e-06],\n",
       "       [ 3.76879937e-06,  3.79271779e-06,  2.84949883e-06,\n",
       "        -1.11527282e-02, -1.22627701e-02, -8.50371364e-03,\n",
       "        -1.35783234e-05, -1.25913618e-05, -9.12849212e-06,\n",
       "         4.04833185e-06,  3.75967960e-06,  3.44601504e-06],\n",
       "       [ 3.52653010e-06,  4.06015970e-06,  2.69539754e-06,\n",
       "         3.98120621e-03,  7.70115206e-03,  6.41041845e-03,\n",
       "        -7.66688878e-06, -4.71449058e-06, -3.10720874e-06,\n",
       "         7.29786797e-06,  9.79319169e-06,  7.61636485e-06],\n",
       "       [ 3.17982796e-06,  3.34265044e-06,  3.37051215e-06,\n",
       "         3.07922077e-03,  6.65661963e-04, -8.06746146e-03,\n",
       "        -5.78892763e-06, -9.12726489e-06, -1.12175774e-05,\n",
       "         7.56908193e-06,  7.36917349e-06,  3.36125600e-06],\n",
       "       [ 6.92729985e-06,  5.54200964e-06,  8.61264203e-06,\n",
       "        -1.75838935e-02, -8.74967059e-03, -2.28615098e-02,\n",
       "        -2.25959963e-05, -1.56551783e-05, -2.68039053e-05,\n",
       "         1.27003821e-05,  6.69896505e-06,  3.87782560e-06],\n",
       "       [ 3.11117165e-06,  2.96209442e-06,  2.57987226e-06,\n",
       "         4.99119843e-03,  1.54519701e-03, -1.66159037e-03,\n",
       "        -3.45123782e-06, -5.27275754e-06, -5.84110807e-06,\n",
       "         9.57615324e-06,  7.68117248e-06,  4.57930650e-06],\n",
       "       [ 1.69686069e-06,  1.89106801e-06,  2.06506052e-06,\n",
       "        -1.86868335e-03, -1.30707639e-03, -5.21914411e-03,\n",
       "        -5.15546914e-06, -5.10966730e-06, -7.79645810e-06,\n",
       "         2.43898682e-06,  3.79006091e-06,  1.63169821e-06],\n",
       "       [ 2.69611538e-06,  3.22155647e-06,  3.29737911e-06,\n",
       "        -5.95980419e-03, -6.03481361e-03, -6.66232857e-03,\n",
       "        -8.60817694e-06, -1.07046159e-05, -9.86124712e-06,\n",
       "         3.54613837e-06,  3.64928347e-06,  2.92705004e-06],\n",
       "       [ 2.45117534e-06,  2.72871317e-06,  1.41764509e-06,\n",
       "        -2.55553165e-03, -6.38659527e-03, -7.37089516e-04,\n",
       "        -6.46320177e-06, -8.22692806e-06, -4.03491144e-06,\n",
       "         3.47202435e-06,  3.14448716e-06,  2.92993896e-06],\n",
       "       [ 7.35089466e-06,  4.30238396e-06,  2.28749057e-06,\n",
       "        -2.30945928e-02, -1.40277157e-02, -7.05475506e-04,\n",
       "        -2.43876406e-05, -1.22641220e-05, -4.95813674e-06,\n",
       "         6.53084010e-06,  5.54592121e-06,  4.42991111e-06],\n",
       "       [ 3.27872333e-06,  2.40411179e-06,  2.43712867e-06,\n",
       "         5.07471191e-03,  1.45595785e-03,  4.25259233e-03,\n",
       "        -5.22424680e-06, -5.62371523e-06, -3.66649979e-06,\n",
       "         9.44505594e-06,  5.49042912e-06,  7.43260677e-06],\n",
       "       [ 2.37109531e-06,  2.98133087e-06,  1.27136816e-06,\n",
       "        -2.40260727e-04, -2.84810451e-03, -1.56753499e-03,\n",
       "        -6.73819025e-06, -8.13817298e-06, -4.36334235e-06,\n",
       "         6.05615725e-06,  5.05074465e-06,  2.40345855e-06],\n",
       "       [ 2.21590090e-06,  3.30893700e-06,  3.06871903e-06,\n",
       "         2.68881688e-04,  7.65585919e-03,  6.86633034e-03,\n",
       "        -4.21467404e-06, -3.43797350e-06, -3.37718505e-06,\n",
       "         4.60500449e-06,  1.04381056e-05,  9.01287192e-06],\n",
       "       [ 2.43393750e-06,  2.79766512e-06,  3.02933063e-06,\n",
       "         9.96822049e-04, -5.31966378e-03, -8.79640491e-03,\n",
       "        -4.99120657e-06, -8.61590970e-06, -1.06774725e-05,\n",
       "         5.09301587e-06,  3.75576977e-06,  3.93444110e-06],\n",
       "       [ 3.21239386e-06,  3.46795334e-06,  2.31773430e-06,\n",
       "         6.32977399e-04,  7.50085909e-03,  3.20454087e-03,\n",
       "        -7.14435584e-06, -3.11962584e-06, -3.97548203e-06,\n",
       "         8.24457724e-06,  1.08345586e-05,  5.48210288e-06],\n",
       "       [ 2.72735582e-06,  1.92208455e-06,  2.07223419e-06,\n",
       "         8.85435379e-03,  6.02498870e-03,  5.27355588e-03,\n",
       "        -2.39902195e-06, -2.10216882e-06, -1.87639746e-06,\n",
       "         8.44689296e-06,  6.34124514e-06,  5.80652421e-06]])>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.convert_to_tensor(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a72edc3-706d-46f9-af99-c6ee1e4ce0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dd16176-91f5-4fe1-9231-ad0680f5fa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, target,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82ce2668-21ea-4c29-b40b-62064f3faa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        normalizer,\n",
    "        tf.keras.layers.Dense(10, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "      ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d735e4f-528c-4b38-8aac-ddcd396a1ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.7900 - accuracy: 0.4464 - val_loss: 0.8264 - val_accuracy: 0.3333\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.7542 - accuracy: 0.5357 - val_loss: 0.8045 - val_accuracy: 0.4667\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.7298 - accuracy: 0.5536 - val_loss: 0.7872 - val_accuracy: 0.6667\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.7111 - accuracy: 0.5357 - val_loss: 0.7771 - val_accuracy: 0.7333\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6989 - accuracy: 0.5893 - val_loss: 0.7698 - val_accuracy: 0.7333\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6860 - accuracy: 0.6071 - val_loss: 0.7601 - val_accuracy: 0.7333\n",
      "Epoch 7/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6776 - accuracy: 0.5893 - val_loss: 0.7509 - val_accuracy: 0.6667\n",
      "Epoch 8/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6684 - accuracy: 0.5893 - val_loss: 0.7428 - val_accuracy: 0.6667\n",
      "Epoch 9/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6606 - accuracy: 0.6071 - val_loss: 0.7378 - val_accuracy: 0.6667\n",
      "Epoch 10/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6561 - accuracy: 0.5893 - val_loss: 0.7325 - val_accuracy: 0.6667\n",
      "Epoch 11/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6499 - accuracy: 0.5893 - val_loss: 0.7266 - val_accuracy: 0.6667\n",
      "Epoch 12/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6453 - accuracy: 0.5893 - val_loss: 0.7203 - val_accuracy: 0.6667\n",
      "Epoch 13/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6404 - accuracy: 0.6071 - val_loss: 0.7260 - val_accuracy: 0.6667\n",
      "Epoch 14/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6342 - accuracy: 0.6071 - val_loss: 0.7163 - val_accuracy: 0.6667\n",
      "Epoch 15/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6285 - accuracy: 0.6071 - val_loss: 0.7116 - val_accuracy: 0.6667\n",
      "Epoch 16/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6246 - accuracy: 0.6071 - val_loss: 0.7041 - val_accuracy: 0.6667\n",
      "Epoch 17/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6198 - accuracy: 0.6071 - val_loss: 0.7073 - val_accuracy: 0.6667\n",
      "Epoch 18/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6160 - accuracy: 0.6071 - val_loss: 0.7023 - val_accuracy: 0.6667\n",
      "Epoch 19/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6114 - accuracy: 0.6071 - val_loss: 0.6968 - val_accuracy: 0.6667\n",
      "Epoch 20/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6095 - accuracy: 0.6071 - val_loss: 0.6941 - val_accuracy: 0.6667\n",
      "Epoch 21/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6054 - accuracy: 0.6071 - val_loss: 0.6898 - val_accuracy: 0.6667\n",
      "Epoch 22/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6011 - accuracy: 0.6071 - val_loss: 0.6893 - val_accuracy: 0.6667\n",
      "Epoch 23/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5971 - accuracy: 0.6071 - val_loss: 0.6855 - val_accuracy: 0.7333\n",
      "Epoch 24/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5941 - accuracy: 0.6250 - val_loss: 0.6796 - val_accuracy: 0.7333\n",
      "Epoch 25/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5905 - accuracy: 0.6250 - val_loss: 0.6777 - val_accuracy: 0.7333\n",
      "Epoch 26/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5872 - accuracy: 0.6250 - val_loss: 0.6751 - val_accuracy: 0.7333\n",
      "Epoch 27/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5829 - accuracy: 0.6250 - val_loss: 0.6688 - val_accuracy: 0.7333\n",
      "Epoch 28/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5804 - accuracy: 0.6250 - val_loss: 0.6631 - val_accuracy: 0.7333\n",
      "Epoch 29/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5749 - accuracy: 0.6250 - val_loss: 0.6652 - val_accuracy: 0.7333\n",
      "Epoch 30/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5728 - accuracy: 0.6250 - val_loss: 0.6632 - val_accuracy: 0.7333\n",
      "Epoch 31/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5686 - accuracy: 0.6250 - val_loss: 0.6607 - val_accuracy: 0.7333\n",
      "Epoch 32/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5663 - accuracy: 0.6250 - val_loss: 0.6568 - val_accuracy: 0.7333\n",
      "Epoch 33/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5629 - accuracy: 0.6607 - val_loss: 0.6494 - val_accuracy: 0.7333\n",
      "Epoch 34/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5593 - accuracy: 0.6786 - val_loss: 0.6445 - val_accuracy: 0.7333\n",
      "Epoch 35/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5549 - accuracy: 0.6964 - val_loss: 0.6475 - val_accuracy: 0.7333\n",
      "Epoch 36/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5508 - accuracy: 0.6964 - val_loss: 0.6416 - val_accuracy: 0.7333\n",
      "Epoch 37/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5479 - accuracy: 0.6964 - val_loss: 0.6310 - val_accuracy: 0.7333\n",
      "Epoch 38/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5443 - accuracy: 0.6964 - val_loss: 0.6329 - val_accuracy: 0.7333\n",
      "Epoch 39/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5380 - accuracy: 0.6964 - val_loss: 0.6341 - val_accuracy: 0.7333\n",
      "Epoch 40/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5377 - accuracy: 0.6964 - val_loss: 0.6237 - val_accuracy: 0.7333\n",
      "Epoch 41/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5319 - accuracy: 0.7143 - val_loss: 0.6286 - val_accuracy: 0.7333\n",
      "Epoch 42/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5296 - accuracy: 0.6964 - val_loss: 0.6114 - val_accuracy: 0.7333\n",
      "Epoch 43/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5213 - accuracy: 0.7143 - val_loss: 0.6169 - val_accuracy: 0.7333\n",
      "Epoch 44/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5228 - accuracy: 0.7143 - val_loss: 0.6222 - val_accuracy: 0.6667\n",
      "Epoch 45/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5151 - accuracy: 0.7143 - val_loss: 0.6130 - val_accuracy: 0.6667\n",
      "Epoch 46/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5149 - accuracy: 0.7321 - val_loss: 0.6113 - val_accuracy: 0.6667\n",
      "Epoch 47/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7321 - val_loss: 0.6129 - val_accuracy: 0.6667\n",
      "Epoch 48/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5050 - accuracy: 0.7143 - val_loss: 0.6032 - val_accuracy: 0.6667\n",
      "Epoch 49/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5033 - accuracy: 0.7321 - val_loss: 0.6113 - val_accuracy: 0.6667\n",
      "Epoch 50/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4977 - accuracy: 0.7321 - val_loss: 0.6127 - val_accuracy: 0.7333\n",
      "Epoch 51/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4948 - accuracy: 0.7321 - val_loss: 0.6041 - val_accuracy: 0.7333\n",
      "Epoch 52/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4912 - accuracy: 0.7321 - val_loss: 0.5971 - val_accuracy: 0.7333\n",
      "Epoch 53/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4870 - accuracy: 0.7321 - val_loss: 0.6046 - val_accuracy: 0.7333\n",
      "Epoch 54/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4828 - accuracy: 0.7321 - val_loss: 0.6011 - val_accuracy: 0.7333\n",
      "Epoch 55/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4796 - accuracy: 0.7500 - val_loss: 0.5996 - val_accuracy: 0.7333\n",
      "Epoch 56/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4754 - accuracy: 0.7500 - val_loss: 0.6035 - val_accuracy: 0.7333\n",
      "Epoch 57/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4733 - accuracy: 0.7500 - val_loss: 0.5966 - val_accuracy: 0.7333\n",
      "Epoch 58/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4686 - accuracy: 0.7500 - val_loss: 0.6020 - val_accuracy: 0.7333\n",
      "Epoch 59/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4682 - accuracy: 0.7500 - val_loss: 0.6029 - val_accuracy: 0.7333\n",
      "Epoch 60/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4644 - accuracy: 0.7500 - val_loss: 0.6142 - val_accuracy: 0.7333\n",
      "Epoch 61/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4591 - accuracy: 0.7500 - val_loss: 0.6039 - val_accuracy: 0.7333\n",
      "Epoch 62/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4550 - accuracy: 0.7500 - val_loss: 0.6031 - val_accuracy: 0.7333\n",
      "Epoch 63/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4538 - accuracy: 0.7500 - val_loss: 0.6036 - val_accuracy: 0.7333\n",
      "Epoch 64/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4507 - accuracy: 0.7500 - val_loss: 0.6049 - val_accuracy: 0.7333\n",
      "Epoch 65/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4469 - accuracy: 0.7500 - val_loss: 0.5997 - val_accuracy: 0.7333\n",
      "Epoch 66/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4434 - accuracy: 0.7500 - val_loss: 0.5997 - val_accuracy: 0.7333\n",
      "Epoch 67/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4403 - accuracy: 0.7500 - val_loss: 0.6017 - val_accuracy: 0.6667\n",
      "Epoch 68/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4382 - accuracy: 0.7500 - val_loss: 0.5957 - val_accuracy: 0.6667\n",
      "Epoch 69/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4358 - accuracy: 0.7857 - val_loss: 0.6117 - val_accuracy: 0.6667\n",
      "Epoch 70/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4335 - accuracy: 0.7857 - val_loss: 0.6004 - val_accuracy: 0.7333\n",
      "Epoch 71/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4348 - accuracy: 0.7679 - val_loss: 0.6104 - val_accuracy: 0.6667\n",
      "Epoch 72/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4261 - accuracy: 0.7679 - val_loss: 0.6097 - val_accuracy: 0.6667\n",
      "Epoch 73/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4251 - accuracy: 0.7857 - val_loss: 0.6043 - val_accuracy: 0.6667\n",
      "Epoch 74/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4226 - accuracy: 0.7857 - val_loss: 0.6151 - val_accuracy: 0.6667\n",
      "Epoch 75/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4177 - accuracy: 0.7857 - val_loss: 0.6185 - val_accuracy: 0.6667\n",
      "Epoch 76/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4209 - accuracy: 0.7857 - val_loss: 0.6034 - val_accuracy: 0.6667\n",
      "Epoch 77/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4166 - accuracy: 0.7857 - val_loss: 0.6178 - val_accuracy: 0.6667\n",
      "Epoch 78/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4107 - accuracy: 0.7857 - val_loss: 0.6108 - val_accuracy: 0.7333\n",
      "Epoch 79/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4093 - accuracy: 0.7857 - val_loss: 0.6162 - val_accuracy: 0.7333\n",
      "Epoch 80/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4061 - accuracy: 0.7857 - val_loss: 0.6205 - val_accuracy: 0.6667\n",
      "Epoch 81/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4046 - accuracy: 0.7857 - val_loss: 0.6159 - val_accuracy: 0.6667\n",
      "Epoch 82/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4032 - accuracy: 0.7857 - val_loss: 0.6229 - val_accuracy: 0.6667\n",
      "Epoch 83/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4004 - accuracy: 0.7857 - val_loss: 0.6205 - val_accuracy: 0.6667\n",
      "Epoch 84/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3985 - accuracy: 0.7857 - val_loss: 0.6128 - val_accuracy: 0.6667\n",
      "Epoch 85/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3950 - accuracy: 0.8036 - val_loss: 0.6210 - val_accuracy: 0.7333\n",
      "Epoch 86/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3928 - accuracy: 0.7857 - val_loss: 0.6261 - val_accuracy: 0.7333\n",
      "Epoch 87/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3906 - accuracy: 0.7857 - val_loss: 0.6303 - val_accuracy: 0.6667\n",
      "Epoch 88/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3880 - accuracy: 0.8036 - val_loss: 0.6276 - val_accuracy: 0.6667\n",
      "Epoch 89/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3857 - accuracy: 0.7857 - val_loss: 0.6249 - val_accuracy: 0.6667\n",
      "Epoch 90/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3845 - accuracy: 0.7857 - val_loss: 0.6288 - val_accuracy: 0.7333\n",
      "Epoch 91/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3826 - accuracy: 0.8036 - val_loss: 0.6346 - val_accuracy: 0.6000\n",
      "Epoch 92/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3807 - accuracy: 0.7857 - val_loss: 0.6207 - val_accuracy: 0.6667\n",
      "Epoch 93/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3762 - accuracy: 0.8036 - val_loss: 0.6335 - val_accuracy: 0.6667\n",
      "Epoch 94/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3744 - accuracy: 0.8036 - val_loss: 0.6395 - val_accuracy: 0.6000\n",
      "Epoch 95/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3712 - accuracy: 0.8036 - val_loss: 0.6379 - val_accuracy: 0.6667\n",
      "Epoch 96/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3709 - accuracy: 0.8036 - val_loss: 0.6402 - val_accuracy: 0.6000\n",
      "Epoch 97/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3674 - accuracy: 0.8036 - val_loss: 0.6432 - val_accuracy: 0.6667\n",
      "Epoch 98/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3652 - accuracy: 0.8036 - val_loss: 0.6417 - val_accuracy: 0.6000\n",
      "Epoch 99/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3630 - accuracy: 0.8036 - val_loss: 0.6316 - val_accuracy: 0.6000\n",
      "Epoch 100/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3604 - accuracy: 0.8036 - val_loss: 0.6434 - val_accuracy: 0.6000\n",
      "Epoch 101/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3574 - accuracy: 0.8036 - val_loss: 0.6521 - val_accuracy: 0.6000\n",
      "Epoch 102/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3572 - accuracy: 0.8036 - val_loss: 0.6464 - val_accuracy: 0.6000\n",
      "Epoch 103/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3548 - accuracy: 0.8036 - val_loss: 0.6521 - val_accuracy: 0.6000\n",
      "Epoch 104/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3510 - accuracy: 0.8036 - val_loss: 0.6424 - val_accuracy: 0.6000\n",
      "Epoch 105/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3507 - accuracy: 0.8036 - val_loss: 0.6421 - val_accuracy: 0.6000\n",
      "Epoch 106/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3477 - accuracy: 0.8036 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
      "Epoch 107/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3453 - accuracy: 0.8036 - val_loss: 0.6527 - val_accuracy: 0.6667\n",
      "Epoch 108/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3465 - accuracy: 0.8036 - val_loss: 0.6534 - val_accuracy: 0.6000\n",
      "Epoch 109/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3454 - accuracy: 0.8036 - val_loss: 0.6400 - val_accuracy: 0.6667\n",
      "Epoch 110/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3414 - accuracy: 0.8036 - val_loss: 0.6494 - val_accuracy: 0.6000\n",
      "Epoch 111/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3380 - accuracy: 0.8036 - val_loss: 0.6584 - val_accuracy: 0.6667\n",
      "Epoch 112/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3361 - accuracy: 0.8036 - val_loss: 0.6657 - val_accuracy: 0.6667\n",
      "Epoch 113/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3335 - accuracy: 0.8036 - val_loss: 0.6600 - val_accuracy: 0.6667\n",
      "Epoch 114/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3317 - accuracy: 0.8036 - val_loss: 0.6643 - val_accuracy: 0.6667\n",
      "Epoch 115/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3297 - accuracy: 0.8036 - val_loss: 0.6651 - val_accuracy: 0.6667\n",
      "Epoch 116/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3300 - accuracy: 0.8036 - val_loss: 0.6666 - val_accuracy: 0.6667\n",
      "Epoch 117/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3281 - accuracy: 0.8036 - val_loss: 0.6777 - val_accuracy: 0.6667\n",
      "Epoch 118/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3257 - accuracy: 0.8036 - val_loss: 0.6739 - val_accuracy: 0.6667\n",
      "Epoch 119/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3212 - accuracy: 0.8214 - val_loss: 0.6751 - val_accuracy: 0.6667\n",
      "Epoch 120/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3192 - accuracy: 0.8214 - val_loss: 0.6819 - val_accuracy: 0.6667\n",
      "Epoch 121/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3215 - accuracy: 0.8214 - val_loss: 0.6810 - val_accuracy: 0.6667\n",
      "Epoch 122/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3185 - accuracy: 0.8214 - val_loss: 0.6897 - val_accuracy: 0.6667\n",
      "Epoch 123/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3155 - accuracy: 0.8214 - val_loss: 0.6877 - val_accuracy: 0.6667\n",
      "Epoch 124/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3149 - accuracy: 0.8214 - val_loss: 0.6793 - val_accuracy: 0.6667\n",
      "Epoch 125/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3129 - accuracy: 0.8036 - val_loss: 0.6809 - val_accuracy: 0.6667\n",
      "Epoch 126/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3105 - accuracy: 0.8214 - val_loss: 0.6834 - val_accuracy: 0.6667\n",
      "Epoch 127/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3076 - accuracy: 0.8214 - val_loss: 0.6922 - val_accuracy: 0.6667\n",
      "Epoch 128/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3100 - accuracy: 0.8214 - val_loss: 0.6706 - val_accuracy: 0.6667\n",
      "Epoch 129/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3050 - accuracy: 0.8214 - val_loss: 0.6968 - val_accuracy: 0.6667\n",
      "Epoch 130/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3047 - accuracy: 0.8214 - val_loss: 0.7035 - val_accuracy: 0.6667\n",
      "Epoch 131/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3040 - accuracy: 0.8214 - val_loss: 0.7053 - val_accuracy: 0.6667\n",
      "Epoch 132/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3013 - accuracy: 0.8214 - val_loss: 0.7035 - val_accuracy: 0.6667\n",
      "Epoch 133/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2994 - accuracy: 0.8214 - val_loss: 0.7094 - val_accuracy: 0.6667\n",
      "Epoch 134/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3008 - accuracy: 0.8214 - val_loss: 0.7206 - val_accuracy: 0.6667\n",
      "Epoch 135/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2981 - accuracy: 0.8214 - val_loss: 0.7082 - val_accuracy: 0.6667\n",
      "Epoch 136/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2948 - accuracy: 0.8214 - val_loss: 0.7047 - val_accuracy: 0.6667\n",
      "Epoch 137/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2918 - accuracy: 0.8214 - val_loss: 0.7229 - val_accuracy: 0.6667\n",
      "Epoch 138/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2914 - accuracy: 0.8214 - val_loss: 0.7145 - val_accuracy: 0.6667\n",
      "Epoch 139/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2896 - accuracy: 0.8214 - val_loss: 0.7089 - val_accuracy: 0.6667\n",
      "Epoch 140/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2848 - accuracy: 0.8214 - val_loss: 0.7231 - val_accuracy: 0.6667\n",
      "Epoch 141/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2854 - accuracy: 0.8214 - val_loss: 0.7182 - val_accuracy: 0.6667\n",
      "Epoch 142/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2824 - accuracy: 0.8214 - val_loss: 0.7319 - val_accuracy: 0.6667\n",
      "Epoch 143/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2818 - accuracy: 0.8214 - val_loss: 0.7206 - val_accuracy: 0.6667\n",
      "Epoch 144/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2800 - accuracy: 0.8214 - val_loss: 0.7221 - val_accuracy: 0.6667\n",
      "Epoch 145/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2781 - accuracy: 0.8214 - val_loss: 0.7267 - val_accuracy: 0.6667\n",
      "Epoch 146/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2771 - accuracy: 0.8214 - val_loss: 0.7132 - val_accuracy: 0.6667\n",
      "Epoch 147/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2745 - accuracy: 0.8214 - val_loss: 0.7291 - val_accuracy: 0.6667\n",
      "Epoch 148/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2722 - accuracy: 0.8214 - val_loss: 0.7268 - val_accuracy: 0.6667\n",
      "Epoch 149/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2695 - accuracy: 0.8214 - val_loss: 0.7328 - val_accuracy: 0.6667\n",
      "Epoch 150/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2712 - accuracy: 0.8214 - val_loss: 0.7185 - val_accuracy: 0.6667\n",
      "Epoch 151/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2680 - accuracy: 0.8214 - val_loss: 0.7283 - val_accuracy: 0.6667\n",
      "Epoch 152/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2682 - accuracy: 0.8393 - val_loss: 0.7153 - val_accuracy: 0.6667\n",
      "Epoch 153/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2709 - accuracy: 0.8214 - val_loss: 0.7353 - val_accuracy: 0.6667\n",
      "Epoch 154/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2683 - accuracy: 0.8214 - val_loss: 0.7117 - val_accuracy: 0.6667\n",
      "Epoch 155/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2614 - accuracy: 0.8214 - val_loss: 0.7191 - val_accuracy: 0.6667\n",
      "Epoch 156/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2616 - accuracy: 0.8214 - val_loss: 0.7187 - val_accuracy: 0.6667\n",
      "Epoch 157/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2570 - accuracy: 0.8393 - val_loss: 0.7319 - val_accuracy: 0.6667\n",
      "Epoch 158/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2590 - accuracy: 0.8393 - val_loss: 0.7477 - val_accuracy: 0.6667\n",
      "Epoch 159/200\n"
     ]
    }
   ],
   "source": [
    "model = get_basic_model()\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=2, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eb77b8-09e2-4ebe-b272-d5412772c6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ca50ea-7ce3-4efa-b775-46733dd404af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb91266d-fbfd-4e7d-a288-78c7cfcd9a93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
