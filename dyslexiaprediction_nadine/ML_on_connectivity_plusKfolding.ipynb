{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning models on connectivity data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook: \n",
    "- Necessary imports\n",
    "- SVM model \n",
    "- Logistic Regression model\n",
    "- Decision Tree model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os       # using operating system dependent functionality (folders)\n",
    "import pandas as pd # data analysis and manipulation\n",
    "import numpy as np    # numerical computing (manipulating and performing operations on arrays of data)\n",
    "import copy     # Can Copy and Deepcopy files so original file is untouched.\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../eegyolk') # path to helper functions\n",
    "import helper_functions as hf # library useful for eeg and erp data cleaning\n",
    "#import initialization_functions #library to import data\n",
    "import epod_helper\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_connectivity.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>32</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>...</th>\n",
       "      <th>1014</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>Group_AccToParents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.152030</td>\n",
       "      <td>0.069506</td>\n",
       "      <td>0.028244</td>\n",
       "      <td>0.216019</td>\n",
       "      <td>0.236540</td>\n",
       "      <td>0.110768</td>\n",
       "      <td>0.146072</td>\n",
       "      <td>0.234775</td>\n",
       "      <td>0.123786</td>\n",
       "      <td>0.224846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092895</td>\n",
       "      <td>0.109223</td>\n",
       "      <td>0.091130</td>\n",
       "      <td>0.110989</td>\n",
       "      <td>0.126434</td>\n",
       "      <td>0.059135</td>\n",
       "      <td>0.116726</td>\n",
       "      <td>0.058032</td>\n",
       "      <td>0.164387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.049602</td>\n",
       "      <td>0.027627</td>\n",
       "      <td>0.060067</td>\n",
       "      <td>0.021557</td>\n",
       "      <td>0.016116</td>\n",
       "      <td>0.090414</td>\n",
       "      <td>0.022394</td>\n",
       "      <td>0.053370</td>\n",
       "      <td>0.050649</td>\n",
       "      <td>0.114483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114692</td>\n",
       "      <td>0.096693</td>\n",
       "      <td>0.043533</td>\n",
       "      <td>0.169527</td>\n",
       "      <td>0.042905</td>\n",
       "      <td>0.028882</td>\n",
       "      <td>0.031812</td>\n",
       "      <td>0.038928</td>\n",
       "      <td>0.098577</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.380034</td>\n",
       "      <td>0.336761</td>\n",
       "      <td>0.322622</td>\n",
       "      <td>0.404456</td>\n",
       "      <td>0.434876</td>\n",
       "      <td>0.360326</td>\n",
       "      <td>0.452442</td>\n",
       "      <td>0.464439</td>\n",
       "      <td>0.064267</td>\n",
       "      <td>0.492716</td>\n",
       "      <td>...</td>\n",
       "      <td>0.339332</td>\n",
       "      <td>0.338046</td>\n",
       "      <td>0.392888</td>\n",
       "      <td>0.127249</td>\n",
       "      <td>0.395458</td>\n",
       "      <td>0.368038</td>\n",
       "      <td>0.422022</td>\n",
       "      <td>0.341902</td>\n",
       "      <td>0.328620</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.044057</td>\n",
       "      <td>0.063730</td>\n",
       "      <td>0.048361</td>\n",
       "      <td>0.123975</td>\n",
       "      <td>0.148566</td>\n",
       "      <td>0.029508</td>\n",
       "      <td>0.107992</td>\n",
       "      <td>0.155943</td>\n",
       "      <td>0.071107</td>\n",
       "      <td>0.087090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045082</td>\n",
       "      <td>0.057582</td>\n",
       "      <td>0.068238</td>\n",
       "      <td>0.159016</td>\n",
       "      <td>0.117418</td>\n",
       "      <td>0.093033</td>\n",
       "      <td>0.115574</td>\n",
       "      <td>0.086475</td>\n",
       "      <td>0.150410</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.137384</td>\n",
       "      <td>0.124695</td>\n",
       "      <td>0.082479</td>\n",
       "      <td>0.244021</td>\n",
       "      <td>0.198389</td>\n",
       "      <td>0.036115</td>\n",
       "      <td>0.171791</td>\n",
       "      <td>0.170327</td>\n",
       "      <td>0.102733</td>\n",
       "      <td>0.158370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027086</td>\n",
       "      <td>0.053197</td>\n",
       "      <td>0.029771</td>\n",
       "      <td>0.063202</td>\n",
       "      <td>0.110786</td>\n",
       "      <td>0.061249</td>\n",
       "      <td>0.121279</td>\n",
       "      <td>0.102245</td>\n",
       "      <td>0.085652</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.066393</td>\n",
       "      <td>0.027664</td>\n",
       "      <td>0.032787</td>\n",
       "      <td>0.065574</td>\n",
       "      <td>0.107992</td>\n",
       "      <td>0.030533</td>\n",
       "      <td>0.080123</td>\n",
       "      <td>0.150205</td>\n",
       "      <td>0.065164</td>\n",
       "      <td>0.136680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021107</td>\n",
       "      <td>0.042828</td>\n",
       "      <td>0.038525</td>\n",
       "      <td>0.177869</td>\n",
       "      <td>0.128074</td>\n",
       "      <td>0.068852</td>\n",
       "      <td>0.125820</td>\n",
       "      <td>0.072951</td>\n",
       "      <td>0.175820</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.126844</td>\n",
       "      <td>0.023566</td>\n",
       "      <td>0.020287</td>\n",
       "      <td>0.169672</td>\n",
       "      <td>0.138115</td>\n",
       "      <td>0.068033</td>\n",
       "      <td>0.156762</td>\n",
       "      <td>0.223156</td>\n",
       "      <td>0.099385</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070287</td>\n",
       "      <td>0.071107</td>\n",
       "      <td>0.085656</td>\n",
       "      <td>0.211066</td>\n",
       "      <td>0.145902</td>\n",
       "      <td>0.079508</td>\n",
       "      <td>0.137295</td>\n",
       "      <td>0.077049</td>\n",
       "      <td>0.206352</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.105123</td>\n",
       "      <td>0.071107</td>\n",
       "      <td>0.068033</td>\n",
       "      <td>0.112910</td>\n",
       "      <td>0.087705</td>\n",
       "      <td>0.031762</td>\n",
       "      <td>0.079713</td>\n",
       "      <td>0.110246</td>\n",
       "      <td>0.099590</td>\n",
       "      <td>0.098566</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155738</td>\n",
       "      <td>0.183811</td>\n",
       "      <td>0.116598</td>\n",
       "      <td>0.162090</td>\n",
       "      <td>0.189959</td>\n",
       "      <td>0.080123</td>\n",
       "      <td>0.158402</td>\n",
       "      <td>0.123156</td>\n",
       "      <td>0.202049</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.121926</td>\n",
       "      <td>0.013934</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.178689</td>\n",
       "      <td>0.180533</td>\n",
       "      <td>0.065779</td>\n",
       "      <td>0.159016</td>\n",
       "      <td>0.177664</td>\n",
       "      <td>0.090164</td>\n",
       "      <td>0.148770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071926</td>\n",
       "      <td>0.055328</td>\n",
       "      <td>0.032787</td>\n",
       "      <td>0.115984</td>\n",
       "      <td>0.097131</td>\n",
       "      <td>0.053689</td>\n",
       "      <td>0.102869</td>\n",
       "      <td>0.072131</td>\n",
       "      <td>0.128484</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.060041</td>\n",
       "      <td>0.025205</td>\n",
       "      <td>0.010861</td>\n",
       "      <td>0.096926</td>\n",
       "      <td>0.094467</td>\n",
       "      <td>0.039344</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.101844</td>\n",
       "      <td>0.046721</td>\n",
       "      <td>0.057992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011270</td>\n",
       "      <td>0.083197</td>\n",
       "      <td>0.066803</td>\n",
       "      <td>0.091598</td>\n",
       "      <td>0.106762</td>\n",
       "      <td>0.060041</td>\n",
       "      <td>0.099590</td>\n",
       "      <td>0.042828</td>\n",
       "      <td>0.119877</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 497 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          32        64        65        96        97        98       128  \\\n",
       "0   0.152030  0.069506  0.028244  0.216019  0.236540  0.110768  0.146072   \n",
       "1   0.049602  0.027627  0.060067  0.021557  0.016116  0.090414  0.022394   \n",
       "2   0.380034  0.336761  0.322622  0.404456  0.434876  0.360326  0.452442   \n",
       "3   0.044057  0.063730  0.048361  0.123975  0.148566  0.029508  0.107992   \n",
       "4   0.137384  0.124695  0.082479  0.244021  0.198389  0.036115  0.171791   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "85  0.066393  0.027664  0.032787  0.065574  0.107992  0.030533  0.080123   \n",
       "86  0.126844  0.023566  0.020287  0.169672  0.138115  0.068033  0.156762   \n",
       "87  0.105123  0.071107  0.068033  0.112910  0.087705  0.031762  0.079713   \n",
       "88  0.121926  0.013934  0.025000  0.178689  0.180533  0.065779  0.159016   \n",
       "89  0.060041  0.025205  0.010861  0.096926  0.094467  0.039344  0.062500   \n",
       "\n",
       "         129       130       131  ...      1014      1015      1016      1017  \\\n",
       "0   0.234775  0.123786  0.224846  ...  0.092895  0.109223  0.091130  0.110989   \n",
       "1   0.053370  0.050649  0.114483  ...  0.114692  0.096693  0.043533  0.169527   \n",
       "2   0.464439  0.064267  0.492716  ...  0.339332  0.338046  0.392888  0.127249   \n",
       "3   0.155943  0.071107  0.087090  ...  0.045082  0.057582  0.068238  0.159016   \n",
       "4   0.170327  0.102733  0.158370  ...  0.027086  0.053197  0.029771  0.063202   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "85  0.150205  0.065164  0.136680  ...  0.021107  0.042828  0.038525  0.177869   \n",
       "86  0.223156  0.099385  0.187500  ...  0.070287  0.071107  0.085656  0.211066   \n",
       "87  0.110246  0.099590  0.098566  ...  0.155738  0.183811  0.116598  0.162090   \n",
       "88  0.177664  0.090164  0.148770  ...  0.071926  0.055328  0.032787  0.115984   \n",
       "89  0.101844  0.046721  0.057992  ...  0.011270  0.083197  0.066803  0.091598   \n",
       "\n",
       "        1018      1019      1020      1021      1022  Group_AccToParents  \n",
       "0   0.126434  0.059135  0.116726  0.058032  0.164387                   1  \n",
       "1   0.042905  0.028882  0.031812  0.038928  0.098577                   0  \n",
       "2   0.395458  0.368038  0.422022  0.341902  0.328620                   1  \n",
       "3   0.117418  0.093033  0.115574  0.086475  0.150410                   1  \n",
       "4   0.110786  0.061249  0.121279  0.102245  0.085652                   1  \n",
       "..       ...       ...       ...       ...       ...                 ...  \n",
       "85  0.128074  0.068852  0.125820  0.072951  0.175820                   1  \n",
       "86  0.145902  0.079508  0.137295  0.077049  0.206352                   1  \n",
       "87  0.189959  0.080123  0.158402  0.123156  0.202049                   1  \n",
       "88  0.097131  0.053689  0.102869  0.072131  0.128484                   1  \n",
       "89  0.106762  0.060041  0.099590  0.042828  0.119877                   0  \n",
       "\n",
       "[90 rows x 497 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Group_AccToParents'].values # dependant variable\n",
    "X = df.drop(['Group_AccToParents'],axis=1).values   # independant features\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train = sc.transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=0.1, kernel=&#x27;linear&#x27;, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=0.1, kernel=&#x27;linear&#x27;, random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=0.1, kernel='linear', random_state=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC(kernel= 'linear', random_state=1, C=0.1)\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.519\n"
     ]
    }
   ],
   "source": [
    "y_pred = svm.predict(X_test)\n",
    "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK let's try a k-fold approach instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.53846154, 0.53846154, 0.53846154, 0.53846154, 0.53846154,\n",
       "       0.53846154, 0.5       ])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "svm_scores = cross_val_score(svm, X, y, cv=7)\n",
    "svm_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.53 accuracy with a standard deviation of 0.01\n"
     ]
    }
   ],
   "source": [
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (svm_scores.mean(), svm_scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=0, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=0, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=0, solver='liblinear')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='liblinear', random_state=0)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.519\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.92624810e-01 -5.56565669e-02 -7.27771977e-02  2.78741247e-01\n",
      "   2.74218231e-02  1.52318120e-01  1.07227274e-01 -1.30908560e-01\n",
      "   1.76854080e-01 -1.16429114e-01  1.47627831e-02 -9.62285537e-02\n",
      "   1.69211862e-01 -2.72913164e-01 -1.87572998e-01 -9.51591420e-02\n",
      "   5.16049131e-03 -1.83982315e-01 -1.71120642e-01 -9.06896955e-02\n",
      "  -3.19999346e-02  1.65016226e-01 -7.89033163e-02  1.16655401e-02\n",
      "  -5.78456021e-02 -1.24686669e-01  1.04039667e-01 -2.62832026e-02\n",
      "  -1.51643191e-01 -1.59677891e-01 -8.12428738e-02 -3.80832414e-02\n",
      "  -8.99005569e-03  1.02317613e-01 -6.68204835e-02  1.43322280e-01\n",
      "   4.48290529e-02 -1.63747496e-02 -3.87396725e-02 -6.43959429e-02\n",
      "   1.86223597e-02  7.05462274e-02 -4.06584221e-02 -7.48669920e-03\n",
      "  -1.19168180e-02  1.29068940e-01  1.59098377e-01  3.81249020e-02\n",
      "  -1.51324702e-03 -8.53993547e-02 -3.38066151e-02  1.11487516e-01\n",
      "   8.68426390e-03 -4.24554480e-02  7.44769384e-02  9.27079156e-02\n",
      "   1.02414269e-01  8.00207628e-02 -4.34146407e-02 -3.09361261e-02\n",
      "   2.06009160e-01 -3.33836784e-02 -9.97445811e-02 -1.42163033e-01\n",
      "   9.17646699e-02 -5.10531781e-02  2.82883411e-02  1.26212750e-01\n",
      "   1.39331881e-01  1.07390640e-01  2.23656474e-01  2.88513377e-01\n",
      "   2.41045990e-02  1.76607858e-01  6.20180180e-02  2.84468732e-01\n",
      "   6.31559629e-02  6.10037072e-02  1.17472841e-01  1.43556258e-01\n",
      "   2.40523836e-01  2.45993969e-02 -2.17001700e-02  2.93456930e-01\n",
      "  -6.19702800e-02 -5.80394992e-02 -2.04843560e-01  6.72760201e-02\n",
      "   4.79935478e-02  2.96786970e-02 -1.16627838e-01 -4.24956019e-02\n",
      "  -1.82410502e-05 -7.45827030e-02 -2.66090336e-02 -6.65378778e-02\n",
      "   1.14598784e-02  6.65386947e-02 -8.69411170e-02 -1.50108284e-01\n",
      "   9.70782685e-02  1.19963989e-02  7.89270778e-02 -2.50101750e-02\n",
      "   7.33680426e-02  5.81887168e-02  6.44443622e-03 -4.02199434e-02\n",
      "  -5.50231158e-02  2.84373240e-02  1.13330864e-01  6.52946291e-02\n",
      "   1.35807397e-03  1.39407774e-01  3.24046905e-02  6.60271756e-02\n",
      "   4.04998227e-02  1.94987617e-01  1.47200829e-01  9.36497961e-02\n",
      "  -1.76427591e-02  3.73715604e-02 -1.74699616e-03 -1.10329200e-01\n",
      "   9.40086073e-02  1.26476011e-01 -1.21524633e-01  4.18949302e-02\n",
      "   6.80311148e-02 -6.65878019e-02  1.33878119e-01  5.04406861e-02\n",
      "   1.54596749e-01 -1.49527195e-01  2.64138021e-01  7.79851965e-02\n",
      "   4.58136288e-02  1.98848501e-02  5.77739420e-02  6.43475800e-03\n",
      "   4.54178379e-03  9.84248245e-02 -1.43304262e-01 -3.36405159e-02\n",
      "  -1.23126874e-01 -1.92178711e-01  1.20762681e-01 -1.70643530e-01\n",
      "   3.18940748e-02 -7.84289997e-02  3.96858216e-01 -5.94939456e-02\n",
      "  -2.35742763e-01 -2.93415302e-02  4.42858334e-02  1.68266511e-01\n",
      "   7.95074063e-02 -5.36366173e-02  1.15763104e-01 -1.34473222e-01\n",
      "  -3.73834107e-02  3.60686672e-02  1.73567090e-01  6.56776023e-02\n",
      "  -7.36721295e-02  1.06589118e-01 -7.93019851e-02  6.14403245e-02\n",
      "  -8.09191724e-02 -2.10851892e-01 -4.09896501e-02 -1.41948608e-01\n",
      "  -1.25443766e-01  3.04195862e-02  2.34902709e-02 -1.06679024e-01\n",
      "   1.55297220e-02  4.92196665e-02  9.25274178e-02  8.16369985e-02\n",
      "   6.55073589e-02  1.85552666e-01 -8.03598476e-02  9.02388851e-02\n",
      "  -4.76827043e-02 -9.08466460e-02  1.71925964e-02  6.30325185e-04\n",
      "  -8.78879765e-02  1.63450016e-01  1.56700778e-02  4.20898149e-03\n",
      "   1.48353131e-01  1.71497599e-01  1.65126881e-01  3.78154921e-02\n",
      "  -2.10083222e-02  1.33584954e-01  8.72243537e-02 -5.95193257e-02\n",
      "  -3.60926826e-02 -1.01239552e-01  1.82973084e-01  1.22840997e-01\n",
      "   8.00971779e-03  3.63910395e-02 -1.17353280e-01 -1.14416905e-01\n",
      "  -2.79217285e-02  8.19732383e-02 -2.79929938e-02 -6.85384868e-02\n",
      "  -1.66343876e-01  1.21803474e-01  1.95804817e-01  1.60417500e-01\n",
      "   1.15698952e-01  1.09064450e-01 -3.14644972e-03  1.79397594e-01\n",
      "  -3.77995188e-02 -1.34632630e-01  9.04074510e-02 -1.77257053e-01\n",
      "  -7.47194747e-02  1.22127528e-01  1.39278238e-01 -6.75664856e-02\n",
      "   3.15882231e-02  7.41997226e-02  2.16736421e-01 -6.47908777e-02\n",
      "  -7.60790419e-02  8.29682501e-02  1.52684439e-03 -1.45067444e-01\n",
      "   5.19210641e-02  1.44971327e-01 -1.56223772e-01 -6.07022769e-02\n",
      "   2.58225905e-01  7.73475327e-02 -2.61382418e-01 -8.67377876e-02\n",
      "  -1.35751996e-01  1.67787342e-02  3.08564684e-02  2.43543766e-02\n",
      "  -1.80106871e-01 -2.07126908e-01  8.15261509e-02  1.39141486e-01\n",
      "   7.34502792e-02  3.09215215e-01  5.14561031e-01  1.18792779e-01\n",
      "   2.32779330e-01  9.33900069e-02 -1.59227662e-02 -1.19402882e-01\n",
      "   1.06382548e-01 -4.01961000e-02 -2.88417059e-01  4.76300401e-02\n",
      "  -8.06986555e-02  6.35241247e-02  2.42949215e-01  4.88724928e-02\n",
      "   1.27566354e-01 -9.64928642e-02 -1.22346400e-01 -6.55882696e-02\n",
      "   3.12597306e-01  8.44651891e-02  8.25705293e-02  1.07424076e-01\n",
      "   1.87509782e-01 -6.91589725e-02  2.34308173e-02  1.53372232e-03\n",
      "  -4.69457827e-02 -7.87101267e-02  3.03088902e-02  7.30109596e-02\n",
      "  -7.88142526e-02 -1.45804257e-02  7.68955516e-02 -1.07565470e-01\n",
      "  -2.14526815e-02 -3.08734869e-02 -8.15728033e-02 -2.15082233e-02\n",
      "  -1.99485504e-01 -9.73449750e-02 -2.61102821e-02 -6.92284031e-02\n",
      "   6.04138031e-02 -2.08506793e-02  1.47542370e-01  2.01669378e-02\n",
      "   1.89477571e-01 -1.14161908e-01  3.05487658e-01 -7.54491748e-03\n",
      "   5.76366314e-02 -1.48683495e-01  1.21678921e-02 -2.33306268e-01\n",
      "   2.40391639e-02  7.70362326e-02 -8.39083244e-02 -2.09804626e-01\n",
      "   1.30170261e-01 -2.87386560e-01 -1.97333114e-01  1.44222615e-02\n",
      "   1.69150788e-02 -3.14053901e-01 -6.97679030e-02 -7.08587390e-02\n",
      "   2.84462517e-01  1.84122057e-01 -3.52623097e-03  3.76524451e-02\n",
      "   1.55652916e-01  1.22800614e-01  5.25630749e-02 -1.06874894e-01\n",
      "   1.54517208e-01 -7.15643906e-02 -8.68895559e-05  1.02131717e-01\n",
      "  -9.26338396e-02 -3.62382791e-02  9.34160498e-02  2.53628546e-01\n",
      "  -2.40296011e-01  1.37026763e-01 -1.18857063e-01 -8.95742903e-02\n",
      "  -7.41349236e-02 -1.26155833e-01 -1.30344022e-01  7.19353918e-03\n",
      "   8.23123399e-03  7.33681487e-02  2.52409675e-01 -3.19045900e-02\n",
      "   1.17966109e-01 -9.87655267e-02 -3.35120180e-02  2.02066541e-01\n",
      "   1.15699372e-02 -3.66310575e-02 -6.63657968e-02 -6.13291299e-02\n",
      "   8.78656521e-02  2.85775515e-02 -1.59569779e-01 -8.63774903e-02\n",
      "  -9.76183057e-02  1.21232201e-02 -8.66734515e-02 -1.63185722e-02\n",
      "  -6.88976488e-03 -2.79002415e-01 -9.26444274e-02 -3.22567927e-01\n",
      "  -5.68879897e-02  1.00197687e-01 -1.64579658e-01 -2.56634862e-02\n",
      "   7.15567839e-02  1.42087096e-01 -1.54679868e-01 -4.58738453e-02\n",
      "   2.83703985e-02 -2.84151573e-01 -3.81012097e-02 -5.59390356e-02\n",
      "  -3.05586507e-01  6.44671332e-02 -5.64617286e-03 -1.29334147e-01\n",
      "  -1.22196969e-01 -7.96249960e-02 -2.19618156e-01 -7.35399788e-02\n",
      "   2.05254194e-01 -7.90858688e-02  6.94811606e-02  4.74014164e-02\n",
      "  -1.13102003e-01 -5.50107447e-02 -7.09556151e-02 -8.78790501e-02\n",
      "  -9.69423808e-02 -7.02761562e-02 -1.75746757e-02 -6.39723899e-02\n",
      "  -3.24596956e-02  5.02260865e-02 -6.58025753e-02  9.20090964e-02\n",
      "   2.62519196e-01 -1.20877993e-02 -3.53937797e-02  1.08221183e-01\n",
      "  -5.26387930e-02  2.27088727e-02  3.58506994e-02 -1.78188237e-02\n",
      "  -4.23544275e-02  1.45090900e-02 -3.39824810e-02 -6.10358376e-02\n",
      "   1.98401307e-01  7.29757542e-02  1.51337565e-01  6.92176445e-02\n",
      "  -1.52366906e-01  9.61965376e-02  3.85493719e-03  4.03960612e-02\n",
      "  -7.68560427e-02 -1.45793062e-01 -5.50598680e-02  1.22352430e-02\n",
      "   4.30463537e-02  1.07811214e-01  4.83029918e-02  1.87229435e-01\n",
      "   1.25005806e-01  1.54443848e-01  6.87380609e-03  4.60756278e-02\n",
      "  -1.62634682e-01 -8.71235450e-02  1.04853177e-01 -1.00317198e-01\n",
      "  -6.83983241e-02  4.35340073e-02 -9.85004415e-02 -7.46819130e-03\n",
      "   1.26331710e-01  1.15661767e-01  3.69456048e-02  9.53944836e-02\n",
      "  -3.02529316e-03 -7.04370028e-02  2.23756854e-02  2.10938697e-01\n",
      "   7.51007040e-02  3.67781423e-02 -1.24479718e-01  1.86506017e-01\n",
      "   1.58325895e-01  4.82134562e-02  1.44517315e-01  2.08263286e-02\n",
      "   8.14621244e-02 -1.07201132e-01 -2.22639816e-01  1.25997384e-02\n",
      "   4.16724803e-02 -3.19084772e-02 -1.99376693e-01  3.37381390e-02\n",
      "  -1.75931611e-02 -1.22976846e-02  5.92118274e-02 -5.16592815e-02\n",
      "  -1.05320776e-01 -2.81462451e-01 -8.62547452e-02 -2.83524552e-02\n",
      "  -2.39405061e-01 -1.33273249e-01 -1.01504547e-01 -9.25600353e-02\n",
      "   4.50252915e-02  2.01914389e-02 -9.43748682e-03 -8.22056730e-03\n",
      "   1.05569349e-01  1.35215405e-01  6.63556752e-02 -6.11943134e-02\n",
      "  -1.27574310e-01 -2.44068147e-02  5.83768356e-02 -9.58383116e-02\n",
      "  -1.51295958e-01 -1.68267076e-01 -7.15217059e-02  7.28152515e-03]]\n"
     ]
    }
   ],
   "source": [
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.61538462, 0.53846154, 0.46153846, 0.76923077, 0.53846154,\n",
       "       0.38461538, 0.75      ])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let'sa try some kfoling on that:\n",
    "lr_scores = cross_val_score(lr, X, y, cv=7)\n",
    "lr_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.58 accuracy with a standard deviation of 0.13\n"
     ]
    }
   ],
   "source": [
    "# notice here you have a lot of variation depending on the fold you pick\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (lr_scores.mean(), lr_scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the problem here is believability...the more your accuracy scores vary, the more of a problem you have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = tree.DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.593\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69230769, 0.61538462, 0.53846154, 0.76923077, 0.61538462,\n",
       "       0.69230769, 0.83333333])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let'sa try some kfoling on that:\n",
    "dt_scores = cross_val_score(dt, X, y, cv=7)\n",
    "dt_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68 accuracy with a standard deviation of 0.09\n"
     ]
    }
   ],
   "source": [
    "# notice here you have a lot of variation depending on the fold you pick\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (dt_scores.mean(), dt_scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suddenly decision tree looks hot?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
