{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning models on connectivity data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook: \n",
    "- Necessary imports\n",
    "- SVM model \n",
    "- Logistic Regression model\n",
    "- Decision Tree model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os       # using operating system dependent functionality (folders)\n",
    "import pandas as pd # data analysis and manipulation\n",
    "import numpy as np    # numerical computing (manipulating and performing operations on arrays of data)\n",
    "import copy     # Can Copy and Deepcopy files so original file is untouched.\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../eegyolk') # path to helper functions\n",
    "import helper_functions as hf # library useful for eeg and erp data cleaning\n",
    "#import initialization_functions #library to import data\n",
    "import epod_helper\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_connectivity.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>32</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>...</th>\n",
       "      <th>1014</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>Group_AccToParents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.245155</td>\n",
       "      <td>0.090722</td>\n",
       "      <td>0.026804</td>\n",
       "      <td>0.285567</td>\n",
       "      <td>0.198144</td>\n",
       "      <td>0.146186</td>\n",
       "      <td>0.274433</td>\n",
       "      <td>0.292371</td>\n",
       "      <td>0.226598</td>\n",
       "      <td>0.251134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116289</td>\n",
       "      <td>0.041856</td>\n",
       "      <td>0.120619</td>\n",
       "      <td>0.158969</td>\n",
       "      <td>0.094639</td>\n",
       "      <td>0.065567</td>\n",
       "      <td>0.097113</td>\n",
       "      <td>0.084948</td>\n",
       "      <td>0.091340</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.242668</td>\n",
       "      <td>0.084469</td>\n",
       "      <td>0.034490</td>\n",
       "      <td>0.290582</td>\n",
       "      <td>0.234201</td>\n",
       "      <td>0.144775</td>\n",
       "      <td>0.295952</td>\n",
       "      <td>0.298843</td>\n",
       "      <td>0.241223</td>\n",
       "      <td>0.250516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009913</td>\n",
       "      <td>0.023337</td>\n",
       "      <td>0.013631</td>\n",
       "      <td>0.009707</td>\n",
       "      <td>0.005370</td>\n",
       "      <td>0.033251</td>\n",
       "      <td>0.007435</td>\n",
       "      <td>0.021272</td>\n",
       "      <td>0.010739</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.031632</td>\n",
       "      <td>0.059378</td>\n",
       "      <td>0.298557</td>\n",
       "      <td>0.283019</td>\n",
       "      <td>0.171476</td>\n",
       "      <td>0.352386</td>\n",
       "      <td>0.380133</td>\n",
       "      <td>0.260266</td>\n",
       "      <td>0.303552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119312</td>\n",
       "      <td>0.171476</td>\n",
       "      <td>0.218091</td>\n",
       "      <td>0.209212</td>\n",
       "      <td>0.261931</td>\n",
       "      <td>0.225305</td>\n",
       "      <td>0.219201</td>\n",
       "      <td>0.210877</td>\n",
       "      <td>0.229745</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.246960</td>\n",
       "      <td>0.037317</td>\n",
       "      <td>0.106289</td>\n",
       "      <td>0.121384</td>\n",
       "      <td>0.022013</td>\n",
       "      <td>0.099161</td>\n",
       "      <td>0.084067</td>\n",
       "      <td>0.042977</td>\n",
       "      <td>0.072746</td>\n",
       "      <td>0.051782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151363</td>\n",
       "      <td>0.137107</td>\n",
       "      <td>0.145702</td>\n",
       "      <td>0.054507</td>\n",
       "      <td>0.106918</td>\n",
       "      <td>0.120964</td>\n",
       "      <td>0.100210</td>\n",
       "      <td>0.101048</td>\n",
       "      <td>0.050943</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.065981</td>\n",
       "      <td>0.011354</td>\n",
       "      <td>0.242502</td>\n",
       "      <td>0.179306</td>\n",
       "      <td>0.118038</td>\n",
       "      <td>0.302699</td>\n",
       "      <td>0.327335</td>\n",
       "      <td>0.191088</td>\n",
       "      <td>0.301628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052057</td>\n",
       "      <td>0.038346</td>\n",
       "      <td>0.029349</td>\n",
       "      <td>0.063410</td>\n",
       "      <td>0.035347</td>\n",
       "      <td>0.011568</td>\n",
       "      <td>0.021422</td>\n",
       "      <td>0.021851</td>\n",
       "      <td>0.009426</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.248847</td>\n",
       "      <td>0.050154</td>\n",
       "      <td>0.062068</td>\n",
       "      <td>0.279593</td>\n",
       "      <td>0.263836</td>\n",
       "      <td>0.170061</td>\n",
       "      <td>0.338586</td>\n",
       "      <td>0.371253</td>\n",
       "      <td>0.250192</td>\n",
       "      <td>0.291699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027095</td>\n",
       "      <td>0.017487</td>\n",
       "      <td>0.004612</td>\n",
       "      <td>0.024981</td>\n",
       "      <td>0.013259</td>\n",
       "      <td>0.027479</td>\n",
       "      <td>0.019216</td>\n",
       "      <td>0.032091</td>\n",
       "      <td>0.027671</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.172960</td>\n",
       "      <td>0.019555</td>\n",
       "      <td>0.093729</td>\n",
       "      <td>0.228928</td>\n",
       "      <td>0.183749</td>\n",
       "      <td>0.185098</td>\n",
       "      <td>0.257249</td>\n",
       "      <td>0.271409</td>\n",
       "      <td>0.220836</td>\n",
       "      <td>0.207687</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020904</td>\n",
       "      <td>0.015846</td>\n",
       "      <td>0.042819</td>\n",
       "      <td>0.051247</td>\n",
       "      <td>0.055293</td>\n",
       "      <td>0.040796</td>\n",
       "      <td>0.047876</td>\n",
       "      <td>0.047876</td>\n",
       "      <td>0.077883</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.180275</td>\n",
       "      <td>0.072018</td>\n",
       "      <td>0.018578</td>\n",
       "      <td>0.202752</td>\n",
       "      <td>0.144725</td>\n",
       "      <td>0.120183</td>\n",
       "      <td>0.141972</td>\n",
       "      <td>0.147248</td>\n",
       "      <td>0.120183</td>\n",
       "      <td>0.125917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023165</td>\n",
       "      <td>0.084862</td>\n",
       "      <td>0.038303</td>\n",
       "      <td>0.015826</td>\n",
       "      <td>0.024083</td>\n",
       "      <td>0.033945</td>\n",
       "      <td>0.032110</td>\n",
       "      <td>0.032339</td>\n",
       "      <td>0.016972</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.182128</td>\n",
       "      <td>0.052524</td>\n",
       "      <td>0.069577</td>\n",
       "      <td>0.141201</td>\n",
       "      <td>0.091405</td>\n",
       "      <td>0.085266</td>\n",
       "      <td>0.104366</td>\n",
       "      <td>0.096180</td>\n",
       "      <td>0.021146</td>\n",
       "      <td>0.061392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056617</td>\n",
       "      <td>0.074352</td>\n",
       "      <td>0.047067</td>\n",
       "      <td>0.023192</td>\n",
       "      <td>0.022510</td>\n",
       "      <td>0.039563</td>\n",
       "      <td>0.030696</td>\n",
       "      <td>0.033424</td>\n",
       "      <td>0.027285</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.118286</td>\n",
       "      <td>0.064166</td>\n",
       "      <td>0.017015</td>\n",
       "      <td>0.162157</td>\n",
       "      <td>0.155392</td>\n",
       "      <td>0.115006</td>\n",
       "      <td>0.159492</td>\n",
       "      <td>0.173022</td>\n",
       "      <td>0.129766</td>\n",
       "      <td>0.147191</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073801</td>\n",
       "      <td>0.048790</td>\n",
       "      <td>0.073391</td>\n",
       "      <td>0.091841</td>\n",
       "      <td>0.086716</td>\n",
       "      <td>0.088151</td>\n",
       "      <td>0.086306</td>\n",
       "      <td>0.078721</td>\n",
       "      <td>0.113981</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 497 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           32        64        65        96        97        98       128  \\\n",
       "0    0.245155  0.090722  0.026804  0.285567  0.198144  0.146186  0.274433   \n",
       "1    0.242668  0.084469  0.034490  0.290582  0.234201  0.144775  0.295952   \n",
       "2    0.205882  0.031632  0.059378  0.298557  0.283019  0.171476  0.352386   \n",
       "3    0.246960  0.037317  0.106289  0.121384  0.022013  0.099161  0.084067   \n",
       "4    0.166667  0.065981  0.011354  0.242502  0.179306  0.118038  0.302699   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "96   0.248847  0.050154  0.062068  0.279593  0.263836  0.170061  0.338586   \n",
       "97   0.172960  0.019555  0.093729  0.228928  0.183749  0.185098  0.257249   \n",
       "98   0.180275  0.072018  0.018578  0.202752  0.144725  0.120183  0.141972   \n",
       "99   0.182128  0.052524  0.069577  0.141201  0.091405  0.085266  0.104366   \n",
       "100  0.118286  0.064166  0.017015  0.162157  0.155392  0.115006  0.159492   \n",
       "\n",
       "          129       130       131  ...      1014      1015      1016  \\\n",
       "0    0.292371  0.226598  0.251134  ...  0.116289  0.041856  0.120619   \n",
       "1    0.298843  0.241223  0.250516  ...  0.009913  0.023337  0.013631   \n",
       "2    0.380133  0.260266  0.303552  ...  0.119312  0.171476  0.218091   \n",
       "3    0.042977  0.072746  0.051782  ...  0.151363  0.137107  0.145702   \n",
       "4    0.327335  0.191088  0.301628  ...  0.052057  0.038346  0.029349   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "96   0.371253  0.250192  0.291699  ...  0.027095  0.017487  0.004612   \n",
       "97   0.271409  0.220836  0.207687  ...  0.020904  0.015846  0.042819   \n",
       "98   0.147248  0.120183  0.125917  ...  0.023165  0.084862  0.038303   \n",
       "99   0.096180  0.021146  0.061392  ...  0.056617  0.074352  0.047067   \n",
       "100  0.173022  0.129766  0.147191  ...  0.073801  0.048790  0.073391   \n",
       "\n",
       "         1017      1018      1019      1020      1021      1022  \\\n",
       "0    0.158969  0.094639  0.065567  0.097113  0.084948  0.091340   \n",
       "1    0.009707  0.005370  0.033251  0.007435  0.021272  0.010739   \n",
       "2    0.209212  0.261931  0.225305  0.219201  0.210877  0.229745   \n",
       "3    0.054507  0.106918  0.120964  0.100210  0.101048  0.050943   \n",
       "4    0.063410  0.035347  0.011568  0.021422  0.021851  0.009426   \n",
       "..        ...       ...       ...       ...       ...       ...   \n",
       "96   0.024981  0.013259  0.027479  0.019216  0.032091  0.027671   \n",
       "97   0.051247  0.055293  0.040796  0.047876  0.047876  0.077883   \n",
       "98   0.015826  0.024083  0.033945  0.032110  0.032339  0.016972   \n",
       "99   0.023192  0.022510  0.039563  0.030696  0.033424  0.027285   \n",
       "100  0.091841  0.086716  0.088151  0.086306  0.078721  0.113981   \n",
       "\n",
       "     Group_AccToParents  \n",
       "0                     1  \n",
       "1                     1  \n",
       "2                     1  \n",
       "3                     1  \n",
       "4                     1  \n",
       "..                  ...  \n",
       "96                    1  \n",
       "97                    1  \n",
       "98                    0  \n",
       "99                    1  \n",
       "100                   0  \n",
       "\n",
       "[101 rows x 497 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Group_AccToParents'].values # dependant variable\n",
    "X = df.drop(['Group_AccToParents'],axis=1).values   # independant features\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train = sc.transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=0.1, kernel=&#x27;linear&#x27;, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=0.1, kernel=&#x27;linear&#x27;, random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=0.1, kernel='linear', random_state=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC(kernel= 'linear', random_state=1, C=0.1)\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.355\n"
     ]
    }
   ],
   "source": [
    "y_pred = svm.predict(X_test)\n",
    "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 0, 0, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=0, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=0, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=0, solver='liblinear')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='liblinear', random_state=0)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.387\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.56377491e-02  1.54483198e-03 -1.59213948e-01  8.63815008e-02\n",
      "  -8.93552139e-02  1.16779902e-01 -9.62270039e-03 -9.96036121e-02\n",
      "  -1.78624558e-02 -5.41832738e-02 -2.80969793e-02  9.80350481e-02\n",
      "  -2.12823803e-01  4.46138953e-02  1.03831484e-01  9.65786969e-02\n",
      "  -1.19444055e-01 -3.33566928e-02 -2.55017861e-02 -7.95170218e-02\n",
      "  -1.38692049e-01  2.84467040e-02  2.26616599e-02 -2.47218109e-02\n",
      "   1.50522137e-02  2.78673041e-02 -1.37854507e-01 -3.20953535e-01\n",
      "  -7.44751589e-02 -9.65723901e-02  4.47276994e-03 -1.56808103e-02\n",
      "  -1.43189092e-01  2.71386284e-02  5.12089086e-02  1.55237222e-02\n",
      "   1.01957359e-01  2.96517748e-03 -1.18569460e-02  6.84902734e-02\n",
      "  -2.25423007e-03 -5.63001009e-02 -1.75706122e-01  2.60956045e-03\n",
      "  -8.54226009e-02  1.46817906e-01 -8.08954595e-03  5.75979243e-02\n",
      "   8.20560474e-02 -9.92649310e-02 -1.44934984e-01 -1.06985814e-01\n",
      "  -1.62979976e-01  3.14846856e-03  1.77728779e-01  8.82277711e-02\n",
      "   2.93342069e-02 -3.03734341e-02  1.09019872e-02  1.49992025e-02\n",
      "  -8.95133264e-02 -1.78883412e-01  6.04566109e-02  5.58651275e-02\n",
      "   4.15034054e-02 -5.72882832e-02 -3.08357170e-02  1.83419373e-03\n",
      "  -3.37814906e-02  1.25890678e-02 -1.15694817e-01 -7.50246557e-02\n",
      "   7.31523464e-02 -1.70625822e-01  2.43363793e-01 -1.11492500e-01\n",
      "  -9.60360282e-02 -8.85687293e-02  1.05099843e-01  1.70984628e-02\n",
      "  -1.22639836e-02  1.55779991e-02 -3.98426900e-02  1.07286340e-01\n",
      "  -1.27779045e-01 -6.02489336e-03  8.27487230e-02 -1.94552004e-02\n",
      "  -7.49597051e-02 -7.28645804e-03 -1.87147253e-01  3.77525011e-02\n",
      "   3.81150290e-02  2.13660693e-02  1.82333053e-01 -4.97997054e-02\n",
      "   6.94103921e-02 -2.68987441e-01 -6.76436974e-02  3.54627831e-03\n",
      "   1.64897838e-01 -9.95055264e-02  3.61682924e-05 -3.04448243e-01\n",
      "  -1.59086380e-01 -2.49351117e-02  1.11183291e-02 -2.80969196e-02\n",
      "   1.06764889e-01  1.30218358e-02  9.15259597e-02 -2.67738100e-01\n",
      "  -6.90204788e-02 -6.19195880e-02 -1.00196913e-02 -5.95475560e-02\n",
      "  -1.59859629e-01 -2.39732874e-01  4.46583939e-02 -8.29209968e-02\n",
      "  -3.89865077e-02  1.34345202e-02  4.66828100e-02  5.67669227e-02\n",
      "  -5.02438267e-02 -5.37211525e-02 -2.07139080e-01 -1.23333295e-01\n",
      "   8.77580805e-03  1.34593053e-01 -5.31049825e-02  1.41531411e-02\n",
      "  -1.17061030e-01 -1.00355539e-01  7.01356702e-03  1.28724263e-01\n",
      "  -7.16716014e-02 -8.57139945e-02 -1.42707483e-01  6.11385813e-02\n",
      "  -3.35042010e-02 -1.80003492e-01 -2.53730942e-01 -1.24223779e-01\n",
      "   1.50611038e-01 -1.10061518e-01 -9.21762752e-02 -1.20396671e-01\n",
      "   7.90218359e-02  7.29821715e-02  4.17581112e-03 -2.84922439e-03\n",
      "  -3.44294001e-01 -5.85920657e-02 -3.53429968e-02 -1.27492502e-01\n",
      "  -5.28082274e-02  3.46098681e-02  2.79289793e-03  8.06866701e-02\n",
      "   1.87266830e-01  1.82881621e-01  9.31920882e-02 -3.04421617e-02\n",
      "   7.50921734e-02  2.63932942e-01 -9.51948919e-02 -4.60820587e-02\n",
      "  -2.88158093e-02  3.91388071e-02  1.58739242e-01 -5.43315462e-02\n",
      "  -5.32773880e-02 -8.55404721e-02 -5.20659650e-02 -1.08631236e-02\n",
      "  -1.54456591e-01  3.96309179e-02 -2.02812114e-02 -1.56518821e-02\n",
      "   7.88255447e-02 -1.00139398e-02 -2.97857935e-02  1.64481746e-01\n",
      "  -1.28440808e-03 -5.40313462e-05  4.40326647e-02 -4.76050862e-02\n",
      "  -9.52226372e-03 -1.06378219e-02  1.21316722e-01 -5.30576027e-02\n",
      "  -1.40204965e-02  1.99671931e-02  8.36799150e-02  2.69517153e-02\n",
      "   1.51164558e-02 -7.70832783e-02  4.03905117e-02  5.93431122e-02\n",
      "   1.25009785e-01  1.34827887e-01  1.06779586e-01  7.48845412e-02\n",
      "   2.15525572e-02  8.08431606e-02  1.37580967e-01 -2.78172460e-02\n",
      "  -4.54250177e-03  1.59962032e-01 -8.08304958e-02 -6.73906995e-02\n",
      "  -8.30735030e-02 -3.98288804e-02 -1.48696244e-01  1.43832689e-01\n",
      "   1.84206034e-01  1.13857558e-01  1.55075495e-01  7.56036317e-02\n",
      "   2.55356699e-01 -6.96476790e-02  6.34403304e-02 -6.01053103e-02\n",
      "   8.58938319e-02  3.50807632e-02  1.94896059e-02 -6.74761931e-03\n",
      "   3.79446431e-02  3.75556149e-02 -3.21879569e-02  5.23384796e-03\n",
      "  -6.72900449e-02  6.40891182e-02 -8.53236012e-02  1.18767938e-01\n",
      "   2.11930519e-01  9.94378137e-02  4.10740843e-02 -2.17335862e-01\n",
      "   2.30182439e-01  1.20487751e-01  4.18591344e-02 -2.62640498e-01\n",
      "   5.96624232e-02 -3.24333319e-02 -1.60670812e-01 -1.55373930e-01\n",
      "  -2.71422899e-01 -1.96965509e-01 -2.22031216e-01 -7.53698372e-02\n",
      "  -1.19034717e-01  4.37972001e-02 -1.21239076e-02  7.19857029e-02\n",
      "   2.17715000e-02  3.55424860e-02  4.46298422e-02 -3.72341618e-02\n",
      "  -1.45253570e-01  1.63559345e-01 -1.41729995e-01  7.06405416e-02\n",
      "   8.02091031e-02  2.52933797e-01  3.18782636e-01  1.75294948e-01\n",
      "   1.37770597e-01  8.62386305e-02  2.28409163e-01  1.10944283e-01\n",
      "  -1.61424874e-01  2.84320340e-02  6.96031887e-02 -1.12226856e-01\n",
      "  -1.15511763e-01 -8.92521087e-02 -7.73001970e-02  4.57491219e-02\n",
      "   1.06512391e-01  2.76536822e-01 -1.89243539e-01  1.42573424e-01\n",
      "   1.05259432e-01  1.86942993e-01  4.24003657e-02  1.37612562e-01\n",
      "   1.74405053e-01  1.23433437e-01 -1.03119304e-01 -1.60577396e-01\n",
      "  -7.19888786e-02 -7.13785483e-02 -1.63585890e-01 -9.65670273e-02\n",
      "   7.94384194e-02  8.49971483e-02  2.91467630e-02 -6.49662810e-02\n",
      "  -6.54056682e-02 -1.10709772e-01  3.90881483e-03  3.69165566e-02\n",
      "  -1.06214108e-01  1.32831494e-01 -7.95151659e-03  1.59975563e-01\n",
      "   1.18113970e-01  1.86932790e-01  6.54180123e-02  3.35296180e-01\n",
      "   4.35522647e-02  1.92880670e-01  3.07098883e-02  6.66295454e-02\n",
      "  -3.49729296e-03  3.98689501e-02  2.41485343e-02 -1.08008403e-02\n",
      "   2.97403623e-02  1.18388710e-01  2.11506172e-01  5.18132433e-02\n",
      "   2.30720842e-01  1.57725000e-01  3.06264362e-02 -1.33595169e-01\n",
      "  -1.47198713e-02  6.65389269e-02  1.72050880e-01 -1.17065511e-01\n",
      "   6.88002153e-02  2.34351969e-02  1.82804939e-01 -1.02777268e-01\n",
      "  -2.57138887e-02 -9.74498776e-03 -8.37742026e-02  2.99850585e-02\n",
      "  -9.25079407e-03  7.91873583e-03 -4.54182910e-02 -1.41665878e-01\n",
      "  -3.86626353e-02 -7.39550830e-02  1.36746695e-01  1.43805239e-02\n",
      "  -1.42237157e-02  1.57640508e-01 -6.45546997e-02 -2.39257314e-02\n",
      "  -7.36553278e-02  2.05295750e-01  1.31677041e-01  7.13114215e-03\n",
      "   7.85388050e-02  6.91910169e-02  7.83400609e-02  1.01392005e-01\n",
      "   2.60833449e-01 -2.12345846e-02  9.98171331e-02  1.88643888e-01\n",
      "   1.55596741e-01  6.57999356e-02  1.04332708e-01  8.68606165e-02\n",
      "  -1.10207348e-02  8.66342254e-02  1.60859249e-02  1.44550763e-02\n",
      "   2.06486414e-01 -6.03662070e-02 -1.53486866e-02 -1.26918917e-01\n",
      "  -1.47339984e-02  4.15422170e-03 -3.10014793e-02  1.78848418e-01\n",
      "  -2.48722542e-01  9.27813671e-03 -5.51132902e-02  4.46391688e-02\n",
      "  -6.85386361e-02 -4.40990970e-02 -3.08809132e-02  1.44843940e-01\n",
      "  -1.75950160e-02  1.51479172e-02  5.23861847e-02  3.62747855e-02\n",
      "  -9.08088970e-02  2.17938865e-03  5.57411906e-02  3.97423045e-02\n",
      "   1.38919563e-02  2.16058630e-02  6.84168477e-02 -3.15699385e-02\n",
      "  -8.40774473e-02 -1.11730467e-01 -1.28920120e-01 -1.40879490e-01\n",
      "  -6.41037582e-02 -3.08149015e-01  6.88057245e-03 -2.18097259e-02\n",
      "   3.55625019e-02  4.30963250e-02 -2.57762444e-02  2.06607673e-02\n",
      "  -4.29444763e-03 -8.20085012e-02 -3.44387293e-02  5.55840775e-02\n",
      "   1.72799797e-02  2.48736591e-02  3.32634409e-02  8.99367433e-02\n",
      "   2.74859904e-02  2.76851063e-02  6.52402340e-02 -2.09214015e-02\n",
      "  -4.24152590e-02 -9.90693741e-02  9.28122442e-02  6.33037171e-03\n",
      "  -2.02112086e-02  6.27323482e-02 -3.52906207e-02 -8.60843129e-02\n",
      "   8.32553736e-02  1.05431185e-01  4.00921491e-02  6.79297754e-02\n",
      "   6.54347677e-03 -7.15665001e-03  8.77376357e-03 -6.76608226e-02\n",
      "  -3.41744297e-02 -1.04223157e-01  1.86826958e-01 -5.03610414e-02\n",
      "   5.76923150e-02  1.17050354e-02  3.25684639e-02  3.54911543e-02\n",
      "   3.25256630e-02  1.07571538e-01  1.68023726e-01  8.93209609e-02\n",
      "   1.26647315e-01  7.61642820e-02  2.85663164e-03 -9.31670543e-03\n",
      "  -9.65783610e-02 -9.66954398e-02 -1.62565719e-02  5.05765094e-02\n",
      "  -1.43127407e-01 -5.04298897e-03 -1.75986032e-02 -5.04538188e-02\n",
      "   3.42106412e-03 -9.88868177e-02 -4.22348848e-02  1.49950440e-01\n",
      "   4.42807174e-02 -4.16785020e-02  3.94641610e-02  3.28696217e-02\n",
      "   2.48599762e-03  1.12645193e-01  4.68322558e-02  1.38427006e-01\n",
      "   1.19515727e-01  1.88285319e-02  6.88431915e-02  3.13873733e-02\n",
      "   3.64371934e-02  2.40717771e-02 -9.60970136e-02 -1.42892439e-01\n",
      "  -4.96698322e-02 -9.98963865e-02  7.46649414e-02 -1.46322390e-01\n",
      "   1.14733218e-02 -2.57342568e-03  9.79668842e-02 -9.56565814e-02\n",
      "  -1.50000769e-01 -1.26440322e-01 -1.93266009e-01 -5.13831368e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = tree.DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.387\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
