{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os       # using operating system dependent functionality (folders)\n",
    "import pandas as pd # data analysis and manipulation\n",
    "import numpy as np    # numerical computing (manipulating and performing operations on arrays of data)\n",
    "import copy     # Can Copy and Deepcopy files so original file is untouched.\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../eegyolk') # path to helper functions\n",
    "import helper_functions as hf # library useful for eeg and erp data cleaning\n",
    "import epod_helper\n",
    "import initialization_functions\n",
    "\n",
    "from tensorflow.keras.layers import Conv1D,BatchNormalization,LeakyReLU,MaxPool1D,\\\n",
    "GlobalAveragePooling1D,Dense,Dropout,AveragePooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.backend import clear_session\n",
    "\n",
    "from sklearn.model_selection import GroupKFold,LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('metadata.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eeg_file</th>\n",
       "      <th>ParticipantID</th>\n",
       "      <th>test</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_months</th>\n",
       "      <th>dyslexic_parent</th>\n",
       "      <th>Group_AccToParents</th>\n",
       "      <th>path_eeg</th>\n",
       "      <th>path_epoch</th>\n",
       "      <th>path_eventmarkers</th>\n",
       "      <th>epoch_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105a</td>\n",
       "      <td>105</td>\n",
       "      <td>a</td>\n",
       "      <td>f</td>\n",
       "      <td>17</td>\n",
       "      <td>f</td>\n",
       "      <td>At risk</td>\n",
       "      <td>../../volume-ceph/ePodium_projectfolder/dataset</td>\n",
       "      <td>../../volume-ceph/nadine_storage/processed_epochs</td>\n",
       "      <td>../../volume-ceph/ePodium_projectfolder/events</td>\n",
       "      <td>105a_epo.fif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107a</td>\n",
       "      <td>107</td>\n",
       "      <td>a</td>\n",
       "      <td>f</td>\n",
       "      <td>16</td>\n",
       "      <td>m</td>\n",
       "      <td>At risk</td>\n",
       "      <td>../../volume-ceph/ePodium_projectfolder/dataset</td>\n",
       "      <td>../../volume-ceph/nadine_storage/processed_epochs</td>\n",
       "      <td>../../volume-ceph/ePodium_projectfolder/events</td>\n",
       "      <td>107a_epo.fif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>106a</td>\n",
       "      <td>106</td>\n",
       "      <td>a</td>\n",
       "      <td>m</td>\n",
       "      <td>19</td>\n",
       "      <td>f</td>\n",
       "      <td>At risk</td>\n",
       "      <td>../../volume-ceph/ePodium_projectfolder/dataset</td>\n",
       "      <td>../../volume-ceph/nadine_storage/processed_epochs</td>\n",
       "      <td>../../volume-ceph/ePodium_projectfolder/events</td>\n",
       "      <td>106a_epo.fif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>109a</td>\n",
       "      <td>109</td>\n",
       "      <td>a</td>\n",
       "      <td>m</td>\n",
       "      <td>21</td>\n",
       "      <td>m</td>\n",
       "      <td>At risk</td>\n",
       "      <td>../../volume-ceph/ePodium_projectfolder/dataset</td>\n",
       "      <td>../../volume-ceph/nadine_storage/processed_epochs</td>\n",
       "      <td>../../volume-ceph/ePodium_projectfolder/events</td>\n",
       "      <td>109a_epo.fif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110a</td>\n",
       "      <td>110</td>\n",
       "      <td>a</td>\n",
       "      <td>m</td>\n",
       "      <td>17</td>\n",
       "      <td>m</td>\n",
       "      <td>At risk</td>\n",
       "      <td>../../volume-ceph/ePodium_projectfolder/dataset</td>\n",
       "      <td>../../volume-ceph/nadine_storage/processed_epochs</td>\n",
       "      <td>../../volume-ceph/ePodium_projectfolder/events</td>\n",
       "      <td>110a_epo.fif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  eeg_file  ParticipantID test sex  age_months dyslexic_parent  \\\n",
       "0     105a            105    a   f          17               f   \n",
       "1     107a            107    a   f          16               m   \n",
       "2     106a            106    a   m          19               f   \n",
       "3     109a            109    a   m          21               m   \n",
       "4     110a            110    a   m          17               m   \n",
       "\n",
       "  Group_AccToParents                                         path_eeg  \\\n",
       "0            At risk  ../../volume-ceph/ePodium_projectfolder/dataset   \n",
       "1            At risk  ../../volume-ceph/ePodium_projectfolder/dataset   \n",
       "2            At risk  ../../volume-ceph/ePodium_projectfolder/dataset   \n",
       "3            At risk  ../../volume-ceph/ePodium_projectfolder/dataset   \n",
       "4            At risk  ../../volume-ceph/ePodium_projectfolder/dataset   \n",
       "\n",
       "                                          path_epoch  \\\n",
       "0  ../../volume-ceph/nadine_storage/processed_epochs   \n",
       "1  ../../volume-ceph/nadine_storage/processed_epochs   \n",
       "2  ../../volume-ceph/nadine_storage/processed_epochs   \n",
       "3  ../../volume-ceph/nadine_storage/processed_epochs   \n",
       "4  ../../volume-ceph/nadine_storage/processed_epochs   \n",
       "\n",
       "                                path_eventmarkers    epoch_file  \n",
       "0  ../../volume-ceph/ePodium_projectfolder/events  105a_epo.fif  \n",
       "1  ../../volume-ceph/ePodium_projectfolder/events  107a_epo.fif  \n",
       "2  ../../volume-ceph/ePodium_projectfolder/events  106a_epo.fif  \n",
       "3  ../../volume-ceph/ePodium_projectfolder/events  109a_epo.fif  \n",
       "4  ../../volume-ceph/ePodium_projectfolder/events  110a_epo.fif  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['Group_AccToParents'] = np.where(\n",
    "    (metadata['Group_AccToParents']=='At risk'), 1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_files= metadata.loc[metadata['Group_AccToParents'] == 0]\n",
    "atrisk_files = metadata.loc[metadata['Group_AccToParents'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_filtered_data(metadata, to_array=False, verbose=False):\n",
    "    epochs = []\n",
    "    for index, file in metadata.iterrows():\n",
    "        print(f\"Checking out file: {file['epoch_file']}\")\n",
    "        path = os.path.join(file['path_epoch'], file['epoch_file'])\n",
    "        epoch = mne.read_epochs(path, preload=False, verbose=verbose)\n",
    "        if to_array ==True: \n",
    "            epoch = epoch.get_data()\n",
    "        epochs.append(epoch)\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking out file: 117a_epo.fif\n",
      "Loading data for 2435 events and 2049 original time points ...\n",
      "Checking out file: 118a_epo.fif\n",
      "Loading data for 2418 events and 2049 original time points ...\n",
      "Checking out file: 119a_epo.fif\n",
      "Loading data for 2325 events and 2049 original time points ...\n",
      "Checking out file: 124a_epo.fif\n",
      "Loading data for 2402 events and 2049 original time points ...\n",
      "Checking out file: 127a_epo.fif\n",
      "Loading data for 2367 events and 2049 original time points ...\n",
      "Checking out file: 126a_epo.fif\n",
      "Loading data for 2333 events and 2049 original time points ...\n",
      "Checking out file: 131a_epo.fif\n",
      "Loading data for 1628 events and 2049 original time points ...\n",
      "Checking out file: 135a_epo.fif\n",
      "Loading data for 2440 events and 2049 original time points ...\n",
      "Checking out file: 133a_epo.fif\n",
      "Loading data for 1628 events and 2049 original time points ...\n",
      "Checking out file: 138a_epo.fif\n",
      "Loading data for 1585 events and 2049 original time points ...\n",
      "Checking out file: 139a_epo.fif\n",
      "Loading data for 2398 events and 2049 original time points ...\n",
      "Checking out file: 144a_epo.fif\n",
      "Loading data for 2262 events and 2049 original time points ...\n",
      "Checking out file: 143a_epo.fif\n",
      "Loading data for 2338 events and 2049 original time points ...\n",
      "Checking out file: 146a_epo.fif\n",
      "Loading data for 864 events and 2049 original time points ...\n",
      "Checking out file: 145a_epo.fif\n",
      "Loading data for 2394 events and 2049 original time points ...\n",
      "Checking out file: 154a_epo.fif\n",
      "Loading data for 2417 events and 2049 original time points ...\n",
      "Checking out file: 153a_epo.fif\n",
      "Loading data for 1937 events and 2049 original time points ...\n",
      "Checking out file: 168a_epo.fif\n",
      "Loading data for 2429 events and 2049 original time points ...\n",
      "Checking out file: 177a_epo.fif\n",
      "Loading data for 2426 events and 2049 original time points ...\n",
      "Checking out file: 190a_epo.fif\n",
      "Loading data for 2412 events and 2049 original time points ...\n",
      "Checking out file: 170a_epo.fif\n",
      "Loading data for 866 events and 2049 original time points ...\n",
      "Checking out file: 174a_epo.fif\n",
      "Loading data for 2417 events and 2049 original time points ...\n",
      "Checking out file: 191a_epo.fif\n",
      "Loading data for 2430 events and 2049 original time points ...\n",
      "Checking out file: 169a_epo.fif\n",
      "Loading data for 2318 events and 2049 original time points ...\n",
      "Checking out file: 173a_epo.fif\n",
      "Loading data for 2264 events and 2049 original time points ...\n",
      "Checking out file: 166a_epo.fif\n",
      "Loading data for 2511 events and 2049 original time points ...\n",
      "Checking out file: 216a_epo.fif\n",
      "Loading data for 2440 events and 2049 original time points ...\n",
      "Checking out file: 175a_epo.fif\n",
      "Loading data for 2396 events and 2049 original time points ...\n",
      "Checking out file: 172a_epo.fif\n",
      "Loading data for 2202 events and 2049 original time points ...\n",
      "Checking out file: 183a_epo.fif\n",
      "Loading data for 2440 events and 2049 original time points ...\n",
      "Checking out file: 185a_epo.fif\n",
      "Loading data for 2431 events and 2049 original time points ...\n",
      "Checking out file: 187a_epo.fif\n",
      "Loading data for 2347 events and 2049 original time points ...\n",
      "Checking out file: 212a_epo.fif\n",
      "Loading data for 2334 events and 2049 original time points ...\n",
      "Checking out file: 215a_epo.fif\n",
      "Loading data for 2191 events and 2049 original time points ...\n",
      "Checking out file: 167a_epo.fif\n",
      "Loading data for 2388 events and 2049 original time points ...\n",
      "Checking out file: 196a_epo.fif\n",
      "Loading data for 2431 events and 2049 original time points ...\n",
      "Checking out file: 199a_epo.fif\n",
      "Loading data for 2416 events and 2049 original time points ...\n",
      "Checking out file: 180a_epo.fif\n",
      "Loading data for 2371 events and 2049 original time points ...\n",
      "Checking out file: 171a_epo.fif\n",
      "Loading data for 2102 events and 2049 original time points ...\n",
      "Checking out file: 176a_epo.fif\n",
      "Loading data for 2437 events and 2049 original time points ...\n",
      "Checking out file: 200a_epo.fif\n",
      "Loading data for 1965 events and 2049 original time points ...\n",
      "Checking out file: 220a_epo.fif\n",
      "Loading data for 1751 events and 2049 original time points ...\n",
      "Checking out file: 218a_epo.fif\n",
      "Loading data for 2257 events and 2049 original time points ...\n",
      "Checking out file: 205a_epo.fif\n",
      "Loading data for 2437 events and 2049 original time points ...\n",
      "Checking out file: 204a_epo.fif\n",
      "Loading data for 2418 events and 2049 original time points ...\n",
      "Checking out file: 206a_epo.fif\n",
      "Loading data for 2402 events and 2049 original time points ...\n",
      "Checking out file: 208a_epo.fif\n",
      "Loading data for 2338 events and 2049 original time points ...\n",
      "Checking out file: 202a_epo.fif\n",
      "Loading data for 2180 events and 2049 original time points ...\n",
      "Checking out file: 221a_epo.fif\n",
      "Loading data for 2439 events and 2049 original time points ...\n"
     ]
    }
   ],
   "source": [
    "control_epochs = initialization_functions.read_filtered_data(control_files, to_array=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking out file: 105a_epo.fif\n",
      "Loading data for 2425 events and 2049 original time points ...\n",
      "Checking out file: 107a_epo.fif\n",
      "Loading data for 2421 events and 2049 original time points ...\n",
      "Checking out file: 106a_epo.fif\n",
      "Loading data for 901 events and 2049 original time points ...\n",
      "Checking out file: 109a_epo.fif\n",
      "Loading data for 2385 events and 2049 original time points ...\n",
      "Checking out file: 110a_epo.fif\n",
      "Loading data for 2334 events and 2049 original time points ...\n",
      "Checking out file: 112a_epo.fif\n",
      "Loading data for 2395 events and 2049 original time points ...\n",
      "Checking out file: 111a_epo.fif\n",
      "Loading data for 2406 events and 2049 original time points ...\n",
      "Checking out file: 114a_epo.fif\n",
      "Loading data for 2114 events and 2049 original time points ...\n",
      "Checking out file: 115a_epo.fif\n",
      "Loading data for 2439 events and 2049 original time points ...\n",
      "Checking out file: 116a_epo.fif\n",
      "Loading data for 2158 events and 2049 original time points ...\n",
      "Checking out file: 123a_epo.fif\n",
      "Loading data for 1891 events and 2049 original time points ...\n",
      "Checking out file: 122a_epo.fif\n",
      "Loading data for 2221 events and 2049 original time points ...\n",
      "Checking out file: 125a_epo.fif\n",
      "Loading data for 1690 events and 2049 original time points ...\n",
      "Checking out file: 130a_epo.fif\n",
      "Loading data for 2440 events and 2049 original time points ...\n",
      "Checking out file: 128a_epo.fif\n",
      "Loading data for 2438 events and 2049 original time points ...\n",
      "Checking out file: 129a_epo.fif\n",
      "Loading data for 1736 events and 2049 original time points ...\n",
      "Checking out file: 137a_epo.fif\n",
      "Loading data for 2014 events and 2049 original time points ...\n",
      "Checking out file: 141a_epo.fif\n",
      "Loading data for 1210 events and 2049 original time points ...\n",
      "Checking out file: 142a_epo.fif\n",
      "Loading data for 2440 events and 2049 original time points ...\n",
      "Checking out file: 140a_epo.fif\n",
      "Loading data for 1794 events and 2049 original time points ...\n",
      "Checking out file: 148a_epo.fif\n",
      "Loading data for 2436 events and 2049 original time points ...\n",
      "Checking out file: 149a_epo.fif\n",
      "Loading data for 2435 events and 2049 original time points ...\n",
      "Checking out file: 155a_epo.fif\n",
      "Loading data for 2368 events and 2049 original time points ...\n",
      "Checking out file: 157a_epo.fif\n",
      "Loading data for 2440 events and 2049 original time points ...\n",
      "Checking out file: 158a_epo.fif\n",
      "Loading data for 2440 events and 2049 original time points ...\n",
      "Checking out file: 161a_epo.fif\n",
      "Loading data for 2112 events and 2049 original time points ...\n",
      "Checking out file: 159a_epo.fif\n",
      "Loading data for 2152 events and 2049 original time points ...\n",
      "Checking out file: 156a_epo.fif\n",
      "Loading data for 2440 events and 2049 original time points ...\n",
      "Checking out file: 192a_epo.fif\n",
      "Loading data for 2239 events and 2049 original time points ...\n"
     ]
    }
   ],
   "source": [
    "atrisk_epochs = initialization_functions.read_filtered_data(atrisk_files, to_array=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'control_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m control_labels \u001b[38;5;241m=\u001b[39m \u001b[43mcontrol_files\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGroup_AccToParents\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      2\u001b[0m atrisk_labels \u001b[38;5;241m=\u001b[39m atrisk_files[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGroup_AccToParents\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'control_files' is not defined"
     ]
    }
   ],
   "source": [
    "control_labels = control_files['Group_AccToParents'][:3].tolist()\n",
    "atrisk_labels = atrisk_files['Group_AccToParents'][:3].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_labels=[len(i)*[0] for i in control_epochs]\n",
    "atrisk_labels=[len(i)*[1] for i in atrisk_epochs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = control_epochs+atrisk_epochs\n",
    "label_list = control_labels+atrisk_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list to \n",
    "groups_list=[[i]*len(j) for i, j in enumerate(data_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array=np.vstack(data_list)\n",
    "label_array=np.hstack(label_list)\n",
    "group_array=np.hstack(groups_list)\n",
    "data_array=np.moveaxis(data_array,1,2)\n",
    "\n",
    "print(data_array.shape,label_array.shape,group_array.shape) #number of segments, length, channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def input_ts_prep(epoch, standard_events, deviant_events): \n",
    "#    print('checkpoint')\n",
    "#    std_evoked = epoch[standard_events].average() \n",
    "#    dev_evoked = epoch[deviant_events].average()\n",
    "#\n",
    "#    # calculate the mismatch response between standard and deviant evoked\n",
    "#    evoked_diff = mne.combine_evoked([std_evoked, dev_evoked], weights=[1, -1])#.get_data() # mismatch for all channels per participant\n",
    "#        \n",
    "#  \n",
    "#    return evoked_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_events = ['GiepM_S'] # standards: 'GiepM_S','GiepS_S','GopM_S','GopS_S'\n",
    "deviant_events = ['GiepM_D'] # deviants: 'GiepM_D','GiepS_D','GopM_D','GopS_D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnnmodel():\n",
    "    clear_session()\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D(filters=5,kernel_size=3,strides=1,input_shape=(2049, 32)))#1\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(MaxPool1D(pool_size=2,strides=2))#2\n",
    "    model.add(Conv1D(filters=5,kernel_size=3,strides=1))#3\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(MaxPool1D(pool_size=2,strides=2))#4\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(filters=5,kernel_size=3,strides=1))#5\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(AveragePooling1D(pool_size=2,strides=2))#6\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(filters=5,kernel_size=3,strides=1))#7\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(AveragePooling1D(pool_size=2,strides=2))#8\n",
    "    model.add(Conv1D(filters=5,kernel_size=3,strides=1))#9\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(GlobalAveragePooling1D())#10\n",
    "    model.add(Dense(1,activation='sigmoid'))#11\n",
    "    \n",
    "    model.compile('adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model=cnnmodel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gkf=GroupKFold(n_splits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=[]\n",
    "for train_index, val_index in gkf.split(data_array, label_array, groups=group_array):\n",
    "    train_features,train_labels=data_array[train_index],label_array[train_index]\n",
    "    val_features,val_labels=data_array[val_index],label_array[val_index]\n",
    "    scaler=StandardScaler()\n",
    "    train_features = scaler.fit_transform(train_features.reshape(-1, train_features.shape[-1])).reshape(train_features.shape)\n",
    "    val_features = scaler.transform(val_features.reshape(-1, val_features.shape[-1])).reshape(val_features.shape)\n",
    "    model=cnnmodel()\n",
    "    model.fit(train_features,train_labels,epochs=50,batch_size=32,validation_data=(val_features,val_labels))\n",
    "    accuracy.append(model.evaluate(val_features,val_labels)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.mean(accuracy)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Garbage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_epoch = []\n",
    "for epoch in epochs:\n",
    "    arr_epoch = input_ts_prep(epoch, standard_events, deviant_events)\n",
    "    tot_epoch.append(arr_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tot_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = epochs[6].get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape #no of epochs, channels, length of signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
