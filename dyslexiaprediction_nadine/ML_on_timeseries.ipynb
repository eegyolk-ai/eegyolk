{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os       # using operating system dependent functionality (folders)\n",
    "import pandas as pd # data analysis and manipulation\n",
    "import numpy as np    # numerical computing (manipulating and performing operations on arrays of data)\n",
    "import copy     # Can Copy and Deepcopy files so original file is untouched.\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../eegyolk') # path to helper functions\n",
    "import helper_functions as hf # library useful for eeg and erp data cleaning\n",
    "import epod_helper\n",
    "import initialization_functions\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('metadata.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eeg_file</th>\n",
       "      <th>ParticipantID</th>\n",
       "      <th>test</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_months</th>\n",
       "      <th>dyslexic_parent</th>\n",
       "      <th>Group_AccToParents</th>\n",
       "      <th>path_eeg</th>\n",
       "      <th>path_epoch</th>\n",
       "      <th>path_eventmarkers</th>\n",
       "      <th>epoch_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101a</td>\n",
       "      <td>101</td>\n",
       "      <td>a</td>\n",
       "      <td>m</td>\n",
       "      <td>20</td>\n",
       "      <td>m</td>\n",
       "      <td>At risk</td>\n",
       "      <td>F:/Stage/ePODIUM/Data/ePodium_projectfolder/Da...</td>\n",
       "      <td>F:/Stage/ePODIUM/Data/ePodium_projectfolder/ep...</td>\n",
       "      <td>F:/Stage/ePODIUM/Data/ePodium_projectfolder/ev...</td>\n",
       "      <td>101a_epo.fif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102a</td>\n",
       "      <td>102</td>\n",
       "      <td>a</td>\n",
       "      <td>f</td>\n",
       "      <td>20</td>\n",
       "      <td>Nee</td>\n",
       "      <td>Control</td>\n",
       "      <td>F:/Stage/ePODIUM/Data/ePodium_projectfolder/Da...</td>\n",
       "      <td>F:/Stage/ePODIUM/Data/ePodium_projectfolder/ep...</td>\n",
       "      <td>F:/Stage/ePODIUM/Data/ePodium_projectfolder/ev...</td>\n",
       "      <td>102a_epo.fif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103a</td>\n",
       "      <td>103</td>\n",
       "      <td>a</td>\n",
       "      <td>f</td>\n",
       "      <td>20</td>\n",
       "      <td>m</td>\n",
       "      <td>At risk</td>\n",
       "      <td>F:/Stage/ePODIUM/Data/ePodium_projectfolder/Da...</td>\n",
       "      <td>F:/Stage/ePODIUM/Data/ePodium_projectfolder/ep...</td>\n",
       "      <td>F:/Stage/ePODIUM/Data/ePodium_projectfolder/ev...</td>\n",
       "      <td>103a_epo.fif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104a</td>\n",
       "      <td>104</td>\n",
       "      <td>a</td>\n",
       "      <td>m</td>\n",
       "      <td>18</td>\n",
       "      <td>f</td>\n",
       "      <td>At risk</td>\n",
       "      <td>F:/Stage/ePODIUM/Data/ePodium_projectfolder/Da...</td>\n",
       "      <td>F:/Stage/ePODIUM/Data/ePodium_projectfolder/ep...</td>\n",
       "      <td>F:/Stage/ePODIUM/Data/ePodium_projectfolder/ev...</td>\n",
       "      <td>104a_epo.fif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105a</td>\n",
       "      <td>105</td>\n",
       "      <td>a</td>\n",
       "      <td>f</td>\n",
       "      <td>17</td>\n",
       "      <td>f</td>\n",
       "      <td>At risk</td>\n",
       "      <td>F:/Stage/ePODIUM/Data/ePodium_projectfolder/Da...</td>\n",
       "      <td>F:/Stage/ePODIUM/Data/ePodium_projectfolder/ep...</td>\n",
       "      <td>F:/Stage/ePODIUM/Data/ePodium_projectfolder/ev...</td>\n",
       "      <td>105a_epo.fif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  eeg_file  ParticipantID test sex  age_months dyslexic_parent  \\\n",
       "0     101a            101    a   m          20               m   \n",
       "1     102a            102    a   f          20             Nee   \n",
       "2     103a            103    a   f          20               m   \n",
       "3     104a            104    a   m          18               f   \n",
       "4     105a            105    a   f          17               f   \n",
       "\n",
       "  Group_AccToParents                                           path_eeg  \\\n",
       "0            At risk  F:/Stage/ePODIUM/Data/ePodium_projectfolder/Da...   \n",
       "1            Control  F:/Stage/ePODIUM/Data/ePodium_projectfolder/Da...   \n",
       "2            At risk  F:/Stage/ePODIUM/Data/ePodium_projectfolder/Da...   \n",
       "3            At risk  F:/Stage/ePODIUM/Data/ePodium_projectfolder/Da...   \n",
       "4            At risk  F:/Stage/ePODIUM/Data/ePodium_projectfolder/Da...   \n",
       "\n",
       "                                          path_epoch  \\\n",
       "0  F:/Stage/ePODIUM/Data/ePodium_projectfolder/ep...   \n",
       "1  F:/Stage/ePODIUM/Data/ePodium_projectfolder/ep...   \n",
       "2  F:/Stage/ePODIUM/Data/ePodium_projectfolder/ep...   \n",
       "3  F:/Stage/ePODIUM/Data/ePodium_projectfolder/ep...   \n",
       "4  F:/Stage/ePODIUM/Data/ePodium_projectfolder/ep...   \n",
       "\n",
       "                                   path_eventmarkers    epoch_file  \n",
       "0  F:/Stage/ePODIUM/Data/ePodium_projectfolder/ev...  101a_epo.fif  \n",
       "1  F:/Stage/ePODIUM/Data/ePodium_projectfolder/ev...  102a_epo.fif  \n",
       "2  F:/Stage/ePODIUM/Data/ePodium_projectfolder/ev...  103a_epo.fif  \n",
       "3  F:/Stage/ePODIUM/Data/ePodium_projectfolder/ev...  104a_epo.fif  \n",
       "4  F:/Stage/ePODIUM/Data/ePodium_projectfolder/ev...  105a_epo.fif  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['Group_AccToParents'] = np.where(\n",
    "    (metadata['Group_AccToParents']=='At risk'), 1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_files = [\"102a\",\"113a\", \"107b (deel 1+2)\", \"132a\", \"121b(2)\", \"113b\", \"107b (deel 3+4)\", \"147a\",\n",
    "                \"121a\", \"134a\", \"143b\", \"121b(1)\", \"145b\", \"150a\",\"152a\", \"184a\", \"165a\", \"151a\", \"163a\", \"179a\",\"179b\", \"182b\", \"186a\", \"193b\"]\n",
    "\n",
    "metadata = metadata[~metadata['eeg_file'].isin(drop_files)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_files= metadata.loc[metadata['Group_AccToParents'] == 0]\n",
    "atrisk_files = metadata.loc[metadata['Group_AccToParents'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_filtered_data(metadata, to_array=False, verbose=False):\n",
    "    epochs = []\n",
    "    for index, file in metadata.iterrows():\n",
    "        print(f\"Checking out file: {file['epoch_file']}\")\n",
    "        path = os.path.join(file['path_epoch'], file['epoch_file'])\n",
    "        epoch = mne.read_epochs(path, preload=False, verbose=verbose)\n",
    "        if to_array ==True: \n",
    "            epoch = epoch.get_data()\n",
    "        epochs.append(epoch)\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking out file: 117a_epo.fif\n",
      "Loading data for 2435 events and 2049 original time points ...\n",
      "Checking out file: 118a_epo.fif\n",
      "Loading data for 2418 events and 2049 original time points ...\n",
      "Checking out file: 119a_epo.fif\n",
      "Loading data for 2325 events and 2049 original time points ...\n"
     ]
    }
   ],
   "source": [
    "control_epochs = initialization_functions.read_filtered_data(control_files[:3], to_array=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking out file: 101a_epo.fif\n",
      "Loading data for 2085 events and 2049 original time points ...\n",
      "Checking out file: 103a_epo.fif\n",
      "Loading data for 837 events and 2049 original time points ...\n",
      "Checking out file: 104a_epo.fif\n",
      "Loading data for 2293 events and 2049 original time points ...\n"
     ]
    }
   ],
   "source": [
    "atrisk_epochs = initialization_functions.read_filtered_data(atrisk_files[:3], to_array=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_labels = control_files['Group_AccToParents'][:3].tolist()\n",
    "atrisk_labels = atrisk_files['Group_AccToParents'][:3].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_labels=[len(i)*[0] for i in control_epochs]\n",
    "atrisk_labels=[len(i)*[1] for i in atrisk_epochs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = control_epochs+atrisk_epochs\n",
    "label_list = control_labels+atrisk_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list to \n",
    "groups_list=[[i]*len(j) for i, j in enumerate(data_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12393, 2049, 32) (12393,) (12393,)\n"
     ]
    }
   ],
   "source": [
    "data_array=np.vstack(data_list)\n",
    "label_array=np.hstack(label_list)\n",
    "group_array=np.hstack(groups_list)\n",
    "data_array=np.moveaxis(data_array,1,2)\n",
    "\n",
    "print(data_array.shape,label_array.shape,group_array.shape) #number of segments, length, channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_array=np.hstack(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def input_ts_prep(epoch, standard_events, deviant_events): \n",
    "#    print('checkpoint')\n",
    "#    std_evoked = epoch[standard_events].average() \n",
    "#    dev_evoked = epoch[deviant_events].average()\n",
    "#\n",
    "#    # calculate the mismatch response between standard and deviant evoked\n",
    "#    evoked_diff = mne.combine_evoked([std_evoked, dev_evoked], weights=[1, -1])#.get_data() # mismatch for all channels per participant\n",
    "#        \n",
    "#  \n",
    "#    return evoked_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_events = ['GiepM_S'] # standards: 'GiepM_S','GiepS_S','GopM_S','GopS_S'\n",
    "deviant_events = ['GiepM_D'] # deviants: 'GiepM_D','GiepS_D','GopM_D','GopS_D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 6248, 5)           290       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 6248, 5)          20        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 6248, 5)           0         \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 3124, 5)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 3122, 5)           80        \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 3122, 5)           0         \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 1561, 5)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1561, 5)           0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 1559, 5)           80        \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 1559, 5)           0         \n",
      "                                                                 \n",
      " average_pooling1d (AverageP  (None, 779, 5)           0         \n",
      " ooling1D)                                                       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 779, 5)            0         \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 777, 5)            80        \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 777, 5)            0         \n",
      "                                                                 \n",
      " average_pooling1d_1 (Averag  (None, 388, 5)           0         \n",
      " ePooling1D)                                                     \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 386, 5)            80        \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 386, 5)            0         \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 5)                0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 636\n",
      "Trainable params: 626\n",
      "Non-trainable params: 10\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv1D,BatchNormalization,LeakyReLU,MaxPool1D,\\\n",
    "GlobalAveragePooling1D,Dense,Dropout,AveragePooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.backend import clear_session\n",
    "def cnnmodel():\n",
    "    clear_session()\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D(filters=5,kernel_size=3,strides=1,input_shape=(6250,19)))#1\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(MaxPool1D(pool_size=2,strides=2))#2\n",
    "    model.add(Conv1D(filters=5,kernel_size=3,strides=1))#3\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(MaxPool1D(pool_size=2,strides=2))#4\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(filters=5,kernel_size=3,strides=1))#5\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(AveragePooling1D(pool_size=2,strides=2))#6\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(filters=5,kernel_size=3,strides=1))#7\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(AveragePooling1D(pool_size=2,strides=2))#8\n",
    "    model.add(Conv1D(filters=5,kernel_size=3,strides=1))#9\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(GlobalAveragePooling1D())#10\n",
    "    model.add(Dense(1,activation='sigmoid'))#11\n",
    "    \n",
    "    model.compile('adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model=cnnmodel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold,LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "gkf=GroupKFold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.19 GiB for an array with shape (2435, 2049, 32) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-48c57c4a8544>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgkf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtrain_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_array\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel_array\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mval_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_labels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_array\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel_array\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mscaler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtrain_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.19 GiB for an array with shape (2435, 2049, 32) and data type float64"
     ]
    }
   ],
   "source": [
    "accuracy=[]\n",
    "for train_index, val_index in gkf.split(data_array, label_array, groups=group_array):\n",
    "    train_features,train_labels=data_array[train_index],label_array[train_index]\n",
    "    val_features,val_labels=data_array[val_index],label_array[val_index]\n",
    "    scaler=StandardScaler()\n",
    "    train_features = scaler.fit_transform(train_features.reshape(-1, train_features.shape[-1])).reshape(train_features.shape)\n",
    "    val_features = scaler.transform(val_features.reshape(-1, val_features.shape[-1])).reshape(val_features.shape)\n",
    "    model=cnnmodel()\n",
    "    model.fit(train_features,train_labels,epochs=50,batch_size=128,validation_data=(val_features,val_labels))\n",
    "    accuracy.append(model.evaluate(val_features,val_labels)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_epoch = []\n",
    "for epoch in epochs:\n",
    "    arr_epoch = input_ts_prep(epoch, standard_events, deviant_events)\n",
    "    tot_epoch.append(arr_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tot_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = epochs[6].get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape #no of epochs, channels, length of signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
