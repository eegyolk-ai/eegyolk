{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive tool for analyzing trained models on the ePodium dataset\n",
    "\n",
    "1. [Input Data](#1ma)\n",
    "2. [Deep Learning Model](#2ma)\n",
    "3. [Make predictions on testing set](#3ma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages\n",
    "Note: This notebook may output tensorflow errors if cuda is not properly installed. The notebook still functions with these errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Local\n",
    "import local_paths\n",
    "from functions import epodium, display_helper\n",
    "from functions.epodium import Epodium\n",
    "from functions.ddp import DDP\n",
    "from functions.train_and_predict import EpodiumSequence, DDPSequence\n",
    "\n",
    "# Models\n",
    "from models.dl_4_tsc import encoder_model, fully_convolutional_model, resnet_model\n",
    "from models.eeg_dl import transformer_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a id=\"1ma\"></a>\n",
    "## 1. Input Data \n",
    "\n",
    "####  Choose which processed data to use\n",
    "Choose from the different processed _experiment_event.npy_ files. If the _local_path.split_ folder is empty, process the raw ePodium files in the _epodium_processing_ notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose between datasets: \"epodium\" \"ddp\"\n",
    "dataset_name = \"ddp\"\n",
    "\n",
    "if dataset_name == \"epodium\":\n",
    "    dataset = Epodium()    \n",
    "    epochs_directory = local_paths.ePod_epochs\n",
    "    event_directory = local_paths.ePod_epochs_events\n",
    "    \n",
    "    epod_labels = dataset.create_labels(local_paths.ePod_metadata)\n",
    "    print(f\"The available labels are:\\n {list(epod_labels.columns)}\")\n",
    "\n",
    "elif dataset_name == \"ddp\":\n",
    "    dataset = DDP()\n",
    "    epochs_directory = local_paths.DDP_epochs\n",
    "    event_directory = local_paths.DDP_epochs_events\n",
    "    \n",
    "    directory_age_metadata = os.path.join(local_paths.DDP_metadata, \"ages\")\n",
    "    ddp_labels = dataset.create_labels(local_paths.DDP_dataset, directory_age_metadata)\n",
    "    print(f\"The available labels are:\\n {list(ddp_labels.columns)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a id=\"2ma\"></a>\n",
    "## 2. Deep Learning Model\n",
    "\n",
    "#### Choose a trained model\n",
    "Choose from the trained models in the _local_paths.models_ folder. If the folder is empty, train a model in the _epodium_model_training_ notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required plot setting\n",
    "%matplotlib inline \n",
    "trained_models = sorted(f for f in os.listdir(os.path.join(local_paths.models)) if not f.startswith(\".\"))\n",
    "\n",
    "model_widget = ipywidgets.RadioButtons(options=trained_models, description='Models:')\n",
    "display(model_widget)\n",
    "\n",
    "history = []\n",
    "                                       \n",
    "def load_model(mod):\n",
    "    base_path = os.path.join(local_paths.models, mod)\n",
    "\n",
    "    path_history = os.path.join(base_path, \"history.npy\")\n",
    "    path_model = os.path.join(base_path, \"model\")\n",
    "    path_testset = os.path.join(base_path, \"subsets\", \"test_set.txt\")\n",
    "    path_weights = os.path.join(base_path, \"weights.h5\")\n",
    "    \n",
    "    global model\n",
    "    global testset\n",
    "    global history\n",
    "\n",
    "    # Load Model\n",
    "    if(os.path.exists(path_model)):\n",
    "        print(f\"Loading Model: '{model_widget.value}'.\")\n",
    "\n",
    "        # Loads the entire model from a folder:\n",
    "        model = tf.keras.models.load_model(path_model)\n",
    "        model.load_weights(path_weights)\n",
    "        # Reads the test-set of the trained model and puts the experiment names into a list:\n",
    "        testset = open(path_testset, \"r\").read().split()\n",
    "        # Loads the training history dictionary:\n",
    "        history = np.load(path_history, allow_pickle=True).item()\n",
    "        \n",
    "        # Show Loss of Training History\n",
    "        display_helper.show_plot(x=range(len(history['loss'][:])), y=history['loss'][:],  ylim=[0,600000], \n",
    "                                 xlabel=\"epochs\", ylabel=\"validation loss\", title=\"Loss during training\")\n",
    "        display_helper.show_plot(x=range(len(history['loss']))[:], y=history['val_loss'][:], ylim=[0,600000],\n",
    "                                 xlabel=\"epochs\", ylabel=\"validation loss\", title=\"Validation loss during training\")\n",
    "        print(f\"The lowest validation loss is {round(min(history['val_loss']), 3)}\")\n",
    "\n",
    "    else: \n",
    "        print(\"Model not found\")\n",
    "\n",
    "out = ipywidgets.interactive_output(load_model, {'mod': model_widget})\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "<a id='3ma'></a>\n",
    "## 3. Make predictions on testing set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_passthroughs = 2\n",
    "results = []\n",
    "\n",
    "# For each experiment in the test-set\n",
    "for i, experiment in enumerate(testset):\n",
    "    test_sequence = DDPSequence([experiment], \n",
    "                                ddp_labels, \n",
    "                                epochs_directory, \n",
    "                                n_experiments_batch=1, \n",
    "                                n_instances_per_experiment=n_passthroughs)\n",
    "    x, y = test_sequence.__getitem__(0)\n",
    "    \n",
    "    # Make a prediction with the model.\n",
    "    real_pred = [y[0], np.squeeze(model.predict(x, verbose=0)).mean()]\n",
    "    results.append(real_pred)\n",
    "    \n",
    "    print(f\"{i+1}/{len(testset)} predicted.\")\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "results = np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_helper.show_plot(np.array(results)[:,0], \n",
    "                         np.array(results)[:,1], \n",
    "                         f\"Age prediction (encoder_age_128_20, passes: {n_passthroughs})\", \n",
    "                         \"Actual age (days)\",\n",
    "                         \"Predicted age (days)\",\n",
    "                         scatter=True,\n",
    "                         show=False)\n",
    "plt.grid()\n",
    "plt.plot([300, 1450], [300, 1450]) # Line where predicted=actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5f6ecf0357e95e30953d0cf08844b8b26fdbdf1f780a6e218131c917612a57e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
