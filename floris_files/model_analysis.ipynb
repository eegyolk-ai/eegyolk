{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive tool for analyzing trained models on the ePodium dataset\n",
    "\n",
    "1. [Input Data](#1ma)\n",
    "2. [Deep Learning Model](#2ma)\n",
    "3. [Make predictions on test-set](#3ma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages\n",
    "Note: This notebook may output tensorflow errors if cuda is not properly installed. The notebook still functions with these errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-30 21:24:51.346015: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-30 21:24:51.346074: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from functions import epodium, epodium_deep_learning, display_helper\n",
    "from models.dnn import fully_connected_model\n",
    "from models.hfawaz import cnn, encoder\n",
    "\n",
    "import local_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a id=\"1ma\"></a>\n",
    "## 1. Input Data \n",
    "\n",
    "####  Choose which processed data to use\n",
    "Choose from the different processed _experiment_event.npy_ files. If the _local_path.split_ folder is empty, process the raw ePodium files in the _epodium_processing_ notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_method = processing_method_widget.value.split('_')[0]\n",
    "experiment_list = epodium_deep_learning.clean_experiments(cleaning_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise the data sequence of a single participant\n",
    "\n",
    "TODO: SHOW Downsample differences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class EvokedDataIterator(Sequence):\n",
    "    \"\"\"\n",
    "        An Iterator Sequence class as input to feed the model.\n",
    "        The next value is given from the __getitem__ function.\n",
    "        For more information, go to:\n",
    "        https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence\n",
    "    \"\"\"    \n",
    "    \n",
    "    def __init__(self, experiments, base_path, n_experiments_batch = 8, n_trials_averaged = 60, gaussian_noise = 0):\n",
    "        self.experiments = experiments\n",
    "        self.n_experiments_batch = n_experiments_batch\n",
    "        self.n_trials_averaged = n_trials_averaged\n",
    "        self.gaussian_noise = gaussian_noise\n",
    "        \n",
    "        metadata_path = os.path.join(local_paths.ePod_metadata, \"children.txt\")\n",
    "        self.metadata = pd.read_table(metadata_path)\n",
    "        \n",
    "        self.base_path = base_path\n",
    "            \n",
    "    # The number of experiments in the entire dataset.\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.experiments)/self.n_experiments_batch))\n",
    "    \n",
    "    def __getitem__(self, index, verbose = False):        \n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        \n",
    "        for i in range(self.n_experiments_batch):\n",
    "            \n",
    "            # Set participant\n",
    "            participant_index = (index * self.n_experiments_batch + i) % len(self.experiments)\n",
    "            participant = self.experiments[participant_index]\n",
    "            participant_id = participant[:3]\n",
    "            participant_metadata = self.metadata.loc[self.metadata['ParticipantID'] == float(participant_id)]\n",
    "            \n",
    "            if(verbose):\n",
    "                print(participant)\n",
    "                \n",
    "            # load participant data\n",
    "            np.load()\n",
    "            \n",
    "            for condition in epodium.conditions:\n",
    "                \n",
    "                # Get Standard and Deviant file\n",
    "                # npy_name_S = f\"{self.experiments[participant_index]}_{condition}_S.npy\"\n",
    "                # npy_name_D = f\"{self.experiments[participant_index]}_{condition}_D.npy\"\n",
    "                # npy_path_S = os.path.join(self.split_path, npy_name_S)\n",
    "                # npy_path_D = os.path.join(self.split_path, npy_name_D)\n",
    "                # npy_S = np.load(npy_path_S)\n",
    "                # npy_D = np.load(npy_path_D)\n",
    "\n",
    "                # Create ERP from averaging 'n_trials_averaged' trials.\n",
    "                trial_indexes_S = np.random.choice(npy_S.shape[0], self.n_trials_averaged, replace=False)\n",
    "                evoked_S = np.mean(npy_S[trial_indexes_S,:,:], axis=0)\n",
    "                trial_indexes_D = np.random.choice(npy_D.shape[0], self.n_trials_averaged, replace=False)\n",
    "                evoked_D = np.mean(npy_D[trial_indexes_D,:,:], axis=0)\n",
    "                \n",
    "                # Merge Standard and Deviant evoked along the channel dimensions.\n",
    "                evoked = np.concatenate((evoked_S, evoked_D))\n",
    "                evoked += np.random.normal(0, self.gaussian_noise, evoked.shape)\n",
    "                x_batch.append(evoked)\n",
    "                \n",
    "                # Binary labels:\n",
    "                # y = np.zeros(2)\n",
    "                # if participant_metadata[\"Sex\"].item() == \"M\" :\n",
    "                #     y[0] = 1\n",
    "                # if participant_metadata[\"Group_AccToParents\"].item() == \"At risk\":\n",
    "                #     y[1] = 1\n",
    "                \n",
    "                if str(participant[-1]) == \"a\":\n",
    "                    y = normalize_age(int(participant_metadata[f\"Age_days_a\"].item()))\n",
    "                if str(participant[-1]) == \"b\":\n",
    "                    try: y = normalize_age(int(participant_metadata[f\"Age_days_b\"].item())) # Not all ages in metadata\n",
    "                    except:  y = normalize_age(int(participant_metadata[f\"Age_days_a\"].item()) + 120)\n",
    "                \n",
    "                y_batch.append(y)\n",
    "        \n",
    "        shuffle_batch = list(zip(x_batch, y_batch))\n",
    "        random.shuffle(shuffle_batch)\n",
    "        x_batch, y_batch = zip(*shuffle_batch)\n",
    "        return np.array(x_batch), np.array(y_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_sequence = EvokedDataIterator([experiment], local_paths.epod_clean, \n",
    "                                          n_experiments_batch=2,\n",
    "                                          n_trials_averaged=30,\n",
    "                                          gaussian_noise=1e-6)\n",
    "participant_sequence.__getitem__(0, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_standard_deviant_instance(experiment, codition, avg, noise):    \n",
    "    # Create data instance\n",
    "    participant_sequence = epodium_deep_learning.EvokedDataIterator([experiment], \n",
    "                                                                    processing_method_widget.value,\n",
    "                                                                    n_experiments_batch=1,\n",
    "                                                                    n_trials_averaged=avg,\n",
    "                                                                    gaussian_noise=noise*1e-6)\n",
    "    x, y = participant_sequence.__getitem__(0)\n",
    "    print(f\"The data instance has shape: {x.shape}\")\n",
    "\n",
    "    # Plot an evoked (ERP) of the specified condition\n",
    "    index = epodium.conditions.index(codition)\n",
    "    display_helper.plot_array_as_evoked(x[index][:32], epodium.channel_names, \n",
    "                                        frequency=128, n_trials=avg, ylim=[-20, 20])\n",
    "    display_helper.plot_array_as_evoked(x[index][32:], epodium.channel_names, \n",
    "                                        frequency=128, n_trials=avg, ylim=[-20, 20])\n",
    "\n",
    "# Create widgets\n",
    "experiment_widget = ipywidgets.Dropdown(options=sorted(experiment_list), description='Experiment:')\n",
    "condition_widget = ipywidgets.RadioButtons(options=epodium.conditions, description='Condition:')\n",
    "n_trials_averaged_widget = ipywidgets.IntSlider(value=60, min=1, max=80,description='Averaged trials:')\n",
    "gaussian_noise_widget = ipywidgets.FloatSlider(value=1, min=0, max=5,description='Noise (micro-volt):')\n",
    "\n",
    "# Widget settings\n",
    "options = {'experiment':experiment_widget, 'codition':condition_widget, 'avg':n_trials_averaged_widget, 'noise':gaussian_noise_widget}\n",
    "ui = ipywidgets.VBox([experiment_widget, condition_widget, n_trials_averaged_widget, gaussian_noise_widget])\n",
    "out = ipywidgets.interactive_output(plot_standard_deviant_instance, options)\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a id=\"2ma\"></a>\n",
    "## 2. Deep Learning Model\n",
    "\n",
    "#### Choose a trained model\n",
    "Choose from the trained models in the _local_paths.models_ folder. If the folder is empty, train a model in the _epodium_model_training_ notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required plot setting\n",
    "%matplotlib inline \n",
    "trained_models = sorted(f for f in os.listdir(local_paths.epod_models) if not f.startswith(\".\"))\n",
    "\n",
    "model_widget = ipywidgets.RadioButtons(options=trained_models, description='Models:')\n",
    "display(model_widget)\n",
    "\n",
    "history = []\n",
    "                                       \n",
    "def load_model(mod):\n",
    "    base_path = os.path.join(local_paths.epod_models, mod)\n",
    "\n",
    "    path_history = os.path.join(base_path, \"history.npy\")\n",
    "    path_model = os.path.join(base_path, \"model\")\n",
    "    path_testset = os.path.join(base_path, \"testset.txt\")\n",
    "    path_weights = os.path.join(base_path, \"weights.h5\")\n",
    "    \n",
    "    global model\n",
    "    global testset\n",
    "    global history\n",
    "\n",
    "    # Load Model\n",
    "    if(os.path.exists(path_model)):\n",
    "        print(f\"Loading Model: '{model_widget.value}'.\")\n",
    "\n",
    "        # Loads the entire model from a folder:\n",
    "        model = tf.keras.models.load_model(path_model)\n",
    "        model.load_weights(path_weights)\n",
    "        # Reads the test-set of the trained model and puts the experiment names into a list:\n",
    "        testset = open(path_testset, \"r\").read().split()\n",
    "        # Loads the training history dictionary:\n",
    "        history = np.load(path_history, allow_pickle=True).item()\n",
    "        \n",
    "        # Show Loss of Training History\n",
    "        display_helper.show_plot(x=range(len(history['loss'][:])), y=history['loss'][:],\n",
    "                                 xlabel=\"epochs\", ylabel=\"validation loss\", title=\"Loss during training\")\n",
    "        display_helper.show_plot(x=range(len(history['loss']))[:], y=history['val_loss'][:],\n",
    "                                 xlabel=\"epochs\", ylabel=\"validation loss\", title=\"Validation loss during training\")\n",
    "        print(f\"The lowest validation loss is {round(min(history['val_loss']), 3)}\")\n",
    "\n",
    "    else: \n",
    "        print(\"Model not found\")\n",
    "\n",
    "out = ipywidgets.interactive_output(load_model, {'mod': model_widget})\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "<a id='3ma'></a>\n",
    "## 3. Make predictions on test-set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_passthroughs = 20 # 1 passtrough gives 4 predictions, 1 for each condition\n",
    "results = []\n",
    "\n",
    "trainset = list(set(experiment_list) - set(testset))\n",
    "\n",
    "# For each experiment in the test-set\n",
    "for experiment in trainset: # TODO: Check difference trainset/testset \n",
    "    test_sequence = epodium_deep_learning.EvokedDataIterator([experiment],\n",
    "                                                             processing_method_widget.value,                                                                    \n",
    "                                                             n_experiments_batch=n_passthroughs,                                                                    \n",
    "                                                             n_trials_averaged=60)\n",
    "\n",
    "    x, y = test_sequence.__getitem__(0)\n",
    "    # print(f\"The data instance has shape: {x.shape}\")\n",
    "\n",
    "    real_pred = [y[0], np.squeeze(model.predict(x, verbose=0)).mean()]\n",
    "    results.append(real_pred)\n",
    "results = np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_helper.show_plot(np.array(results)[:,0], \n",
    "                         np.array(results)[:,1], \n",
    "                         f\"Age prediction on train set (encoder_age_128_20, passes: {n_passthroughs})\", \n",
    "                         \"Actual age (normalized)\",\n",
    "                         \"Predicted age (normalized)\",\n",
    "                         scatter=True,\n",
    "                         show=False)\n",
    "plt.plot([-1, 1], [-1, 1]) # Line where predicted=actual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OLD!\n",
    "\n",
    "\n",
    "#     label_sex = 'male' if y[index][0] else 'female'\n",
    "#     label_dyslexia = 'risk' if y[index][1] else 'no risk'\n",
    "#     label_event = 'deviant' if y[index][2] else 'standard'\n",
    "\n",
    "#     print (f\"{int(y[index][0])}: {label_sex}, \\\n",
    "#          {int(y[index][1])}: {label_risk}, \\\n",
    "#          {int(y[index][2])}: {label_event}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5f6ecf0357e95e30953d0cf08844b8b26fdbdf1f780a6e218131c917612a57e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
