{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Learning Models.\n",
    "\n",
    "In this notebook the Dutch Dyslexia Programme (DDP) and ePodium dataset are used to train a deep neural network model.\n",
    "The model is trained to predict the age and risk of dyslexia.\n",
    "The input data consists of averaged epochs of the EEG data.\n",
    "\n",
    "+ In section 1. [Prepare Dataset](#1mt) ...\n",
    "+ In section 2. [Deep Learning](#2mt) ...\n",
    "\n",
    "It is recommended to run this notebook with CUDA enabled with a dedicated graphics card to speed-up the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, MeanSquaredError\n",
    "from tensorflow.keras.metrics import Precision, BinaryAccuracy, Recall\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import local_paths\n",
    "from functions import processing, display_helper, data_io\n",
    "from functions.epodium import Epodium\n",
    "from functions.ddp import DDP\n",
    "#from functions.train_and_predict import EpodiumSequence, DDPSequence\n",
    "\n",
    "from models import transformer\n",
    "from models.dnn import fully_connected_model\n",
    "from models.hfawaz import cnn, encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='1mt'></a>\n",
    "## 1. Prepare Dataset\n",
    "\n",
    "__input dimensions__: \n",
    "+ x (batches, timesteps, channels)\n",
    "+ y (batches, labels)\n",
    "\n",
    "__labels__: \n",
    "+ Binary: At risk of dyslexia\n",
    "+ Regressive: Age, Vocabulary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The available labels are:\n",
      " ['filename', 'participant', 'age_group', 'age_days']\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"ddp\" # \"epodium\" \"ddp\"\n",
    "\n",
    "if dataset_name == \"epodium\":\n",
    "    dataset = Epodium()    \n",
    "    epochs_directory = local_paths.ePod_epochs\n",
    "    event_directory = local_paths.ePod_epochs_events\n",
    "    \n",
    "    epod_labels = dataset.create_labels(local_paths.ePod_metadata)\n",
    "    print(f\"The available labels are:\\n {list(epod_labels.columns)}\")\n",
    "\n",
    "elif dataset_name == \"ddp\":\n",
    "    dataset = DDP()\n",
    "    epochs_directory = local_paths.DDP_epochs\n",
    "    event_directory = local_paths.DDP_epochs_events\n",
    "    \n",
    "    directory_age_metadata = os.path.join(local_paths.DDP_metadata, \"ages\")\n",
    "    ddp_labels = dataset.create_labels(local_paths.DDP_dataset, directory_age_metadata)\n",
    "    print(f\"The available labels are:\\n {list(ddp_labels.columns)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split processed epochs* into train and test sequence.\n",
    "\n",
    "    *In the context of electroencephalography (EEG), *epochs* are EEG segments in which an event occurs. In the context of deep learning, *epochs* are iterations over the entire training dataset.\n",
    "\n",
    "First choose which processed data to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed: 1057, bad: 238\n",
      "819 experiments have enough epochs for analysis.\n",
      "The dataset is split up into 618 train and 201 test experiments\n"
     ]
    }
   ],
   "source": [
    "experiment_list = processing.valid_experiments(dataset, event_directory, min_standards=180, min_deviants=80)\n",
    "experiments_train, experiments_test = dataset.split_train_test_datasets(experiment_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing data iterator (Sequence) as input to the deep learning models.\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "import random\n",
    "\n",
    "class EpodiumSequence(Sequence):\n",
    "    \"\"\"\n",
    "        An Iterator Sequence class as input to feed the model.\n",
    "        The next value is given from the __getitem__ function.\n",
    "        For more information on Sequences, go to:\n",
    "        https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence\n",
    "\n",
    "        self.labels contains:  ['Participant', 'Age_days_a', 'Age_days_b', 'Risk_of_dyslexia']\n",
    "    \"\"\"    \n",
    "\n",
    "    def __init__(self, experiments, target_labels, epochs_directory, n_experiments_batch=8, n_trials_averaged=30, gaussian_noise=0):\n",
    "        self.experiments = experiments\n",
    "        self.labels = target_labels\n",
    "        self.epochs_directory = epochs_directory\n",
    "        \n",
    "        self.n_experiments_batch = n_experiments_batch\n",
    "        self.n_trials_averaged = n_trials_averaged\n",
    "        self.gaussian_noise = gaussian_noise\n",
    "\n",
    "\n",
    "    # The number of experiments in the entire dataset.\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.experiments)/self.n_experiments_batch))\n",
    "\n",
    "    def __getitem__(self, index, verbose=False):        \n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        \n",
    "        #print(self.labels)\n",
    "\n",
    "        for i in range(self.n_experiments_batch):\n",
    "\n",
    "            # Set participant\n",
    "            experiment_index = (index * self.n_experiments_batch + i) % len(self.experiments)\n",
    "            experiment = self.experiments[experiment_index]\n",
    "            participant = experiment[:3]\n",
    "            participant_labels = self.labels.loc[self.labels['Participant']==float(participant)]\n",
    "\n",
    "            if(verbose):\n",
    "                print(experiment)\n",
    "                \n",
    "            # Load .fif file\n",
    "            path_epochs = os.path.join(epochs_directory, experiment + \"_epo.fif\")\n",
    "            epochs = mne.read_epochs(path_epochs, verbose=0)\n",
    "            print(epochs)\n",
    "            \n",
    "            # A data instance is created for each condition\n",
    "            for condition in ['GiepM', \"GiepS\", \"GopM\", \"GopS\"]:\n",
    "                \n",
    "                standard_event = condition + '_S'\n",
    "                deviant_event = condition + '_D'\n",
    "                npy_S = epochs[standard_event].get_data() # TODO: DDP channels/ePod\n",
    "                npy_D = epochs[deviant_event].get_data()\n",
    "                                \n",
    "                # Create ERP from averaging 'n_trials_averaged' trials.\n",
    "                trial_indexes_S = np.random.choice(npy_S.shape[0], self.n_trials_averaged, replace=False)\n",
    "                evoked_S = np.mean(npy_S[trial_indexes_S,:,:], axis=0)\n",
    "                trial_indexes_D = np.random.choice(npy_D.shape[0], self.n_trials_averaged, replace=False)\n",
    "                evoked_D = np.mean(npy_D[trial_indexes_D,:,:], axis=0)\n",
    "                \n",
    "                x_batch.append(evoked_S)\n",
    "\n",
    "                ## Merge Standard and Deviant evoked along the channel dimensions.\n",
    "                # evoked = np.concatenate((evoked_S, evoked_D))\n",
    "                # evoked += np.random.normal(0, self.gaussian_noise, evoked.shape)\n",
    "                # x_batch.append(evoked)\n",
    "\n",
    "                # Binary labels:\n",
    "                # y = np.zeros(2)\n",
    "                # if participant_labels[\"Sex\"].item() == \"M\" :\n",
    "                #     y[0] = 1\n",
    "                # if participant_labels[\"Group_AccToParents\"].item() == \"At risk\":\n",
    "                #     y[1] = 1\n",
    "                \n",
    "                # Append age to target 'y'\n",
    "                if str(experiment[-1]) == \"a\":\n",
    "                    y = int(participant_labels[f\"Age_days_a\"].item())\n",
    "                elif str(experiment[-1]) == \"b\":\n",
    "                    try: \n",
    "                        y = int(participant_labels[f\"Age_days_b\"].item())\n",
    "                    except: # If age of 'b' experiment not in metadata\n",
    "                        y = int(participant_labels[f\"Age_days_a\"].item()) + 120\n",
    "\n",
    "                y_batch.append(y)\n",
    "\n",
    "        # Shuffle batch\n",
    "        shuffle_batch = list(zip(x_batch, y_batch))\n",
    "        random.shuffle(shuffle_batch)\n",
    "        x_batch, y_batch = zip(*shuffle_batch)\n",
    "\n",
    "        return np.array(x_batch), np.array(y_batch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == \"epodium\":\n",
    "    train_sequence = EpodiumSequence(experiments_train, epod_labels, epochs_directory, n_experiments_batch=1, gaussian_noise=1e-6)\n",
    "    test_sequence = EpodiumSequence(experiments_test, epod_labels, epochs_directory, n_experiments_batch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise data instance\n",
    "\n",
    "During processing, the epochs are chosen to be 1 second in which the event occurs at 0.2s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_sequence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[43mtest_sequence\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe shape of one data instance is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;66;03m# 4 data instances for each experiment\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_sequence' is not defined"
     ]
    }
   ],
   "source": [
    "x, y = test_sequence.__getitem__(6, True)\n",
    "print(f\"The shape of one data instance is {x[0].shape}\")\n",
    "\n",
    "index = 3 # 4 data instances for each experiment\n",
    "display_helper.plot_array_as_evoked(x[index], dataset.channel_names, frequency=2048, n_trials=30)\n",
    "# display_helper.plot_array_as_evoked(x[index][32:], frequency=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "<a id='2mt'></a>\n",
    "## 2. Deep Learning\n",
    "\n",
    "The data is an *evoked* or *ERP* from a participant in the ePodium experiment. 60 EEG signals were averaged from -0.2 to +0.8 seconds after onset of an event. This is done for each of the 12 event types seperately.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"encoder_age_128_3\"\n",
    "model = encoder((64,128), 1)\n",
    "epochs = 300\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# Paths to save model info\n",
    "base_path = os.path.join(local_paths.models, model_name)\n",
    "\n",
    "path_history = os.path.join(base_path, \"history.npy\")\n",
    "path_model = os.path.join(base_path, \"model\")\n",
    "path_testset = os.path.join(base_path, \"testset.txt\")\n",
    "path_weights = os.path.join(base_path, \"weights.h5\")\n",
    "\n",
    "if os.path.exists(path_model):\n",
    "    print(f\"Model: '{model_name}' already exist. Delete the existing model first or rename this model.\")    \n",
    "else:\n",
    "    print(f\"Create model: {model_name}\")\n",
    "    if not os.path.exists(base_path):\n",
    "        os.mkdir(base_path)\n",
    "\n",
    "    # Save validation-set for future testing\n",
    "    with open(path_testset, 'w') as f:\n",
    "        for participant in test:\n",
    "            f.write(participant + '\\n')\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=MeanSquaredError()) # , metrics=[Precision(), BinaryAccuracy(), Recall()]\n",
    "\n",
    "    # Fit model\n",
    "    checkpointer = ModelCheckpoint(filepath=path_weights, monitor='val_loss', verbose=1, save_weights_only=True, save_best_only=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=10, factor=0.7, verbose=1) # add to callbacks if uncomment\n",
    "    history = model.fit(x=train_sequence, validation_data=test_sequence, epochs=epochs, callbacks=[checkpointer])\n",
    "\n",
    "    np.save(path_history, history.history)\n",
    "    model.save(path_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_helper.show_plot(x=range(len(history.history['loss'])), y=history.history['loss'], xlabel=\"epochs\", ylabel=\"validation loss\", title=f\"Loss during training ({model_name})\")\n",
    "display_helper.show_plot(x=range(len(history.history['loss'])), y=history.history['val_loss'], xlabel=\"epochs\", ylabel=\"validation loss\", title=f\"Validation loss during training ({model_name})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5f6ecf0357e95e30953d0cf08844b8b26fdbdf1f780a6e218131c917612a57e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
