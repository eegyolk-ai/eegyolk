{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Learning Models.\n",
    "\n",
    "In this notebook the Dutch Dyslexia Programme (DDP) and ePodium dataset are used to train a deep neural network model.\n",
    "The model is trained to predict the age and risk of dyslexia.\n",
    "The input data consists of averaged epochs of the EEG data.\n",
    "\n",
    "+ In section 1. [Prepare Dataset](#1mt) ...\n",
    "+ In section 2. [Deep Learning](#2mt) ...\n",
    "\n",
    "It is recommended to run this notebook with CUDA enabled with a dedicated graphics card to speed-up the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import local_paths\n",
    "from functions import processing, display_helper, data_io\n",
    "\n",
    "from functions.epodium import Epodium\n",
    "from functions.ddp import DDP\n",
    "from functions.train_and_predict import EpodiumSequence, DDPSequence\n",
    "\n",
    "from models import transformer\n",
    "from models.dnn import fully_connected_model\n",
    "from models.hfawaz import cnn, encoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import BinaryCrossentropy, MeanSquaredError\n",
    "from keras.metrics import Precision, BinaryAccuracy, Recall\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='1mt'></a>\n",
    "## 1. Prepare Dataset\n",
    "\n",
    "This notebook works with both the ePodium and the DDP dataset. Choose which dataset to initialize below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose between datasets: \"epodium\" \"ddp\"\n",
    "dataset_name = \"ddp\"\n",
    "\n",
    "if dataset_name == \"epodium\":\n",
    "    dataset = Epodium()    \n",
    "    epochs_directory = local_paths.ePod_epochs\n",
    "    event_directory = local_paths.ePod_epochs_events\n",
    "    \n",
    "    epod_labels = dataset.create_labels(local_paths.ePod_metadata)\n",
    "    print(f\"The available labels are:\\n {list(epod_labels.columns)}\")\n",
    "\n",
    "elif dataset_name == \"ddp\":\n",
    "    dataset = DDP()\n",
    "    epochs_directory = local_paths.DDP_epochs\n",
    "    event_directory = local_paths.DDP_epochs_events\n",
    "    \n",
    "    directory_age_metadata = os.path.join(local_paths.DDP_metadata, \"ages\")\n",
    "    ddp_labels = dataset.create_labels(local_paths.DDP_dataset, directory_age_metadata)\n",
    "    print(f\"The available labels are:\\n {list(ddp_labels.columns)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split processed epochs* into train and test sequence.\n",
    "\n",
    "\n",
    "First choose which processed data to use\n",
    "\n",
    "\n",
    "    *In the context of electroencephalography (EEG), epochs are EEG segments in which an event occurs. In the context of deep learning, epochs are iterations over the entire training dataset. We now use epochs in the deep learning context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_list = processing.valid_experiments(dataset, event_directory, min_standards=180, min_deviants=80)\n",
    "\n",
    "if dataset_name == \"ddp\":\n",
    "    experiment_list = list(set(experiment_list)-set(dataset.wrong_channels_experiments))\n",
    "    print(f\"{len(dataset.wrong_channels_experiments)} experiments have incorrect channels. \"\n",
    "          f\"{len(experiment_list)} experiments remain\")\n",
    "\n",
    "# train / test / validation ratio\n",
    "r = np.array([0.7, 0.15, 0.15])\n",
    "experiments_train_val, experiments_test = dataset.split_dataset(experiment_list, (r[0]+r[2])/r.sum())\n",
    "experiments_train, experiments_val = dataset.split_dataset(experiments_train_val, r[0]/(r[0]+r[2]))\n",
    "\n",
    "print(f\"\\nThe dataset is split up into {len(experiments_train)} train, \"\n",
    "      f\"{len(experiments_test)} test, and {len(experiments_val)} validation experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing data iterator (Sequence) as input to the deep learning models.\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence\n",
    "\n",
    "TODO explanation on noise + other variables / difference in train/test/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == \"epodium\":\n",
    "    train_sequence = EpodiumSequence(experiments_train, epod_labels, epochs_directory, n_experiments_batch=4, gaussian_noise=1e-6)\n",
    "    test_sequence = EpodiumSequence(experiments_test, epod_labels, epochs_directory, n_experiments_batch=4)\n",
    "    val_sequence = EpodiumSequence(experiments_val, epod_labels, epochs_directory, n_experiments_batch=4)\n",
    "    \n",
    "if dataset_name == \"ddp\":\n",
    "    train_sequence = DDPSequence(experiments_train, ddp_labels, epochs_directory, n_experiments_batch=8, n_instances_per_experiment=4, gaussian_noise=1e-6)\n",
    "    test_sequence = DDPSequence(experiments_test, ddp_labels, epochs_directory, n_experiments_batch=8, n_instances_per_experiment=4)\n",
    "    val_sequence = DDPSequence(experiments_val, ddp_labels, epochs_directory, n_experiments_batch=8, n_instances_per_experiment=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise data instance\n",
    "\n",
    "During processing, the epochs are chosen to be 1 second in which the event occurs at 0.2s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_participant_index = 0\n",
    "\n",
    "temp_sequence = DDPSequence(experiments_train, ddp_labels, \n",
    "                            epochs_directory, \n",
    "                            gaussian_noise=1e-6, \n",
    "                            n_trials_averaged=30, \n",
    "                            n_experiments_batch=1, \n",
    "                            n_instances_per_experiment=1)\n",
    "\n",
    "x, y = temp_sequence.__getitem__(random_participant_index, True)\n",
    "\n",
    "print(f\"The shape of one data instance is {x[0].shape}\")\n",
    "print(\"Beware, the absolute value of the y-axis is meaningless due to the data normalization.\")\n",
    "display_helper.plot_array_as_evoked(x[0]*1e-6, dataset.channels_epod_ddp, frequency=500, n_trials=30)\n",
    "print(f\"In this experiment the age of the participant is {int(y[0])} days.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "<a id='2mt'></a>\n",
    "## 2. Deep Learning\n",
    "\n",
    "The data is an *evoked* or *ERP* from a participant in the ePodium experiment. 60 EEG signals were averaged from -0.2 to +0.8 seconds after onset of an event. This is done for each of the 12 event types seperately.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__input dimensions__: \n",
    "+ x (batches, timesteps, channels)\n",
    "+ y (batches, labels)\n",
    "\n",
    "__labels__: \n",
    "+ Binary: At risk of dyslexia\n",
    "+ Regressive: Age, Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ddp_encoder_age_26ch_500hz_2\"\n",
    "\n",
    "# Model dimensions\n",
    "n_channels = 26\n",
    "n_timesteps = 501\n",
    "x_dimension = (n_channels, n_timesteps)\n",
    "y_dimension = 1\n",
    "model = encoder(x_dimension, y_dimension)\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 100\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Paths for saving model data\n",
    "base_dir = os.path.join(local_paths.models, model_name)\n",
    "model_dir = os.path.join(base_dir, \"model\")\n",
    "subsets_dir = os.path.join(base_dir, \"subsets\")\n",
    "path_history = os.path.join(base_dir, \"history.npy\")\n",
    "path_weights = os.path.join(base_dir, \"weights.h5\")\n",
    "\n",
    "if os.path.exists(model_dir):\n",
    "    print(f\"Model: '{model_name}' already exist. Delete the existing model first or rename this model.\")    \n",
    "else:\n",
    "    print(f\"Create model: {model_name}\")\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.mkdir(base_dir)\n",
    "    if not os.path.exists(subsets_dir):\n",
    "        os.mkdir(subsets_dir)\n",
    "\n",
    "    # Save train / test / validation sets for future testing\n",
    "    data_io.save_experiment_names(experiments_train, os.path.join(subsets_dir, \"train_set\"))\n",
    "    data_io.save_experiment_names(experiments_test, os.path.join(subsets_dir, \"test_set\"))\n",
    "    data_io.save_experiment_names(experiments_val, os.path.join(subsets_dir, \"validation_set\"))\n",
    "\n",
    "    # Model configurations\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=MeanSquaredError()) # , metrics=[Precision(), BinaryAccuracy(), Recall()]\n",
    "    checkpointer = ModelCheckpoint(filepath=path_weights, monitor='val_loss', verbose=1, save_weights_only=True, save_best_only=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.7, verbose=1)\n",
    "    \n",
    "    # Fit model\n",
    "    history = model.fit(x=train_sequence, validation_data=val_sequence, epochs=epochs, callbacks=[checkpointer])\n",
    "    \n",
    "    # Save model and training history\n",
    "    model.save(model_dir)\n",
    "    np.save(path_history, history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_helper.show_plot(x=range(len(history.history['loss'])), y=history.history['loss'], xlabel=\"epochs\", ylabel=\"validation loss\", title=f\"Loss during training ({model_name})\")\n",
    "display_helper.show_plot(x=range(len(history.history['loss'])), y=history.history['val_loss'], xlabel=\"epochs\", ylabel=\"validation loss\", title=f\"Validation loss during training ({model_name})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5f6ecf0357e95e30953d0cf08844b8b26fdbdf1f780a6e218131c917612a57e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
