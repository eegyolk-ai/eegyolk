{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "In this notebook the *Dutch Dyslexia Programme (DDP)* and *ePodium* datasets are used to train a deep neural network model.\n",
    "\n",
    "The model is trained to predict the age and risk of dyslexia from brain signals. This input data consists of the average of multiple epochs of the EEG data, called an *Event Related Potential* (ERP).\n",
    "\n",
    "+ In section 1. [Prepare Dataset](#1mt) the ePodium and DDP dataset are prepared for input in the deep learning model. The metadata containing the participant info is loaded and the dataset is split into a train, test and validation set. The data is loaded with the *Sequencer* class from TensorFlow, which iterates over the participants in the sets when the data is needed by the model.\n",
    "+ In section 2. [Deep Learning](#2mt) a deep neural network is trained to predict the age and risk of dyslexia from the ERPs from toddlers. The model types that are used are the *encoder*, *resnet* models. The models, training history, and subset contents are saved in *local_paths.models*.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "    It is recommended to run this notebook with CUDA enabled with a dedicated graphics card to speed-up the model training.\n",
    "    \n",
    "    In the context of electroencephalography, 'epochs' are EEG intervals in which an event occurs. In this notebook 'epochs' are used in the context of deep learning, in which epochs are iterations over the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-09 10:00:07.078205: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-09 10:00:07.225325: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-09 10:00:07.256131: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-09 10:00:08.399648: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-09 10:00:08.399714: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-09 10:00:08.399719: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Local\n",
    "import local_paths\n",
    "from functions import processing, display_helper, data_io\n",
    "from functions.epodium import Epodium\n",
    "from functions.ddp import DDP\n",
    "from functions.sequences import EpodiumSequence, DDPSequence\n",
    "\n",
    "# Models\n",
    "from models.dl_4_tsc import encoder_model, fully_convolutional_model, resnet_model\n",
    "from models.eeg_dl import transformer_model\n",
    "\n",
    "# Tensorflow dependencies\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import BinaryCrossentropy, MeanSquaredError\n",
    "from keras.metrics import Precision, BinaryAccuracy, Recall\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1mt'></a>\n",
    "## 1. Prepare Data\n",
    "\n",
    "#### Choose dataset\n",
    "\n",
    "This notebook works with both the ePodium and the DDP dataset. Choose which dataset to use by changing the variable: *dataset_name*. \n",
    "\n",
    "The *dataset* variable contains information and functions about the specific dataset. The directories to the data and the labels of the selected dataset are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The available labels are:\n",
      " ['Participant', 'Age_days_a', 'Age_days_b', 'Risk_of_dyslexia', 'Dyslexia_score']\n"
     ]
    }
   ],
   "source": [
    "# Choose between datasets: \"epodium\" \"ddp\"\n",
    "dataset_name = \"epodium\"\n",
    "\n",
    "if dataset_name == \"epodium\":\n",
    "    dataset = Epodium()    \n",
    "    epochs_directory = local_paths.ePod_epochs\n",
    "    event_directory = local_paths.ePod_epochs_events    \n",
    "    epod_labels = dataset.create_labels(local_paths.ePod_metadata)\n",
    "    print(f\"The available labels are:\\n {list(epod_labels.columns)}\")\n",
    "\n",
    "elif dataset_name == \"ddp\":\n",
    "    dataset = DDP()\n",
    "    epochs_directory = local_paths.DDP_epochs\n",
    "    event_directory = local_paths.DDP_epochs_events    \n",
    "    directory_age_metadata = os.path.join(local_paths.DDP_metadata, \"ages\")\n",
    "    ddp_labels = dataset.create_labels(local_paths.DDP_dataset, directory_age_metadata)\n",
    "    print(f\"The available labels are:\\n {list(ddp_labels.columns)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Split valid experiments into train, validation, test set.\n",
    "\n",
    "1. Get only the valid experiments from each participants. The minimum amount of *standard* and *deviant* trials that need to be in each experiment can be selected.\n",
    "\n",
    "2. The data is split up into a train, validation, and test set. Each participant can have multiple experiments. The data is split up according to participants, so that no two experiments from the same participant are in multiple sets. The ratio *r* of the subsets is set to 70% test, 15% validation and 15% test set size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed: 248 bad: 62\n",
      "186 experiments have enough epochs for analysis.\n",
      "\n",
      "The dataset is split up into 127 train, 30 test, and 29 validation experiments\n"
     ]
    }
   ],
   "source": [
    "experiment_list = processing.valid_experiments(dataset, event_directory, min_standards=180, min_deviants=80)\n",
    "\n",
    "# In the DDP dataset some experiments are ignored due to incorrect channels \n",
    "if dataset_name == \"ddp\":\n",
    "    experiment_list = list(set(experiment_list)-set(dataset.wrong_channels_experiments))\n",
    "    print(f\"{len(dataset.wrong_channels_experiments)} experiments have incorrect channels. \"\n",
    "          f\"{len(experiment_list)} experiments remain\")\n",
    "\n",
    "# [train / test / validation] ratio\n",
    "r = np.array([0.7, 0.15, 0.15])\n",
    "experiments_train_val, experiments_test = dataset.split_dataset(experiment_list, (r[0]+r[2])/r.sum())\n",
    "experiments_train, experiments_val = dataset.split_dataset(experiments_train_val, r[0]/(r[0]+r[2]))\n",
    "\n",
    "print(f\"\\nThe dataset is split up into {len(experiments_train)} train, \"\n",
    "      f\"{len(experiments_test)} test, and {len(experiments_val)} validation experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing a 'Sequence' as input to the deep learning models.\n",
    "\n",
    "In this deep learning notebook, the model iterates over a sequence of data. Each data instance in this sequence is only loaded when it is used, and unloaded when the model is done using the data instance. In Tensorflow, such a sequence is implemented with the *Sequence* class:\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence\n",
    "\n",
    "This notebook uses an extended version of this *Sequence* class for both the ePodium and the DDP dataset: *EpodiumSequence*, *DDPSequence*. When instantiating, some of the variables can be varied without breaking the code. These variables are discussed below.\n",
    "\n",
    "+ *n_trials_averaged*: The number of trials that are averaged together to get the ERP that is the input to the model. A lower number means more data-points, while a higher number of averaged trials reduces the noise.\n",
    "+ *gaussian_noise*: Noise can be artificially added to the data to reduce overfitting on the training set. The value of this parameter indicates the variation of the noise that is added to each individual time-step of each channel. \n",
    "+ *batch_size*: The number of experiments that are put into a single batch. In deep learning, a model is updated after processing a batch. A lower batch size means more updates, while a larger batch size has the advantage of less variations in the updates.\n",
    "\n",
    "\n",
    "The *train_sequence* is used for training the model, and the *val_sequence* is used to measure the actual performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == \"epodium\":\n",
    "    train_sequence = EpodiumSequence(experiments_train, epod_labels, epochs_directory, \n",
    "                                     batch_size=2, n_trials_averaged=15,\n",
    "                                     input_type=\"MMR\", label='dyslexia')\n",
    "    val_sequence = EpodiumSequence(experiments_val, epod_labels, epochs_directory, n_trials_averaged=15,\n",
    "                                   batch_size=2, input_type=\"MMR\", label='dyslexia')\n",
    "    \n",
    "if dataset_name == \"ddp\":\n",
    "    train_sequence = DDPSequence(experiments_train, ddp_labels, epochs_directory, \n",
    "                                 batch_size=8, n_trials_averaged=30, gaussian_noise=1e-6)\n",
    "    val_sequence = DDPSequence(experiments_val, ddp_labels, epochs_directory, \n",
    "                               batch_size=8, n_trials_averaged=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "<a id='2mt'></a>\n",
    "## 2. Deep Learning\n",
    "\n",
    "Now that the data is set-up, the training model created. The model dimensions are set to the dimensions of the input data and predicted output label(s): \n",
    "\n",
    "+ Input x has dimensions: *(batches, timesteps, channels)*\n",
    "+ Output y has dimensions: *(batches, labels)*\n",
    "\n",
    "An ERP consists of 2-dimensional input data. One of the dimensions represents the time where *n_timesteps* contains the number of timesteps. The other dimension represents the channels, i.e. the sensor locations on the scalp. The variable *n_channels* signifies the number of channels in the ERP.\n",
    "\n",
    "The model is trained to predict the output y from an input x. When *y_dimension* is set to 1, the model outputs a single floating point number from the ERP data input. This number can represent a regressive label like *age* and the *risk of dyslexia*.\n",
    "\n",
    "#### Choose model\n",
    "\n",
    "Multiple model types can be trained. To pick a model, change the *model_name* variable to contain the desired model. The model options are: **encoder** / **transformer** / **resnet**. Of course, it is also possible to import and use other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-09 10:00:11.034445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-09 10:00:11.100132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-09 10:00:11.101601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-09 10:00:11.103474: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-09 10:00:11.104306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-09 10:00:11.105686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-09 10:00:11.107037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-09 10:00:11.632049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-09 10:00:11.632843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-09 10:00:11.633417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-09 10:00:11.633991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20639 MB memory:  -> device: 0, name: NVIDIA A10, pci bus id: 0000:00:05.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model_name = \"epod_encoder_dyslexia_avg15_new\"\n",
    "# os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "# Model dimensions\n",
    "n_channels = 32\n",
    "n_timesteps = 2049\n",
    "x_dimension = (n_channels, n_timesteps)\n",
    "y_dimension = 1\n",
    "\n",
    "if \"encoder\" in model_name:\n",
    "    model = encoder_model(x_dimension, y_dimension)\n",
    "elif \"resnet\" in model_name:\n",
    "    model = resnet_model(x_dimension, y_dimension)\n",
    "else:\n",
    "    print(\"No model found. Add a model to the model name.\")\n",
    "#elif \"transformer\" in model_name:\n",
    "#     model = transformer_model(x_dimension, y_dimension)\n",
    "# elif \"fcn\" in model_name:\n",
    "#     model = fully_convolutional_model(x_dimension, y_dimension)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train model\n",
    "\n",
    "The models, training history, and subset contents are saved in the folder *model_name* inside *local_paths.models*. \n",
    "\n",
    "Multiple hyperparameters can be adjusted:\n",
    "+ *epochs*: number of iterations over the dataset\n",
    "+ *learning_rate*: step size in optimizing the model parameters at each update\n",
    "\n",
    "*Adam* is used as an optimizer, since this optimizer performs well in a wide variety of cases. The loss of the optimizer is set to *MeanSquaredError*. This is a commonly used loss function in regression analysis where a 2x increase in error corresponds to a 4x increase in the loss.\n",
    "\n",
    "The model weights are saved when the validation loss is improved with the *ModelCheckpoint* callback. If however the validation loss is not improved, the learning rate is reduced with the *ReduceLROnPlateau* callback. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create model: epod_encoder_dyslexia_avg15_new\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-09 10:00:30.615698: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8600\n",
      "2022-11-09 10:00:31.220441: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-11-09 10:00:31.323507: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - ETA: 0s - loss: 0.0554\n",
      "Epoch 1: val_loss improved from inf to 0.01923, saving model to /home/fpauwels/eegyolk/floris_files/models/trained_models/epod_encoder_dyslexia_avg15_new/weights.h5\n",
      "64/64 [==============================] - 653s 10s/step - loss: 0.0554 - val_loss: 0.0192 - lr: 5.0000e-04\n",
      "Epoch 2/30\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0221\n",
      "Epoch 2: val_loss improved from 0.01923 to 0.01763, saving model to /home/fpauwels/eegyolk/floris_files/models/trained_models/epod_encoder_dyslexia_avg15_new/weights.h5\n",
      "64/64 [==============================] - 625s 10s/step - loss: 0.0221 - val_loss: 0.0176 - lr: 5.0000e-04\n",
      "Epoch 3/30\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0204\n",
      "Epoch 3: val_loss improved from 0.01763 to 0.01526, saving model to /home/fpauwels/eegyolk/floris_files/models/trained_models/epod_encoder_dyslexia_avg15_new/weights.h5\n",
      "64/64 [==============================] - 551s 9s/step - loss: 0.0204 - val_loss: 0.0153 - lr: 5.0000e-04\n",
      "Epoch 4/30\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0181\n",
      "Epoch 4: val_loss did not improve from 0.01526\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0004500000213738531.\n",
      "64/64 [==============================] - 543s 9s/step - loss: 0.0181 - val_loss: 0.0208 - lr: 5.0000e-04\n",
      "Epoch 5/30\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0204\n",
      "Epoch 5: val_loss did not improve from 0.01526\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0004050000192364678.\n",
      "64/64 [==============================] - 499s 8s/step - loss: 0.0204 - val_loss: 0.0159 - lr: 4.5000e-04\n",
      "Epoch 6/30\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0187\n",
      "Epoch 6: val_loss did not improve from 0.01526\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0003645000251708552.\n",
      "64/64 [==============================] - 498s 8s/step - loss: 0.0187 - val_loss: 0.0356 - lr: 4.0500e-04\n",
      "Epoch 7/30\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0232\n",
      "Epoch 7: val_loss did not improve from 0.01526\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0003280500357504934.\n",
      "64/64 [==============================] - 506s 8s/step - loss: 0.0232 - val_loss: 0.0274 - lr: 3.6450e-04\n",
      "Epoch 8/30\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0201\n",
      "Epoch 8: val_loss did not improve from 0.01526\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00029524502169806513.\n",
      "64/64 [==============================] - 530s 8s/step - loss: 0.0201 - val_loss: 0.0192 - lr: 3.2805e-04\n",
      "Epoch 9/30\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0202\n",
      "Epoch 9: val_loss did not improve from 0.01526\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00026572052738629284.\n",
      "64/64 [==============================] - 488s 8s/step - loss: 0.0202 - val_loss: 0.0162 - lr: 2.9525e-04\n",
      "Epoch 10/30\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0195\n",
      "Epoch 10: val_loss did not improve from 0.01526\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00023914847988635302.\n",
      "64/64 [==============================] - 484s 8s/step - loss: 0.0195 - val_loss: 0.0249 - lr: 2.6572e-04\n",
      "Epoch 11/30\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0195\n",
      "Epoch 11: val_loss did not improve from 0.01526\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00021523362665902825.\n",
      "64/64 [==============================] - 510s 8s/step - loss: 0.0195 - val_loss: 0.0160 - lr: 2.3915e-04\n",
      "Epoch 12/30\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0181\n",
      "Epoch 12: val_loss did not improve from 0.01526\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00019371026137378068.\n",
      "64/64 [==============================] - 480s 8s/step - loss: 0.0181 - val_loss: 0.0185 - lr: 2.1523e-04\n",
      "Epoch 13/30\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0188\n",
      "Epoch 13: val_loss did not improve from 0.01526\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00017433923785574736.\n",
      "64/64 [==============================] - 464s 7s/step - loss: 0.0188 - val_loss: 0.0174 - lr: 1.9371e-04\n",
      "Epoch 14/30\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0176\n",
      "Epoch 14: val_loss did not improve from 0.01526\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.00015690531145082787.\n",
      "64/64 [==============================] - 482s 8s/step - loss: 0.0176 - val_loss: 0.0158 - lr: 1.7434e-04\n",
      "Epoch 15/30\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0191\n",
      "Epoch 15: val_loss improved from 0.01526 to 0.01518, saving model to /home/fpauwels/eegyolk/floris_files/models/trained_models/epod_encoder_dyslexia_avg15_new/weights.h5\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00014121478161541746.\n",
      "64/64 [==============================] - 488s 8s/step - loss: 0.0191 - val_loss: 0.0152 - lr: 1.5691e-04\n",
      "Epoch 16/30\n",
      " 6/64 [=>............................] - ETA: 5:38 - loss: 0.0171"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "epochs = 30\n",
    "learning_rate = 5e-4\n",
    "\n",
    "# Paths for saving model data\n",
    "base_dir = os.path.join(local_paths.models, model_name)\n",
    "model_dir = os.path.join(base_dir, \"model\")\n",
    "subsets_dir = os.path.join(base_dir, \"subsets\")\n",
    "path_history = os.path.join(base_dir, \"history.npy\")\n",
    "path_weights = os.path.join(base_dir, \"weights.h5\")\n",
    "\n",
    "if os.path.exists(model_dir):\n",
    "    print(f\"Model: '{model_name}' already exist. Delete the existing model first or rename this model.\")    \n",
    "else:\n",
    "    print(f\"Create model: {model_name}\")\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.mkdir(base_dir)\n",
    "    if not os.path.exists(subsets_dir):\n",
    "        os.mkdir(subsets_dir)\n",
    "\n",
    "    # Save train / test / validation sets for future testing\n",
    "    data_io.save_experiment_names(experiments_train, os.path.join(subsets_dir, \"train_set.txt\"))\n",
    "    data_io.save_experiment_names(experiments_test, os.path.join(subsets_dir, \"test_set.txt\"))\n",
    "    data_io.save_experiment_names(experiments_val, os.path.join(subsets_dir, \"validation_set.txt\"))\n",
    "\n",
    "    # Model configurations\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=MeanSquaredError())\n",
    "    checkpointer = ModelCheckpoint(filepath=path_weights, monitor='val_loss', verbose=1, save_weights_only=True, save_best_only=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=1, factor=0.9, verbose=1)\n",
    "    \n",
    "    # Fit model\n",
    "    history = model.fit(x=train_sequence, validation_data=val_sequence, epochs=epochs, callbacks=[checkpointer, reduce_lr])\n",
    "    \n",
    "    # Save model and training history\n",
    "    model.save(model_dir)\n",
    "    np.save(path_history, history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_helper.show_plot(x=range(len(history.history['loss'])), \n",
    "                         y=[history.history['loss'], history.history['val_loss']], \n",
    "                         legend=[\"loss\",\"validation loss\"], \n",
    "                         xlabel=\"epochs\", ylabel=\"validation loss (MSE)\", \n",
    "                         title=f\"Loss during training ({model_name})\",\n",
    "                         xlim=[0,20], ylim=[0,0.1])\n",
    "                         # xlim=[0,100], ylim=[0,150000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5f6ecf0357e95e30953d0cf08844b8b26fdbdf1f780a6e218131c917612a57e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
