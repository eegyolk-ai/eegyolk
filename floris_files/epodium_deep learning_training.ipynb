{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applies Deep Learning to ePodium dataset for prediction of Dyslexia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import Precision, BinaryAccuracy, Recall\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from functions import epodium, epodium_deep_learning, display_helper\n",
    "from models.dnn import fully_connected_model\n",
    "from models import transformer\n",
    "\n",
    "import local_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose which processed data to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c11a8daa86a441695060ff7805b88c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='processing:', options=('autoreject', 'ransac'), value='autoreject')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processing_method_widget = ipywidgets.RadioButtons(options=['autoreject', 'ransac'], \n",
    "                                                   value='autoreject', \n",
    "                                                   description='processing:')\n",
    "display(processing_method_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Preparing data iterator (Sequence) as input to the deep learning models.\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence\n",
    "\n",
    "#### Split processed epochs* into train and test sequence.\n",
    "\n",
    "*In the context of electroencephalography (EEG), *epochs* are EEG segments in which an event occurs. During processing, the epochs are chosen to be 1 second in which the event occurs at 0.2s. In the context of deep learning, *epochs* are iterations over the entire training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed: 228, bad: 42\n",
      "186 files have enough epochs for analysis.\n",
      "The dataset is split up into 138 train and 48 test experiments\n"
     ]
    }
   ],
   "source": [
    "if(processing_method_widget.value == \"autoreject\"):\n",
    "    path_processed = local_paths.ePod_processed_autoreject\n",
    "if(processing_method_widget.value == \"ransac\"):\n",
    "    path_processed = local_paths.ePod_processed_ransac\n",
    "\n",
    "train, test = epodium_deep_learning.split_train_test_datasets(path_processed)\n",
    "train_sequence = epodium_deep_learning.EvokedDataIterator(train, path_processed)\n",
    "test_sequence = epodium_deep_learning.EvokedDataIterator(test, path_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "The data is an *evoked* or *ERP* from a participant in the ePodium experiment. 60 EEG signals were averaged from -0.2 to +0.8 seconds after onset of an event. This is done for each of the 12 event types seperately.\n",
    "\n",
    "__dimensions__: \n",
    "+ x (batches, timesteps, channels)\n",
    "+ y (batches, labels)\n",
    "\n",
    "__labels__: \n",
    "+ (Sex, At risk of dyslexia, first standard, standard, deviant)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose Deep Learning model\n",
    "\n",
    "TODO check all trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2b068d960a4d268f4bc8c4b5d50cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Models:', options=('fully_connected', 'transformer', 'new_transformer'), value='fullâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_widget = ipywidgets.RadioButtons(options=['fully_connected', 'transformer', 'new_transformer'],\n",
    "                                       value='fully_connected', \n",
    "                                       description='Models:')\n",
    "display(model_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create model: fully_connected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-07 18:23:44.937558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-07 18:23:44.982979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-07 18:23:44.983262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-07 18:23:44.985101: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-07 18:23:44.985752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-07 18:23:44.986165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-07 18:23:44.986356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-07 18:23:45.751122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-07 18:23:45.751399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-07 18:23:45.751663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-07 18:23:45.751822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9650 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6930 - precision: 0.5187 - binary_accuracy: 0.5127 - recall: 0.8359 \n",
      "Epoch 1: val_loss improved from inf to 0.69349, saving model to /volume-ceph/floris_storage/models/fully_connected_weights.h5\n",
      "18/18 [==============================] - 280s 16s/step - loss: 0.6930 - precision: 0.5187 - binary_accuracy: 0.5127 - recall: 0.8359 - val_loss: 0.6935 - val_precision: 0.4931 - val_binary_accuracy: 0.4931 - val_recall: 1.0000 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6923 - precision: 0.5221 - binary_accuracy: 0.5185 - recall: 0.8443\n",
      "Epoch 2: val_loss did not improve from 0.69349\n",
      "18/18 [==============================] - 199s 11s/step - loss: 0.6923 - precision: 0.5221 - binary_accuracy: 0.5185 - recall: 0.8443 - val_loss: 0.6937 - val_precision: 0.4931 - val_binary_accuracy: 0.4931 - val_recall: 1.0000 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6920 - precision: 0.5193 - binary_accuracy: 0.5136 - recall: 0.8343\n",
      "Epoch 3: val_loss did not improve from 0.69349\n",
      "18/18 [==============================] - 192s 11s/step - loss: 0.6920 - precision: 0.5193 - binary_accuracy: 0.5136 - recall: 0.8343 - val_loss: 0.6941 - val_precision: 0.4896 - val_binary_accuracy: 0.4931 - val_recall: 0.6620 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6917 - precision: 0.5212 - binary_accuracy: 0.5168 - recall: 0.8371\n",
      "Epoch 4: val_loss did not improve from 0.69349\n",
      "18/18 [==============================] - 195s 11s/step - loss: 0.6917 - precision: 0.5212 - binary_accuracy: 0.5168 - recall: 0.8371 - val_loss: 0.6945 - val_precision: 0.4896 - val_binary_accuracy: 0.4931 - val_recall: 0.6620 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6922 - precision: 0.5212 - binary_accuracy: 0.5168 - recall: 0.8359\n",
      "Epoch 5: val_loss did not improve from 0.69349\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "18/18 [==============================] - 191s 11s/step - loss: 0.6922 - precision: 0.5212 - binary_accuracy: 0.5168 - recall: 0.8359 - val_loss: 0.6950 - val_precision: 0.4896 - val_binary_accuracy: 0.4931 - val_recall: 0.6620 - lr: 0.0010\n",
      "INFO:tensorflow:Assets written to: /volume-ceph/floris_storage/models/fully_connected_model/assets\n"
     ]
    }
   ],
   "source": [
    "# Paths to save model info\n",
    "base_path = os.path.join(local_paths.models, model_widget.value)\n",
    "path_history = base_path + \"_history.npy\"\n",
    "path_model = base_path + \"_model\"\n",
    "path_testset = base_path + \"_testset.txt\"\n",
    "path_weights = base_path + \"_weights.h5\"\n",
    "\n",
    "# Train Model\n",
    "print(f\"Create model: {model_widget.value}\")\n",
    "\n",
    "# Save validation-set for future testing\n",
    "with open(path_testset, 'w') as f:\n",
    "    for participant in test:\n",
    "        f.write(participant + '\\n')\n",
    "\n",
    "# Instantiate model\n",
    "if(model_widget.value == \"fully_connected\"):\n",
    "    model = fully_connected_model()\n",
    "elif(model_widget.value == \"transformer\"):\n",
    "    model = transformer.TransformerModel()\n",
    "elif(model_widget.value == \"new_transformer\"):\n",
    "    model = transformer.TransformerModel()\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-3), loss=BinaryCrossentropy(), metrics=[Precision(), BinaryAccuracy(), Recall()])\n",
    "checkpointer = ModelCheckpoint(filepath = path_weights, monitor='val_loss', verbose=1, save_weights_only=True, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=4, factor=0.5, verbose=1)\n",
    "\n",
    "# Fit model\n",
    "history = model.fit(x=train_sequence,\n",
    "                    validation_data=test_sequence,\n",
    "                    epochs=5,\n",
    "                    callbacks=[checkpointer, reduce_lr])\n",
    "\n",
    "np.save(path_history, history.history)\n",
    "model.save(path_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'History' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m display_helper\u001b[38;5;241m.\u001b[39mshow_plot(x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)), y \u001b[38;5;241m=\u001b[39m history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m] ,xlabel \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m, ylabel \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss during training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m display_helper\u001b[38;5;241m.\u001b[39mshow_plot(x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m])), y \u001b[38;5;241m=\u001b[39m history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] ,xlabel \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m, ylabel \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation loss during training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'History' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "display_helper.show_plot(x = range(len(history['loss'])), y = history['loss'] ,xlabel = \"epochs\", ylabel = \"validation loss\", title = \"Loss during training\")\n",
    "display_helper.show_plot(x = range(len(history['loss'])), y = history['val_loss'] ,xlabel = \"epochs\", ylabel = \"validation loss\", title = \"Validation loss during training\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5f6ecf0357e95e30953d0cf08844b8b26fdbdf1f780a6e218131c917612a57e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
