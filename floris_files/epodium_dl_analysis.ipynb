{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive tool for analyzing trained models on the ePodium dataset\n",
    "\n",
    "1. [Input Data](#1)\n",
    "2. [Deep Learning Model](#02)\n",
    "3. [Make predictions on test-set](#3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages\n",
    "Note: This notebook may output tensorflow errors if cuda is not properly installed. The notebook still functions with these errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from functions import epodium, epodium_deep_learning, display_helper\n",
    "from models.dnn import fully_connected_model\n",
    "from models.hfawaz import cnn, encoder\n",
    "\n",
    "import local_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a id=\"1\"></a>\n",
    "## 1. Input Data \n",
    "\n",
    "####  Choose which processed data to use\n",
    "Choose from the different processed _experiment_event.npy_ files. If the _local_path.split_ folder is empty, process the raw ePodium files in the _epodium_processing_ notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64674b683cc341b6bafa1909fd74d1ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='processing:', options=('autoreject_128hz', 'autoreject_512hz', 'ransac_512hz'), valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processing_methods = sorted(f for f in os.listdir(local_paths.epod_split) if not f.startswith(\".\"))\n",
    "processing_method_widget = ipywidgets.RadioButtons(options=processing_methods, \n",
    "                                                   description='processing:',\n",
    "                                                   value='autoreject_128hz')\n",
    "display(processing_method_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed: 228, bad: 42\n",
      "186 experiments have enough epochs for analysis.\n"
     ]
    }
   ],
   "source": [
    "cleaning_method = processing_method_widget.value.split('_')[0]\n",
    "experiment_list = epodium_deep_learning.clean_experiments(cleaning_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise the data sequence of a single participant\n",
    "\n",
    "TODO: SHOW Downsample differences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class EvokedDataIterator(Sequence):\n",
    "    \"\"\"\n",
    "        An Iterator Sequence class as input to feed the model.\n",
    "        The next value is given from the __getitem__ function.\n",
    "        For more information, go to:\n",
    "        https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence\n",
    "    \"\"\"    \n",
    "    \n",
    "    def __init__(self, experiments, base_path, n_experiments_batch = 8, n_trials_averaged = 60, gaussian_noise = 0):\n",
    "        self.experiments = experiments\n",
    "        self.n_experiments_batch = n_experiments_batch\n",
    "        self.n_trials_averaged = n_trials_averaged\n",
    "        self.gaussian_noise = gaussian_noise\n",
    "        \n",
    "        metadata_path = os.path.join(local_paths.ePod_metadata, \"children.txt\")\n",
    "        self.metadata = pd.read_table(metadata_path)\n",
    "        \n",
    "        self.base_path = base_path\n",
    "            \n",
    "    # The number of experiments in the entire dataset.\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.experiments)/self.n_experiments_batch))\n",
    "    \n",
    "    def __getitem__(self, index, verbose = False):        \n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        \n",
    "        for i in range(self.n_experiments_batch):\n",
    "            \n",
    "            # Set participant\n",
    "            participant_index = (index * self.n_experiments_batch + i) % len(self.experiments)\n",
    "            participant = self.experiments[participant_index]\n",
    "            participant_id = participant[:3]\n",
    "            participant_metadata = self.metadata.loc[self.metadata['ParticipantID'] == float(participant_id)]\n",
    "            \n",
    "            if(verbose):\n",
    "                print(participant)\n",
    "                \n",
    "            # load participant data\n",
    "            np.load()\n",
    "            \n",
    "            for condition in epodium.conditions:\n",
    "                \n",
    "                # Get Standard and Deviant file\n",
    "                # npy_name_S = f\"{self.experiments[participant_index]}_{condition}_S.npy\"\n",
    "                # npy_name_D = f\"{self.experiments[participant_index]}_{condition}_D.npy\"\n",
    "                # npy_path_S = os.path.join(self.split_path, npy_name_S)\n",
    "                # npy_path_D = os.path.join(self.split_path, npy_name_D)\n",
    "                # npy_S = np.load(npy_path_S)\n",
    "                # npy_D = np.load(npy_path_D)\n",
    "\n",
    "                # Create ERP from averaging 'n_trials_averaged' trials.\n",
    "                trial_indexes_S = np.random.choice(npy_S.shape[0], self.n_trials_averaged, replace=False)\n",
    "                evoked_S = np.mean(npy_S[trial_indexes_S,:,:], axis=0)\n",
    "                trial_indexes_D = np.random.choice(npy_D.shape[0], self.n_trials_averaged, replace=False)\n",
    "                evoked_D = np.mean(npy_D[trial_indexes_D,:,:], axis=0)\n",
    "                \n",
    "                # Merge Standard and Deviant evoked along the channel dimensions.\n",
    "                evoked = np.concatenate((evoked_S, evoked_D))\n",
    "                evoked += np.random.normal(0, self.gaussian_noise, evoked.shape)\n",
    "                x_batch.append(evoked)\n",
    "                \n",
    "                # Binary labels:\n",
    "                # y = np.zeros(2)\n",
    "                # if participant_metadata[\"Sex\"].item() == \"M\" :\n",
    "                #     y[0] = 1\n",
    "                # if participant_metadata[\"Group_AccToParents\"].item() == \"At risk\":\n",
    "                #     y[1] = 1\n",
    "                \n",
    "                if str(participant[-1]) == \"a\":\n",
    "                    y = normalize_age(int(participant_metadata[f\"Age_days_a\"].item()))\n",
    "                if str(participant[-1]) == \"b\":\n",
    "                    try: y = normalize_age(int(participant_metadata[f\"Age_days_b\"].item())) # Not all ages in metadata\n",
    "                    except:  y = normalize_age(int(participant_metadata[f\"Age_days_a\"].item()) + 120)\n",
    "                \n",
    "                y_batch.append(y)\n",
    "        \n",
    "        shuffle_batch = list(zip(x_batch, y_batch))\n",
    "        random.shuffle(shuffle_batch)\n",
    "        x_batch, y_batch = zip(*shuffle_batch)\n",
    "        return np.array(x_batch), np.array(y_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155b\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'npy_S' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m participant_sequence \u001b[38;5;241m=\u001b[39m EvokedDataIterator([experiment], local_paths\u001b[38;5;241m.\u001b[39mepod_clean, \n\u001b[1;32m      2\u001b[0m                                           n_experiments_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m      3\u001b[0m                                           n_trials_averaged\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[1;32m      4\u001b[0m                                           gaussian_noise\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mparticipant_sequence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36mEvokedDataIterator.__getitem__\u001b[0;34m(self, index, verbose)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(participant)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m condition \u001b[38;5;129;01min\u001b[39;00m epodium\u001b[38;5;241m.\u001b[39mconditions:\n\u001b[1;32m     42\u001b[0m     \n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Get Standard and Deviant file\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# Create ERP from averaging 'n_trials_averaged' trials.\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     trial_indexes_S \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[43mnpy_S\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_trials_averaged, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m     evoked_S \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(npy_S[trial_indexes_S,:,:], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     54\u001b[0m     trial_indexes_D \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(npy_D\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_trials_averaged, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'npy_S' is not defined"
     ]
    }
   ],
   "source": [
    "participant_sequence = EvokedDataIterator([experiment], local_paths.epod_clean, \n",
    "                                          n_experiments_batch=2,\n",
    "                                          n_trials_averaged=30,\n",
    "                                          gaussian_noise=1e-6)\n",
    "participant_sequence.__getitem__(0, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb04ffe98a5948408007a3de1bdffc9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Experiment:', options=('101a', '101b', '102a', '102b', '103b', '104a', '1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b67491f6c7840e4a1627d328a4515e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_standard_deviant_instance(experiment, codition, avg, noise):    \n",
    "    # Create data instance\n",
    "    participant_sequence = epodium_deep_learning.EvokedDataIterator([experiment], \n",
    "                                                                    processing_method_widget.value,\n",
    "                                                                    n_experiments_batch=1,\n",
    "                                                                    n_trials_averaged=avg,\n",
    "                                                                    gaussian_noise=noise*1e-6)\n",
    "    x, y = participant_sequence.__getitem__(0)\n",
    "    print(f\"The data instance has shape: {x.shape}\")\n",
    "\n",
    "    # Plot an evoked (ERP) of the specified condition\n",
    "    index = epodium.conditions.index(codition)\n",
    "    display_helper.plot_array_as_evoked(x[index][:32], epodium.channel_names, \n",
    "                                        frequency=128, n_trials=avg, ylim=[-20, 20])\n",
    "    display_helper.plot_array_as_evoked(x[index][32:], epodium.channel_names, \n",
    "                                        frequency=128, n_trials=avg, ylim=[-20, 20])\n",
    "\n",
    "# Create widgets\n",
    "experiment_widget = ipywidgets.Dropdown(options=sorted(experiment_list), description='Experiment:')\n",
    "condition_widget = ipywidgets.RadioButtons(options=epodium.conditions, description='Condition:')\n",
    "n_trials_averaged_widget = ipywidgets.IntSlider(value=60, min=1, max=80,description='Averaged trials:')\n",
    "gaussian_noise_widget = ipywidgets.FloatSlider(value=1, min=0, max=5,description='Noise (micro-volt):')\n",
    "\n",
    "# Widget settings\n",
    "options = {'experiment':experiment_widget, 'codition':condition_widget, 'avg':n_trials_averaged_widget, 'noise':gaussian_noise_widget}\n",
    "ui = ipywidgets.VBox([experiment_widget, condition_widget, n_trials_averaged_widget, gaussian_noise_widget])\n",
    "out = ipywidgets.interactive_output(plot_standard_deviant_instance, options)\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a id=\"02\"></a>\n",
    "## 2. Deep Learning Model\n",
    "\n",
    "#### Choose a trained model\n",
    "Choose from the trained models in the _local_paths.models_ folder. If the folder is empty, train a model in the _epodium_model_training_ notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa5cf42482b4714883dab8568760139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Models:', options=('encoder_age_128', 'encoder_age_128_2', 'encoder_age_128_20', 'en…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f1143cae9446999f7aad8209adbfa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Required plot setting\n",
    "%matplotlib inline \n",
    "trained_models = sorted(f for f in os.listdir(local_paths.epod_models) if not f.startswith(\".\"))\n",
    "\n",
    "model_widget = ipywidgets.RadioButtons(options=trained_models, description='Models:')\n",
    "display(model_widget)\n",
    "\n",
    "history = []\n",
    "                                       \n",
    "def load_model(mod):\n",
    "    base_path = os.path.join(local_paths.epod_models, mod)\n",
    "\n",
    "    path_history = os.path.join(base_path, \"history.npy\")\n",
    "    path_model = os.path.join(base_path, \"model\")\n",
    "    path_testset = os.path.join(base_path, \"testset.txt\")\n",
    "    path_weights = os.path.join(base_path, \"weights.h5\")\n",
    "    \n",
    "    global model\n",
    "    global testset\n",
    "    global history\n",
    "\n",
    "    # Load Model\n",
    "    if(os.path.exists(path_model)):\n",
    "        print(f\"Loading Model: '{model_widget.value}'.\")\n",
    "\n",
    "        # Loads the entire model from a folder:\n",
    "        model = tf.keras.models.load_model(path_model)\n",
    "        model.load_weights(path_weights)\n",
    "        # Reads the test-set of the trained model and puts the experiment names into a list:\n",
    "        testset = open(path_testset, \"r\").read().split()\n",
    "        # Loads the training history dictionary:\n",
    "        history = np.load(path_history, allow_pickle=True).item()\n",
    "        \n",
    "        # Show Loss of Training History\n",
    "        display_helper.show_plot(x=range(len(history['loss'][:])), y=history['loss'][:],\n",
    "                                 xlabel=\"epochs\", ylabel=\"validation loss\", title=\"Loss during training\")\n",
    "        display_helper.show_plot(x=range(len(history['loss']))[:], y=history['val_loss'][:],\n",
    "                                 xlabel=\"epochs\", ylabel=\"validation loss\", title=\"Validation loss during training\")\n",
    "        print(f\"The lowest validation loss is {round(min(history['val_loss']), 3)}\")\n",
    "\n",
    "    else: \n",
    "        print(\"Model not found\")\n",
    "\n",
    "out = ipywidgets.interactive_output(load_model, {'mod': model_widget})\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "<a id='3'></a>\n",
    "## 3. Make predictions on test-set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'npy_S' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m experiment \u001b[38;5;129;01min\u001b[39;00m trainset: \u001b[38;5;66;03m# TODO: Check difference trainset/testset \u001b[39;00m\n\u001b[1;32m      8\u001b[0m     test_sequence \u001b[38;5;241m=\u001b[39m epodium_deep_learning\u001b[38;5;241m.\u001b[39mEvokedDataIterator([experiment],\n\u001b[1;32m      9\u001b[0m                                                              processing_method_widget\u001b[38;5;241m.\u001b[39mvalue,                                                                    \n\u001b[1;32m     10\u001b[0m                                                              n_experiments_batch\u001b[38;5;241m=\u001b[39mn_passthroughs,                                                                    \n\u001b[1;32m     11\u001b[0m                                                              n_trials_averaged\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[43mtest_sequence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# print(f\"The data instance has shape: {x.shape}\")\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     real_pred \u001b[38;5;241m=\u001b[39m [y[\u001b[38;5;241m0\u001b[39m], np\u001b[38;5;241m.\u001b[39msqueeze(model\u001b[38;5;241m.\u001b[39mpredict(x, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mmean()]\n",
      "File \u001b[0;32m~/eegyolk/floris_files/functions/epodium_deep_learning.py:142\u001b[0m, in \u001b[0;36mEvokedDataIterator.__getitem__\u001b[0;34m(self, index, verbose)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mprint\u001b[39m(participant)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m condition \u001b[38;5;129;01min\u001b[39;00m epodium\u001b[38;5;241m.\u001b[39mconditions:\n\u001b[1;32m    132\u001b[0m     \n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Get Standard and Deviant file\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# Create ERP from averaging 'n_trials_averaged' trials.\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     trial_indexes_S \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[43mnpy_S\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_trials_averaged, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    143\u001b[0m     evoked_S \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(npy_S[trial_indexes_S,:,:], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    144\u001b[0m     trial_indexes_D \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(npy_D\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_trials_averaged, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'npy_S' is not defined"
     ]
    }
   ],
   "source": [
    "n_passthroughs = 20 # 1 passtrough gives 4 predictions, 1 for each condition\n",
    "results = []\n",
    "\n",
    "trainset = list(set(experiment_list) - set(testset))\n",
    "\n",
    "# For each experiment in the test-set\n",
    "for experiment in trainset: # TODO: Check difference trainset/testset \n",
    "    test_sequence = epodium_deep_learning.EvokedDataIterator([experiment],\n",
    "                                                             processing_method_widget.value,                                                                    \n",
    "                                                             n_experiments_batch=n_passthroughs,                                                                    \n",
    "                                                             n_trials_averaged=60)\n",
    "\n",
    "    x, y = test_sequence.__getitem__(0)\n",
    "    # print(f\"The data instance has shape: {x.shape}\")\n",
    "\n",
    "    real_pred = [y[0], np.squeeze(model.predict(x, verbose=0)).mean()]\n",
    "    results.append(real_pred)\n",
    "results = np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_helper.show_plot(np.array(results)[:,0], \n",
    "                         np.array(results)[:,1], \n",
    "                         f\"Age prediction on train set (encoder_age_128_20, passes: {n_passthroughs})\", \n",
    "                         \"Actual age (normalized)\",\n",
    "                         \"Predicted age (normalized)\",\n",
    "                         scatter=True,\n",
    "                         show=False)\n",
    "plt.plot([-1, 1], [-1, 1]) # Line where predicted=actual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OLD!\n",
    "\n",
    "\n",
    "#     label_sex = 'male' if y[index][0] else 'female'\n",
    "#     label_dyslexia = 'risk' if y[index][1] else 'no risk'\n",
    "#     label_event = 'deviant' if y[index][2] else 'standard'\n",
    "\n",
    "#     print (f\"{int(y[index][0])}: {label_sex}, \\\n",
    "#          {int(y[index][1])}: {label_risk}, \\\n",
    "#          {int(y[index][2])}: {label_event}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5f6ecf0357e95e30953d0cf08844b8b26fdbdf1f780a6e218131c917612a57e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
