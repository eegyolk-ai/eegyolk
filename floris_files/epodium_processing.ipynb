{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing raw EEG data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import autoreject\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import functools\n",
    "import ipywidgets\n",
    "# from multiprocessing import Pool\n",
    "\n",
    "import local_paths\n",
    "from functions import epodium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering ePodium dataset and rejecting bad trials\n",
    "\n",
    "The EEG data is processed with the following techniques:\n",
    "+ A high-pass filter on the raw EEG sequence with cutoff frequency 0.1 Hz to remove slow trends\n",
    "+ Splitting the raw data into 1 second epochs in which the event occurs at 0.2s.\n",
    "+ The epochs are cleaned with the autoreject library. This library contains classes that automatically reject bad trials and repair bad sensors in EEG data. The AutoReject and Ransac classes are used. https://autoreject.github.io/stable/index.html\n",
    "\n",
    "\n",
    "+ A low-pass filter on the epochs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose processing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_method_widget = ipywidgets.RadioButtons(options=['autoreject', 'ransac'], \n",
    "                                                   value = 'autoreject', \n",
    "                                                   description='processing:')\n",
    "display(processing_method_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing pipeline \n",
    "The *process_raw* function processes a raw file with the chosen method and saves the resulting .npy file into a new folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These experiments are incomplete\n",
    "ignore_files = [\"113a\", \"107b (deel 1+2)\", \"132a\", \"121b(2)\", \"113b\", \"107b (deel 3+4)\", \"147a\",\n",
    "                \"121a\", \"134a\", \"143b\", \"121b(1)\", \"145b\", \"152a\", \"184a\", \"165a\", \"151a\", \"163a\",\n",
    "                \"207a\", \"215b\", \"201b\"]\n",
    "\n",
    "def process_raw(path_file, path_processed, method, verbose = False):\n",
    "    \"\"\"\n",
    "        The 'process_raw' function processes a raw file with the chosen method and saves the resulting .npy file into 'path_processed'.\n",
    "        Processing methods are: 'autoreject', 'ransac'\n",
    "    \"\"\"\n",
    "    # \n",
    "    file = os.path.basename(path_file)\n",
    "    filename, extension = os.path.splitext(file)\n",
    "    path = os.path.join(local_paths.ePod_dataset, file)    \n",
    "    \n",
    "    path_processed_file = os.path.join(path_processed, 'epochs', filename + \".npy\")\n",
    "    path_processed_events = os.path.join(path_processed, 'events', filename + \".txt\")        \n",
    "\n",
    "    if os.path.exists(path_processed_file) and os.path.exists(path_processed_events):\n",
    "        if verbose:\n",
    "            print(f\"File {file} already processed \\n\", end = '')\n",
    "        return\n",
    "\n",
    "    if filename in ignore_files:\n",
    "        if verbose:\n",
    "            print(f\"File {file} ignored \\n\", end = '')\n",
    "        return\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Processing file: {file}  \\n\" , end = '')        \n",
    "    raw = mne.io.read_raw_bdf(path_file, preload = True, verbose = False)\n",
    "    events = mne.find_events(raw, verbose = False, min_duration = 2/epodium.frequency)\n",
    "    events_12 = epodium.group_events_12(events)\n",
    "\n",
    "    # Set electrodes\n",
    "    raw.pick_channels(epodium.channel_names)\n",
    "    montage = mne.channels.make_standard_montage('standard_1020') \n",
    "    raw.info.set_montage(montage, on_missing = 'ignore')\n",
    "\n",
    "    # High-pass filter for detrending\n",
    "    raw.filter(0.1, None, verbose = False)\n",
    "    # Create epochs from raw\n",
    "    try:\n",
    "        epochs = mne.Epochs(raw, events_12, epodium.event_dictionary, -0.2, 0.8, preload = True, verbose = False)\n",
    "    except:\n",
    "        print(f\"Not all events in file {file} \\n\", end = '')\n",
    "        return\n",
    "    # Low pass filter for high-frequency artifacts\n",
    "    epochs.filter(None, 40, verbose = False)\n",
    "\n",
    "    # Reject bad trials and repair bad sensors in EEG\n",
    "    if(method == \"autoreject\"):\n",
    "        ar = autoreject.AutoReject()\n",
    "    elif(method == \"ransac\"):\n",
    "        ar = autoreject.Ransac()\n",
    "    else:\n",
    "        print(\"method not known\")\n",
    "    epochs_clean = ar.fit_transform(epochs)  \n",
    "\n",
    "    # Save data and events\n",
    "    np.save(path_processed_file, epochs_clean.get_data())        \n",
    "    np.savetxt(path_processed_events, epochs_clean.events, fmt='%i')\n",
    "\n",
    "if(processing_method_widget.value == \"autoreject\"):\n",
    "    path_processed = local_paths.ePod_processed_autoreject\n",
    "if(processing_method_widget.value == \"ransac\"):\n",
    "    path_processed = local_paths.ePod_processed_ransac\n",
    "\n",
    "## Multiprocessing\n",
    "# pool = Pool(processes = 8)\n",
    "# pool.map(functools.partial(process_file, method = \"autoreject\"), sorted(glob.glob(os.path.join(local_paths.ePod_dataset, '*.bdf'))))\n",
    "\n",
    "for path_raw in sorted(glob.glob(os.path.join(local_paths.ePod_dataset, '*.bdf'))):\n",
    "    process_raw(path_raw, path_processed, method = processing_method_widget.value, verbose = False)\n",
    "\n",
    "print(\"All files processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into seperate files for each events\n",
    "\n",
    "+ The following function splits the processed epochs up into into a seperate file for each event.\n",
    "+ The sampling rate is also reduced to decrease the data size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_clean_epochs(path_processed, sample_rate = 512):\n",
    "    \"\"\"\n",
    "        This function splits the processed epochs up into into a seperate file for each event.\n",
    "        The sampling rate is also reduced to decrease the data size.\n",
    "        \n",
    "        From path_processed, the function uses the epochs from the 'epochs' folder and saves them in 'epochs_split'.\n",
    "    \"\"\"\n",
    "\n",
    "    montage = mne.channels.make_standard_montage('standard_1020') \n",
    "    info = mne.create_info(epodium.channel_names, 2048, ch_types='eeg')\n",
    "\n",
    "    npy_filepaths = glob.glob(os.path.join(path_processed, 'epochs', '*.npy'))\n",
    "    for npy_filepath in npy_filepaths:\n",
    "        npy_filename = os.path.basename(npy_filepath)\n",
    "        filename = os.path.splitext(npy_filename)[0]\n",
    "        \n",
    "        # Find missing files\n",
    "        missing_split_paths = []\n",
    "        for event in epodium.event_dictionary:\n",
    "            split_filename = filename + \"_\" + event + \".npy\"\n",
    "            path_split = os.path.join(path_processed, 'epochs_split', split_filename)\n",
    "\n",
    "            if not os.path.exists(path_split):\n",
    "                missing_split_paths.append(path_split)\n",
    "        \n",
    "        # Split and save missing files\n",
    "        if missing_split_paths != []:\n",
    "            npy = np.load(os.path.join(path_processed, 'epochs', npy_filepath))\n",
    "            events_12 = np.loadtxt(os.path.join(path_processed, 'events', filename + \".txt\"), dtype=int)\n",
    "            epochs = mne.EpochsArray(npy, info, events=events_12, tmin=-0.2, \n",
    "                                     event_id=epodium.event_dictionary, verbose=False)\n",
    "            epochs.info.set_montage(montage, on_missing = 'ignore')\n",
    "\n",
    "            for path_split in missing_split_paths: \n",
    "                np.save(path_split, epochs[event].resample(sample_rate).get_data())\n",
    "                print(f\"{os.path.basename(path_split)} saved\")\n",
    "\n",
    "split_clean_epochs(path_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "## Steps of ideal processing pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Pipeline\n",
    "\n",
    "+ Prepare EEG \n",
    "1. Drop unused channels\n",
    "2. Subtract reference (mastoids)\n",
    "3. Detrend \n",
    "4. Filter\n",
    "5. Remove bad channels\n",
    "\n",
    "+ Segment EEG into standard and deviant epochs\n",
    "1. subtract baseline\n",
    "2. Reject artefacts\n",
    "3. Average to get the evoked for each subject, marker, and channel\n",
    "\n",
    "+ Calculate Mismatch response \n",
    "1. deviant - standard for a single subject, for example GiepST_D - GiepST_S\n",
    "2. check differences between channels and subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse mismatch response \n",
    "\n",
    "Deviant minus standard ERP\n",
    "+ Check between subjects to see if the subjects have similar responses\n",
    "+ Check between channels to observe which parts of the brain are more influenced by the events\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract features (Optional)\n",
    "+ peak latency\n",
    "+ peak amplitude\n",
    "+ mean amplitude\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input data into DL models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise results of model predictions"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5f6ecf0357e95e30953d0cf08844b8b26fdbdf1f780a6e218131c917612a57e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
