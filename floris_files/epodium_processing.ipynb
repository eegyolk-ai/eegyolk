{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing raw EEG data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import autoreject\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import functools\n",
    "import ipywidgets\n",
    "# from multiprocessing import Pool\n",
    "\n",
    "import local_paths\n",
    "from functions import epodium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering ePodium dataset and rejecting bad trials\n",
    "\n",
    "The EEG data located in _local_paths.ePod_dataset_ is cleaned with the following techniques:\n",
    "+ A high-pass filter on the raw EEG sequence with cutoff frequency 0.1 Hz to remove slow trends\n",
    "+ Splitting the raw data into 1 second epochs in which the event occurs at 0.2s.\n",
    "+ The epochs are cleaned with the autoreject library. This library contains classes that automatically reject bad trials and repair bad sensors in EEG data. The AutoReject and Ransac classes are used. https://autoreject.github.io/stable/index.html\n",
    "\n",
    "\n",
    "+ A low-pass filter on the epochs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Cleaning raw files \n",
    "The *clean_raw* function cleans a raw file with the chosen method and saves the resulting .npy file into a new folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 101a.bdf already cleaned \n",
      "File 101b.bdf already cleaned \n",
      "File 102a.bdf already cleaned \n",
      "File 102b.bdf already cleaned \n",
      "File 103a.bdf already cleaned \n",
      "File 103b.bdf already cleaned \n",
      "File 104a.bdf already cleaned \n",
      "File 104b.bdf already cleaned \n",
      "File 105a.bdf already cleaned \n",
      "File 105b.bdf already cleaned \n",
      "File 106a.bdf already cleaned \n",
      "File 106b.bdf already cleaned \n",
      "File 107a.bdf already cleaned \n",
      "File 107b (deel 1+2).bdf ignored \n",
      "File 107b (deel 3+4).bdf ignored \n",
      "File 108a.bdf already cleaned \n",
      "File 109a.bdf already cleaned \n",
      "File 109b.bdf already cleaned \n",
      "File 110a.bdf already cleaned \n",
      "File 110b.bdf already cleaned \n",
      "File 111a.bdf already cleaned \n",
      "File 111b.bdf already cleaned \n",
      "File 112a.bdf already cleaned \n",
      "File 112b.bdf already cleaned \n",
      "File 113a.bdf ignored \n",
      "File 113b.bdf ignored \n",
      "File 114a.bdf already cleaned \n",
      "File 114b.bdf already cleaned \n",
      "File 115a.bdf already cleaned \n",
      "File 115b.bdf already cleaned \n",
      "File 116a.bdf already cleaned \n",
      "File 116b.bdf already cleaned \n",
      "File 117a.bdf already cleaned \n",
      "File 117b.bdf already cleaned \n",
      "File 118a.bdf already cleaned \n",
      "File 118b.bdf already cleaned \n",
      "File 119a.bdf already cleaned \n",
      "File 119b.bdf already cleaned \n",
      "File 120a.bdf already cleaned \n",
      "File 120b.bdf already cleaned \n",
      "File 121a.bdf ignored \n",
      "File 121b(1).bdf ignored \n",
      "File 121b(2).bdf ignored \n",
      "File 122a.bdf already cleaned \n",
      "File 122b.bdf already cleaned \n",
      "File 123a.bdf already cleaned \n",
      "File 123b.bdf already cleaned \n",
      "File 124a.bdf already cleaned \n",
      "File 125a.bdf already cleaned \n",
      "File 125b.bdf already cleaned \n",
      "File 126a.bdf already cleaned \n",
      "File 126b.bdf already cleaned \n",
      "File 127a.bdf already cleaned \n",
      "File 127b.bdf already cleaned \n",
      "File 128a.bdf already cleaned \n",
      "File 128b.bdf already cleaned \n",
      "File 129a.bdf already cleaned \n",
      "File 129b.bdf already cleaned \n",
      "File 130a.bdf already cleaned \n",
      "File 130b.bdf already cleaned \n",
      "File 131a.bdf already cleaned \n",
      "File 131b.bdf already cleaned \n",
      "File 132a.bdf ignored \n",
      "File 132b.bdf already cleaned \n",
      "File 133a.bdf already cleaned \n",
      "File 133b.bdf already cleaned \n",
      "File 134a.bdf ignored \n",
      "File 134b.bdf already cleaned \n",
      "File 135a.bdf already cleaned \n",
      "File 135b.bdf already cleaned \n",
      "File 136a.bdf already cleaned \n",
      "File 136b.bdf already cleaned \n",
      "File 137a.bdf already cleaned \n",
      "File 137b.bdf already cleaned \n",
      "File 138a.bdf already cleaned \n",
      "File 138b.bdf already cleaned \n",
      "File 139a.bdf already cleaned \n",
      "File 139b.bdf already cleaned \n",
      "File 140a.bdf already cleaned \n",
      "File 140b.bdf already cleaned \n",
      "File 141a.bdf already cleaned \n",
      "File 142a.bdf already cleaned \n",
      "File 142b.bdf already cleaned \n",
      "File 143a.bdf already cleaned \n",
      "File 143b.bdf ignored \n",
      "File 144a.bdf already cleaned \n",
      "File 144b.bdf already cleaned \n",
      "File 145a.bdf already cleaned \n",
      "File 145b.bdf ignored \n",
      "File 146a.bdf already cleaned \n",
      "File 146b.bdf already cleaned \n",
      "File 147a.bdf ignored \n",
      "File 148a.bdf already cleaned \n",
      "File 148b.bdf already cleaned \n",
      "File 149a.bdf already cleaned \n",
      "File 149b.bdf already cleaned \n",
      "File 150a.bdf already cleaned \n",
      "File 150b.bdf already cleaned \n",
      "File 151a.bdf ignored \n",
      "File 151b.bdf already cleaned \n",
      "File 152a.bdf ignored \n",
      "File 152b.bdf already cleaned \n",
      "File 153a.bdf already cleaned \n",
      "File 153b.bdf already cleaned \n",
      "File 154a.bdf already cleaned \n",
      "File 154b.bdf already cleaned \n",
      "File 155a.bdf already cleaned \n",
      "File 155b.bdf already cleaned \n",
      "File 156a.bdf already cleaned \n",
      "File 156b.bdf already cleaned \n",
      "File 157a.bdf already cleaned \n",
      "File 157b.bdf already cleaned \n",
      "File 158a.bdf already cleaned \n",
      "File 158b.bdf already cleaned \n",
      "File 159a.bdf already cleaned \n",
      "File 159b.bdf already cleaned \n",
      "File 160a.bdf already cleaned \n",
      "File 160b.bdf already cleaned \n",
      "File 161a.bdf already cleaned \n",
      "File 161b.bdf already cleaned \n",
      "File 162a.bdf already cleaned \n",
      "File 162b.bdf already cleaned \n",
      "File 163a.bdf ignored \n",
      "File 163b.bdf already cleaned \n",
      "File 164a.bdf already cleaned \n",
      "File 164b.bdf already cleaned \n",
      "File 165a.bdf ignored \n",
      "File 165b.bdf already cleaned \n",
      "File 166a.bdf already cleaned \n",
      "File 166b.bdf already cleaned \n",
      "File 167a.bdf already cleaned \n",
      "File 168a.bdf already cleaned \n",
      "File 168b.bdf already cleaned \n",
      "File 169a.bdf already cleaned \n",
      "File 169b.bdf already cleaned \n",
      "File 170a.bdf already cleaned \n",
      "File 170b.bdf already cleaned \n",
      "File 171a.bdf already cleaned \n",
      "File 171b.bdf already cleaned \n",
      "File 172a.bdf already cleaned \n",
      "File 172b.bdf already cleaned \n",
      "File 173a.bdf already cleaned \n",
      "File 173b.bdf already cleaned \n",
      "File 174a.bdf already cleaned \n",
      "File 174b.bdf already cleaned \n",
      "File 175a.bdf already cleaned \n",
      "File 175b.bdf already cleaned \n",
      "File 176a.bdf already cleaned \n",
      "File 176b.bdf already cleaned \n",
      "File 177a.bdf already cleaned \n",
      "File 177b.bdf already cleaned \n",
      "File 178a.bdf already cleaned \n",
      "File 178b.bdf already cleaned \n",
      "File 179a.bdf already cleaned \n",
      "File 179b.bdf already cleaned \n",
      "File 180a.bdf already cleaned \n",
      "File 180b.bdf already cleaned \n",
      "File 181a.bdf already cleaned \n",
      "File 181b.bdf already cleaned \n",
      "File 182a.bdf already cleaned \n",
      "File 182b.bdf already cleaned \n",
      "File 183a.bdf already cleaned \n",
      "File 183b.bdf already cleaned \n",
      "File 184a.bdf ignored \n",
      "File 184b.bdf already cleaned \n",
      "File 185a.bdf already cleaned \n",
      "File 185b.bdf already cleaned \n",
      "File 186a.bdf already cleaned \n",
      "File 186b.bdf already cleaned \n",
      "File 187a.bdf already cleaned \n",
      "File 188a.bdf already cleaned \n",
      "File 188b.bdf already cleaned \n",
      "File 189a.bdf already cleaned \n",
      "File 189b.bdf already cleaned \n",
      "File 190a.bdf already cleaned \n",
      "File 190b.bdf already cleaned \n",
      "File 191a.bdf already cleaned \n",
      "File 191b.bdf already cleaned \n",
      "File 192a.bdf already cleaned \n",
      "File 192b.bdf already cleaned \n",
      "File 193a.bdf already cleaned \n",
      "File 193b.bdf already cleaned \n",
      "File 194a.bdf already cleaned \n",
      "File 194b.bdf already cleaned \n",
      "File 195a.bdf already cleaned \n",
      "File 196a.bdf already cleaned \n",
      "File 196b.bdf already cleaned \n",
      "File 197a.bdf already cleaned \n",
      "File 197b.bdf already cleaned \n",
      "File 198a.bdf already cleaned \n",
      "File 198b.bdf already cleaned \n",
      "File 199a.bdf already cleaned \n",
      "File 199b.bdf already cleaned \n",
      "File 200a.bdf already cleaned \n",
      "File 200b.bdf already cleaned \n",
      "File 201a.bdf already cleaned \n",
      "Cleaning file: 201b.bdf  \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'EpodiumClass' object has no attribute 'group_events_12'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 94>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m paths_raw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_directory, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m dataset_info\u001b[38;5;241m.\u001b[39mfile_extension)))\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m raw_path \u001b[38;5;129;01min\u001b[39;00m paths_raw:\n\u001b[0;32m---> 95\u001b[0m     \u001b[43mprocess_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessed_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll files cleaned\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mprocess_raw\u001b[0;34m(raw_path, dataset_info, processed_directory, verbose)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mraw_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist or has an incompatible extension.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m events \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_events_from_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Set electrodes\u001b[39;00m\n\u001b[1;32m     56\u001b[0m raw\u001b[38;5;241m.\u001b[39mpick_channels(dataset_info\u001b[38;5;241m.\u001b[39mchannel_names)\n",
      "File \u001b[0;32m~/eegyolk/floris_files/functions/epodium.py:50\u001b[0m, in \u001b[0;36mEpodiumClass.get_events_from_raw\u001b[0;34m(self, raw)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_events_from_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw):\n\u001b[1;32m     49\u001b[0m     events \u001b[38;5;241m=\u001b[39m mne\u001b[38;5;241m.\u001b[39mfind_events(raw, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, min_duration\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfrequency)\n\u001b[0;32m---> 50\u001b[0m     events_12 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_events_12\u001b[49m(events)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m events_12\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EpodiumClass' object has no attribute 'group_events_12'"
     ]
    }
   ],
   "source": [
    "# These experiments are incomplete\n",
    "ignore_files = [\"113a\", \"107b (deel 1+2)\", \"132a\", \"121b(2)\", \"113b\", \"107b (deel 3+4)\", \"147a\",\n",
    "                \"121a\", \"134a\", \"143b\", \"121b(1)\", \"145b\", \"152a\", \"184a\", \"165a\", \"151a\", \"163a\",\n",
    "                \"207a\", \"215b\", \"201b\"]\n",
    "\n",
    "def process_raw(raw_path, dataset_info, processed_directory, verbose=False):\n",
    "    \"\"\"\n",
    "        This function processes a raw EEG file from the MNE class.\n",
    "        Processing steps: 1. high-pass filter, 2. create epochs, 3. low-pass filter, 4. AutoReject.\n",
    "        The processed .fif file, along with a .txt file with events is stored in processed_directory.\n",
    "        \n",
    "        Args:\n",
    "        raw_path: Path to the raw EEG-file\n",
    "        dataset_info: Class containing information on the dataset, e.g. \n",
    "        processed_directory: Directory for storing the files.\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Raw file-names\n",
    "    file = os.path.basename(raw_path)\n",
    "    filename, extension = os.path.splitext(file)\n",
    "    \n",
    "    # Paths for cleaned data    \n",
    "    path_epoch = os.path.join(processed_directory, filename+\"_epo.fif\")\n",
    "    path_events = os.path.join(processed_directory, \"events\", filename+\"_events.txt\")  \n",
    "\n",
    "    # If file already processed:\n",
    "    if os.path.exists(path_epoch) or os.path.exists(path_events):\n",
    "        if verbose:\n",
    "            print(f\"File {file} already cleaned \\n\", end='')\n",
    "        # If the event .txt file is missing:\n",
    "        if not os.path.exists(path_events):\n",
    "            print(f\"Creating the event file {filename}.txt \\n\", end='')\n",
    "            epochs_clean = mne.read_epochs(path_epoch, verbose=0)\n",
    "            np.savetxt(path_events, epochs_clean.events, fmt='%i')\n",
    "        return\n",
    "    \n",
    "    if filename in dataset_info.incomplete_experiments:\n",
    "        if verbose:\n",
    "            print(f\"File {file} ignored \\n\", end='')\n",
    "        return\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Cleaning file: {file}  \\n\" , end='')\n",
    "    \n",
    "    # Read-in raw file\n",
    "    if extension == \".bdf\":\n",
    "        raw = mne.io.read_raw_bdf(raw_path, preload=True, verbose=False)\n",
    "    elif extension == \".cnt\":\n",
    "        raw = mne.io.read_raw_cnt(raw_path, preload=True, verbose=False)\n",
    "    else:\n",
    "        print(f\"The file {raw_path} has doesn't exist or has an incompatible extension.\")\n",
    "    \n",
    "    events = dataset_info.get_events_from_raw(raw)\n",
    "\n",
    "    # Set electrodes\n",
    "    raw.pick_channels(dataset_info.channel_names)\n",
    "    raw.info.set_montage(dataset_info.mne_montage, on_missing='ignore')\n",
    "\n",
    "    # High-pass filter for detrending\n",
    "    raw.filter(0.1, None, verbose=False)\n",
    "    \n",
    "    # Create epochs from raw. Epoch creation sometimes returns an error.\n",
    "    try:\n",
    "        epochs = mne.Epochs(raw, events, epodium.event_dictionary, -0.2, 0.8, preload=True, verbose=False)\n",
    "    except:\n",
    "        print(f\"Not all events in file {file} \\n\", end='')\n",
    "        return\n",
    "    \n",
    "    # Low pass filter for high-frequency artifacts\n",
    "    epochs.filter(None, 40, verbose=False)\n",
    "\n",
    "    # Reject bad trials and repair bad sensors in EEG\n",
    "    # autoreject.Ransac() is a quicker but less accurate method than AutoReject.\n",
    "    ar = autoreject.AutoReject() \n",
    "    epochs_clean = ar.fit_transform(epochs)  \n",
    "\n",
    "    # # Save data and events    \n",
    "    epochs_clean.save(path_epoch)\n",
    "    # np.save(path_cleaned_file, epochs_clean.get_data())   \n",
    "    np.savetxt(path_events, epochs_clean.events, fmt='%i')\n",
    "\n",
    "    \n",
    "    \n",
    "## Multiprocessing\n",
    "# pool = Pool(processes=8)\n",
    "# pool.map(functools.partial(file, method=\"autoreject\"), sorted(glob.glob(os.path.join(local_paths.ePod_dataset, '*.bdf'))))\n",
    "\n",
    "dataset_directory = local_paths.ePod_dataset\n",
    "dataset_info = epodium.EpodiumClass()\n",
    "processed_directory = local_paths.ePod_processed\n",
    "\n",
    "paths_raw = sorted(glob.glob(os.path.join(dataset_directory, '*' + dataset_info.file_extension)))\n",
    "\n",
    "for raw_path in paths_raw:\n",
    "    process_raw(raw_path, dataset_info, processed_directory, verbose=True)\n",
    "\n",
    "print(\"All files cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO extern "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Steps of ideal processing pipeline:\n",
    "\n",
    "For an thorough explanation on ERP and processing, read the book: An Introduction to the Event-Related Potential Technique, Second Edition\n",
    "By Steven J. Luck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Pipeline\n",
    "\n",
    "+ Prepare EEG \n",
    "1. Drop unused channels\n",
    "2. Subtract reference (mastoids)\n",
    "3. Detrend \n",
    "4. Filter\n",
    "5. Remove bad channels\n",
    "\n",
    "+ Segment EEG into standard and deviant epochs\n",
    "1. subtract baseline\n",
    "2. Reject artefacts\n",
    "3. Average to get the evoked (ERP) for each subject, marker, and channel\n",
    "\n",
    "+ Calculate Mismatch response \n",
    "1. deviant - standard for a single subject, for example GiepST_D - GiepST_S\n",
    "2. check differences between channels and subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse mismatch response \n",
    "\n",
    "Deviant minus standard ERP\n",
    "+ Check between subjects to see if the subjects have similar responses\n",
    "+ Check between channels to observe which parts of the brain are more influenced by the events\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract features (Optional)\n",
    "+ peak latency\n",
    "+ peak amplitude\n",
    "+ mean amplitude\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input data into DL models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise results of model predictions"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5f6ecf0357e95e30953d0cf08844b8b26fdbdf1f780a6e218131c917612a57e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
