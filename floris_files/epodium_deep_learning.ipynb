{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applies Deep Learning methods to ePodium dataset for prediction of Dyslexia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 08:33:01.399019: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-15 08:33:01.399055: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from functions import epodium\n",
    "from models.dnn import fully_connected_model\n",
    "from models.transformer import TransformerModel\n",
    "\n",
    "import local_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check number of epochs in each experiment\n",
    "Experiments with enough epochs are added to *clean_list*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed: 188, bad: 37\n",
      "151 files have enough epochs for analysis.\n"
     ]
    }
   ],
   "source": [
    "standard_minimum = 180 # total of 360\n",
    "deviant_minimum = 80 # total size of 120\n",
    "firststandard_minimum = 80 # total size of 120\n",
    "\n",
    "count_analyzed = 0\n",
    "count_bad = 0\n",
    "\n",
    "clean_list = []\n",
    "\n",
    "firststandard_index = [1, 4, 7, 10]\n",
    "standard_index = [2, 5, 8, 11]\n",
    "deviant_index = [3, 6, 9, 12]\n",
    "\n",
    "for event_file in os.listdir(local_paths.ePod_processed_autoreject_events):\n",
    "    if event_file.endswith('.txt') and len(event_file) == 8:\n",
    "        # print(f\"Analyzing {event_file}\")\n",
    "        count_analyzed += 1\n",
    "        event = np.loadtxt(os.path.join(local_paths.ePod_processed_autoreject_events, event_file), dtype=int)\n",
    "\n",
    "        # Count how many events are left in standard, deviant, and FS\n",
    "        for i in range(4): \n",
    "            if (np.count_nonzero(event[:, 2] == standard_index[i]) < standard_minimum\n",
    "            or np.count_nonzero(event[:, 2] == deviant_index[i]) < deviant_minimum\n",
    "            or np.count_nonzero(event[:, 2] == firststandard_index[i]) < firststandard_minimum):\n",
    "                count_bad += 1\n",
    "                break\n",
    "            if i == 3: # No bads found at end of for loop\n",
    "                clean_list.append(event_file)   \n",
    "\n",
    "clean_list = sorted(clean_list)\n",
    "print(f\"Analyzed: {count_analyzed}, bad: {count_bad}\")\n",
    "print(f\"{len(clean_list)} files have enough epochs for analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Split into train and test dataset\n",
    "Both the train and test sets have the same proportion of participants that did either a, b, or both experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split test/train on participant\n",
    "experiments = [file.replace('.txt', '') for file in clean_list]\n",
    "\n",
    "# Split experiments into participants that did a, b, and both\n",
    "experiments_a = [file.replace('a', '') for file in experiments]\n",
    "experiments_a = [item for item in experiments_a if len(item) == 3]\n",
    "experiments_b = [file.replace('b', '') for file in experiments]\n",
    "experiments_b = [item for item in experiments_b if len(item) == 3]\n",
    "experiments_a_and_b = [file for file in experiments_a if file in experiments_b]\n",
    "experiments_a_only = [file for file in experiments_a if file not in experiments_b]\n",
    "experiments_b_only = [file for file in experiments_b if file not in experiments_a]\n",
    "\n",
    "participants = sorted(experiments_a_and_b + experiments_a_only + experiments_b_only)\n",
    "\n",
    "# Split participants into train and test dataset\n",
    "train_ab, test_ab = train_test_split(experiments_a_and_b, test_size=0.25)  \n",
    "train_a, test_a = train_test_split(experiments_a_only, test_size=0.25) \n",
    "train_b, test_b = train_test_split(experiments_b_only, test_size=0.25) \n",
    "\n",
    "train = [x + 'a' for x in train_ab] + [x + 'b' for x in train_ab] + \\\n",
    "        [x + 'a' for x in train_a] + [x + 'b' for x in train_b]\n",
    "test = [x + 'a' for x in test_ab] + [x + 'b' for x in test_ab] + \\\n",
    "       [x + 'a' for x in test_a] + [x + 'b' for x in test_b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Iterator Sequence as input to feed the model\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvokedIterator(Sequence):\n",
    "    \n",
    "    def __init__(self, experiments, n_experiments = 8, n_trials_averaged = 60):\n",
    "        self.experiments = experiments                \n",
    "        self.n_experiments = n_experiments\n",
    "        self.n_trials_averaged = n_trials_averaged\n",
    "                \n",
    "        metadata_path = os.path.join(local_paths.ePod_metadata, \"children.txt\")\n",
    "        self.metadata = pd.read_table(metadata_path)\n",
    "        \n",
    "        event_types = 12 # (FS/S/D in 4 conditions)\n",
    "        self.n_files =  len(self.experiments) * event_types        \n",
    "        self.batch_size = self.n_experiments * event_types\n",
    "    \n",
    "    def __len__(self):\n",
    "        # The number of batches in the Sequence.\n",
    "        return int(np.ceil(len(self.experiments) / self.n_experiments))   \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        \n",
    "        for i in range(self.n_experiments):\n",
    "            participant_index = (index * self.n_experiments + i) % len(self.experiments)\n",
    "            participant_id = self.experiments[participant_index][:3]\n",
    "            participant_metadata = self.metadata.loc[self.metadata['ParticipantID'] == float(participant_id)]\n",
    "            \n",
    "            for key in epodium.event_dictionary:\n",
    "            \n",
    "                # Get file\n",
    "                npy_name = self.experiments[participant_index] + \"_\" + key + \".npy\"\n",
    "                npy_path = os.path.join(local_paths.ePod_processed_autoreject_epochs_split_downsampled, npy_name)\n",
    "                npy = np.load(npy_path)     \n",
    "                \n",
    "                # Create ERP from averaging 'n_trials_averaged' trials.\n",
    "                trial_indexes = np.random.choice(npy.shape[0], self.n_trials_averaged, replace=False)\n",
    "                evoked = np.mean(npy[trial_indexes,:,:], axis=0)\n",
    "                x_batch.append(evoked)\n",
    "                \n",
    "                # Create labels\n",
    "                y = np.zeros(5)\n",
    "                if(participant_metadata[\"Sex\"].item() == \"F\"):\n",
    "                    y[0] = 1\n",
    "                if(participant_metadata[\"Group_AccToParents\"].item() == \"At risk\"):\n",
    "                    y[1] = 1\n",
    "                if(key.endswith(\"_FS\")):\n",
    "                    y[2] = 1\n",
    "                if(key.endswith(\"_S\")):\n",
    "                    y[3] = 1                \n",
    "                if(key.endswith(\"_D\")):\n",
    "                    y[4] = 1\n",
    "                y_batch.append(y)        \n",
    "        \n",
    "        return np.array(x_batch), np.array(y_batch)\n",
    "    \n",
    "train_sequence = EvokedIterator(train)\n",
    "test_sequence = EvokedIterator(test)\n",
    "# x,y = train_sequence.__getitem__(0)\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Train model\n",
    "\n",
    "The data is an *evoked* or *ERP* from a participant in the ePodium experiment. 60 EEG signals were averaged from -0.2 to +0.8 seconds after onset of an event. This is done for each of the 12 event types seperately.\n",
    "\n",
    "dimensions: \n",
    "+ x (batches, timesteps, channels)\n",
    "+ y (batches, labels)\n",
    "\n",
    "labels: \n",
    "+ (Sex, At risk of dyslexia, first standard, standard, deviant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialise model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 08:33:06.272643: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-08-15 08:33:06.272672: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-08-15 08:33:06.272693: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (floriscpu): /proc/driver/nvidia/version does not exist\n",
      "2022-08-15 08:33:06.272849: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 16384) for input KerasTensor(type_spec=TensorSpec(shape=(None, 16384), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\"), but it was called on an input with incompatible shape (None, None, None).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 16384) for input KerasTensor(type_spec=TensorSpec(shape=(None, 16384), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\"), but it was called on an input with incompatible shape (None, None, None).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 08:33:29.022625: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 805306368 exceeds 10% of free system memory.\n",
      "2022-08-15 08:33:29.337401: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 805306368 exceeds 10% of free system memory.\n",
      "2022-08-15 08:33:29.917128: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 805306368 exceeds 10% of free system memory.\n",
      "2022-08-15 08:33:30.195133: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 805306368 exceeds 10% of free system memory.\n",
      "2022-08-15 08:33:30.826165: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 805306368 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/14 [======================>.......] - ETA: 1:03 - loss: 1.8715 - precision: 0.4024 - binary_accuracy: 0.4676 - recall: 0.6068"
     ]
    }
   ],
   "source": [
    "# fit network\n",
    "try:\n",
    "    print(f\"{model} already loaded\")\n",
    "except:\n",
    "    print(\"initialise model\")\n",
    "    model = TransformerModel()\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "                             loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                             metrics=[tf.keras.metrics.Precision(),tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.Recall()])\n",
    "\n",
    "    output_filename = 'fully_connecteed_model'\n",
    "    output_file = os.path.join(local_paths.models, output_filename)\n",
    "    checkpointer = ModelCheckpoint(filepath = output_file + \".hdf5\", monitor='val_loss', verbose=1, save_best_only=True)\n",
    "    earlystopper = EarlyStopping(monitor='val_loss', patience=1200, verbose=1)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=200, min_lr=0.0001, verbose=1)\n",
    "\n",
    "history = model.fit(x=train_sequence,\n",
    "                    validation_data=test_sequence,\n",
    "                    epochs=100, \n",
    "                    callbacks=[checkpointer, earlystopper, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5f6ecf0357e95e30953d0cf08844b8b26fdbdf1f780a6e218131c917612a57e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
