{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applies Deep Learning to ePodium dataset for prediction of Dyslexia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import Precision, BinaryAccuracy, Recall\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from functions import epodium_deep_learning\n",
    "from models.dnn import fully_connected_model\n",
    "from models.transformer import TransformerModel\n",
    "\n",
    "import local_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose which processed data to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab32725430d744d7997ac81110727672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='processing:', options=('autoreject', 'ransac'), value='autoreject')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processing_method_widget = ipywidgets.RadioButtons(options=['autoreject', 'ransac'], \n",
    "                                                   value='autoreject', \n",
    "                                                   description='processing:')\n",
    "display(processing_method_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Preparing data as input to the deep learning models.\n",
    "\n",
    "#### Check number of clean epochs* in each file after processing and split into train and test dataset\n",
    "\n",
    "*In the context of electroencephalography (EEG), *epochs* are EEG segments in which an event occurs. During processing, the epochs are chosen to be 1 second in which the event occurs at 0.2s. In the context of deep learning, *epochs* are iterations over the entire training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed: 220, bad: 40\n",
      "180 files have enough epochs for analysis.\n",
      "The dataset is split up into 132 train and 48 test experiments\n"
     ]
    }
   ],
   "source": [
    "if(processing_method_widget.value == \"autoreject\"):\n",
    "    path_processed = local_paths.ePod_processed_autoreject\n",
    "if(processing_method_widget.value == \"ransac\"):\n",
    "    path_processed = local_paths.ePod_processed_ransac\n",
    "\n",
    "train, test = epodium_deep_learning.split_train_test_datasets(path_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Iterator Sequence as input to feed the model\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 32, 512)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequence = epodium_deep_learning.EvokedDataIterator(train)\n",
    "test_sequence = epodium_deep_learning.EvokedDataIterator(test)\n",
    "\n",
    "x, y = train_sequence.__getitem__(0)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "The data is an *evoked* or *ERP* from a participant in the ePodium experiment. 60 EEG signals were averaged from -0.2 to +0.8 seconds after onset of an event. This is done for each of the 12 event types seperately.\n",
    "\n",
    "__dimensions__: \n",
    "+ x (batches, timesteps, channels)\n",
    "+ y (batches, labels)\n",
    "\n",
    "__labels__: \n",
    "+ (Sex, At risk of dyslexia, first standard, standard, deviant)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose Deep Learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ac45f8d8dc4c96890a6b47a6332531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='processing:', index=1, options=('fully_connected', 'transformer'), value='transformeâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_widget = ipywidgets.RadioButtons(options=['fully_connected', 'transformer'],\n",
    "                                       value='transformer', \n",
    "                                       description='processing:')\n",
    "display(model_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 16384) for input KerasTensor(type_spec=TensorSpec(shape=(None, 16384), dtype=tf.float32, name='input_5'), name='input_5', description=\"created by layer 'input_5'\"), but it was called on an input with incompatible shape (None, None, None).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 16384) for input KerasTensor(type_spec=TensorSpec(shape=(None, 16384), dtype=tf.float32, name='input_5'), name='input_5', description=\"created by layer 'input_5'\"), but it was called on an input with incompatible shape (None, None, None).\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.6443 - precision_3: 0.4055 - binary_accuracy: 0.4870 - recall_3: 0.6474 WARNING:tensorflow:Model was constructed with shape (None, 16384) for input KerasTensor(type_spec=TensorSpec(shape=(None, 16384), dtype=tf.float32, name='input_5'), name='input_5', description=\"created by layer 'input_5'\"), but it was called on an input with incompatible shape (None, None, None).\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.79692, saving model to /volume-ceph/floris_storage/models/transformer.hdf5\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "\nLayer TokenAndPositionEmbedding has arguments ['self', 'maxlen', 'embed_dim']\nin `__init__` and therefore must override `get_config()`.\n\nExample:\n\nclass CustomLayer(keras.layers.Layer):\n    def __init__(self, arg1, arg2):\n        super().__init__()\n        self.arg1 = arg1\n        self.arg2 = arg2\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"arg1\": self.arg1,\n            \"arg2\": self.arg2,\n        })\n        return config",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m checkpointer \u001b[38;5;241m=\u001b[39m ModelCheckpoint(filepath \u001b[38;5;241m=\u001b[39m model_path, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m reduce_lr \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_sequence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_sequence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpointer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/base_layer.py:745\u001b[0m, in \u001b[0;36mLayer.get_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;66;03m# Check that either the only argument in the `__init__` is  `self`,\u001b[39;00m\n\u001b[1;32m    743\u001b[0m \u001b[38;5;66;03m# or that `get_config` has been overridden:\u001b[39;00m\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(extra_args) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_config, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_is_default\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 745\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(textwrap\u001b[38;5;241m.\u001b[39mdedent(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;124m      Layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has arguments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextra_args\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;124m      in `__init__` and therefore must override `get_config()`.\u001b[39m\n\u001b[1;32m    748\u001b[0m \n\u001b[1;32m    749\u001b[0m \u001b[38;5;124m      Example:\u001b[39m\n\u001b[1;32m    750\u001b[0m \n\u001b[1;32m    751\u001b[0m \u001b[38;5;124m      class CustomLayer(keras.layers.Layer):\u001b[39m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;124m          def __init__(self, arg1, arg2):\u001b[39m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;124m              super().__init__()\u001b[39m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;124m              self.arg1 = arg1\u001b[39m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;124m              self.arg2 = arg2\u001b[39m\n\u001b[1;32m    756\u001b[0m \n\u001b[1;32m    757\u001b[0m \u001b[38;5;124m          def get_config(self):\u001b[39m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;124m              config = super().get_config()\u001b[39m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;124m              config.update(\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;124m                  \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg1\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: self.arg1,\u001b[39m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;124m                  \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg2\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: self.arg2,\u001b[39m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;124m              \u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;124m              return config\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m))\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m config\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: \nLayer TokenAndPositionEmbedding has arguments ['self', 'maxlen', 'embed_dim']\nin `__init__` and therefore must override `get_config()`.\n\nExample:\n\nclass CustomLayer(keras.layers.Layer):\n    def __init__(self, arg1, arg2):\n        super().__init__()\n        self.arg1 = arg1\n        self.arg2 = arg2\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"arg1\": self.arg1,\n            \"arg2\": self.arg2,\n        })\n        return config"
     ]
    }
   ],
   "source": [
    "if(model_widget.value == \"fully_connected\"): # TODO\n",
    "    model = fully_connected_model()\n",
    "    \n",
    "if(model_widget.value == \"transformer\"):\n",
    "    model = TransformerModel()\n",
    "\n",
    "# Save file with names of test set, so \n",
    "path_testset = os.path.join(local_paths.models, model_widget.value + \"_testset.txt\")\n",
    "\n",
    "with open(path_testset, 'w') as f:\n",
    "    for participant in test:\n",
    "        f.write(participant + '\\n')\n",
    "\n",
    "\n",
    "    \n",
    "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "              loss=BinaryCrossentropy(),\n",
    "              metrics=[Precision(), BinaryAccuracy(), Recall()])\n",
    "\n",
    "model_path = os.path.join(local_paths.models, model_widget.value + \".hdf5\")\n",
    "checkpointer = ModelCheckpoint(filepath = model_path, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, verbose=1)\n",
    "\n",
    "history = model.fit(x=train_sequence,\n",
    "                    validation_data=test_sequence,\n",
    "                    epochs=100,\n",
    "                    callbacks=[checkpointer, reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_extension = '.hdf5'\n",
    "\n",
    "path_models = glob.glob(os.path.join(local_paths.models, '*' + model_extension))\n",
    "\n",
    "models = []\n",
    "paths_model_file = []\n",
    "paths_model_history = []\n",
    "\n",
    "for path_model in path_models:\n",
    "    model_temp = os.path.splitext(path_model)[0]    \n",
    "    models.append(os.path.basename(model_temp))\n",
    "    paths_model_file.append(model_temp + model_extension)\n",
    "    paths_model_history.append(model_temp + '_history.npy')\n",
    "\n",
    "model = ipywidgets.Select(options=models, description='Models:')\n",
    "display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = np.load(paths_model_history[model.index], allow_pickle=True).item()\n",
    "display_helper.show_plot(x = range(len(history['loss'])), y = history['loss'] ,xlabel = \"epochs\", ylabel = \"loss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## test set:\n",
    "# with open(the_filename, 'r') as f:\n",
    "#     my_list = [line.rstrip('\\n') for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5f6ecf0357e95e30953d0cf08844b8b26fdbdf1f780a6e218131c917612a57e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
