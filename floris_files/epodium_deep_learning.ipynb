{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applies Deep Learning to ePodium dataset for prediction of Dyslexia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import Precision, BinaryAccuracy, Recall\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from functions import epodium_deep_learning\n",
    "from models.dnn import fully_connected_model\n",
    "from models.transformer import TransformerModel\n",
    "\n",
    "import local_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose which processed data to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab32725430d744d7997ac81110727672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='processing:', options=('autoreject', 'ransac'), value='autoreject')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processing_method_widget = ipywidgets.RadioButtons(options=['autoreject', 'ransac'], \n",
    "                                                   value='autoreject', \n",
    "                                                   description='processing:')\n",
    "display(processing_method_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Preparing data as input to the deep learning models.\n",
    "\n",
    "#### Check number of clean epochs* in each file after processing and split into train and test dataset\n",
    "\n",
    "*In the context of electroencephalography (EEG), *epochs* are EEG segments in which an event occurs. During processing, the epochs are chosen to be 1 second in which the event occurs at 0.2s. In the context of deep learning, *epochs* are iterations over the entire training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed: 220, bad: 40\n",
      "180 files have enough epochs for analysis.\n",
      "The dataset is split up into 132 train and 48 test experiments\n"
     ]
    }
   ],
   "source": [
    "if(processing_method_widget.value == \"autoreject\"):\n",
    "    path_processed = local_paths.ePod_processed_autoreject\n",
    "if(processing_method_widget.value == \"ransac\"):\n",
    "    path_processed = local_paths.ePod_processed_ransac\n",
    "\n",
    "train, test = epodium_deep_learning.split_train_test_datasets(path_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Iterator Sequence as input to feed the model\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 32, 512)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequence = epodium_deep_learning.EvokedDataIterator(train)\n",
    "test_sequence = epodium_deep_learning.EvokedDataIterator(test)\n",
    "\n",
    "x, y = train_sequence.__getitem__(0)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "The data is an *evoked* or *ERP* from a participant in the ePodium experiment. 60 EEG signals were averaged from -0.2 to +0.8 seconds after onset of an event. This is done for each of the 12 event types seperately.\n",
    "\n",
    "__dimensions__: \n",
    "+ x (batches, timesteps, channels)\n",
    "+ y (batches, labels)\n",
    "\n",
    "__labels__: \n",
    "+ (Sex, At risk of dyslexia, first standard, standard, deviant)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose Deep Learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ac45f8d8dc4c96890a6b47a6332531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='processing:', index=1, options=('fully_connected', 'transformer'), value='transformeâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_widget = ipywidgets.RadioButtons(options=['fully_connected', 'transformer'],\n",
    "                                       value='transformer', \n",
    "                                       description='processing:')\n",
    "display(model_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 16384) for input KerasTensor(type_spec=TensorSpec(shape=(None, 16384), dtype=tf.float32, name='input_3'), name='input_3', description=\"created by layer 'input_3'\"), but it was called on an input with incompatible shape (None, None, None).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 16384) for input KerasTensor(type_spec=TensorSpec(shape=(None, 16384), dtype=tf.float32, name='input_3'), name='input_3', description=\"created by layer 'input_3'\"), but it was called on an input with incompatible shape (None, None, None).\n",
      " 1/17 [>.............................] - ETA: 12:13 - loss: 2.0562 - precision_1: 0.3234 - binary_accuracy: 0.4271 - recall_1: 0.4833"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-31 16:46:50.590489: W tensorflow/core/framework/op_kernel.cc:1733] UNKNOWN: FileNotFoundError: [Errno 2] No such file or directory: '/volume-ceph/floris_storage/processed/ePod_autoreject/epochs_split/223b_GiepM_FS.npy'\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/fpauwels/.local/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/home/fpauwels/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/home/fpauwels/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1030, in generator_py_func\n",
      "    values = next(generator_state.get_iterator(iterator_id))\n",
      "\n",
      "  File \"/home/fpauwels/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 831, in wrapped_generator\n",
      "    for data in generator_fn():\n",
      "\n",
      "  File \"/home/fpauwels/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 957, in generator_fn\n",
      "    yield x[i]\n",
      "\n",
      "  File \"/home/fpauwels/eegyolk/floris_files/functions/epodium_deep_learning.py\", line 107, in __getitem__\n",
      "    npy = np.load(npy_path)\n",
      "\n",
      "  File \"/home/fpauwels/.local/lib/python3.8/site-packages/numpy/lib/npyio.py\", line 390, in load\n",
      "    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\n",
      "\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/volume-ceph/floris_storage/processed/ePod_autoreject/epochs_split/223b_GiepM_FS.npy'\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nFileNotFoundError: [Errno 2] No such file or directory: '/volume-ceph/floris_storage/processed/ePod_autoreject/epochs_split/223b_GiepM_FS.npy'\nTraceback (most recent call last):\n\n  File \"/home/fpauwels/.local/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n\n  File \"/home/fpauwels/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/home/fpauwels/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1030, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/home/fpauwels/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 831, in wrapped_generator\n    for data in generator_fn():\n\n  File \"/home/fpauwels/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 957, in generator_fn\n    yield x[i]\n\n  File \"/home/fpauwels/eegyolk/floris_files/functions/epodium_deep_learning.py\", line 107, in __getitem__\n    npy = np.load(npy_path)\n\n  File \"/home/fpauwels/.local/lib/python3.8/site-packages/numpy/lib/npyio.py\", line 390, in load\n    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\n\nFileNotFoundError: [Errno 2] No such file or directory: '/volume-ceph/floris_storage/processed/ePod_autoreject/epochs_split/223b_GiepM_FS.npy'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_5525]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m checkpointer \u001b[38;5;241m=\u001b[39m ModelCheckpoint(filepath \u001b[38;5;241m=\u001b[39m model_path, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m reduce_lr \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_sequence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_sequence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpointer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nFileNotFoundError: [Errno 2] No such file or directory: '/volume-ceph/floris_storage/processed/ePod_autoreject/epochs_split/223b_GiepM_FS.npy'\nTraceback (most recent call last):\n\n  File \"/home/fpauwels/.local/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n\n  File \"/home/fpauwels/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/home/fpauwels/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1030, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/home/fpauwels/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 831, in wrapped_generator\n    for data in generator_fn():\n\n  File \"/home/fpauwels/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 957, in generator_fn\n    yield x[i]\n\n  File \"/home/fpauwels/eegyolk/floris_files/functions/epodium_deep_learning.py\", line 107, in __getitem__\n    npy = np.load(npy_path)\n\n  File \"/home/fpauwels/.local/lib/python3.8/site-packages/numpy/lib/npyio.py\", line 390, in load\n    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\n\nFileNotFoundError: [Errno 2] No such file or directory: '/volume-ceph/floris_storage/processed/ePod_autoreject/epochs_split/223b_GiepM_FS.npy'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_5525]"
     ]
    }
   ],
   "source": [
    "if(model_widget.value == \"fully_connected\"): # TODO\n",
    "    model = fully_connected_model()\n",
    "    \n",
    "if(model_widget.value == \"transformer\"):\n",
    "    model = TransformerModel()\n",
    "\n",
    "# Save file with names of test set, so \n",
    "path_testset = os.path.join(local_paths.models, model_widget.value + \".txt\")\n",
    "\n",
    "with open(path_testset, 'w') as f:\n",
    "    for participant in test:\n",
    "        f.write(participant + '\\n')\n",
    "\n",
    "# with open(the_filename, 'r') as f:\n",
    "#     my_list = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "    \n",
    "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "              loss=BinaryCrossentropy(),\n",
    "              metrics=[Precision(), BinaryAccuracy(), Recall()])\n",
    "\n",
    "model_path = os.path.join(local_paths.models, model_widget.value + \".hdf5\")\n",
    "checkpointer = ModelCheckpoint(filepath = model_path, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, verbose=1)\n",
    "\n",
    "history = model.fit(x=train_sequence,\n",
    "                    validation_data=test_sequence,\n",
    "                    epochs=100,\n",
    "                    callbacks=[checkpointer, reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_extension = '.hdf5'\n",
    "\n",
    "path_models = glob.glob(os.path.join(local_paths.models, '*' + model_extension))\n",
    "\n",
    "models = []\n",
    "paths_model_file = []\n",
    "paths_model_history = []\n",
    "\n",
    "for path_model in path_models:\n",
    "    model_temp = os.path.splitext(path_model)[0]    \n",
    "    models.append(os.path.basename(model_temp))\n",
    "    paths_model_file.append(model_temp + model_extension)\n",
    "    paths_model_history.append(model_temp + '_history.npy')\n",
    "\n",
    "model = ipywidgets.Select(options=models, description='Models:')\n",
    "display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = np.load(paths_model_history[model.index], allow_pickle=True).item()\n",
    "display_helper.show_plot(x = range(len(history['loss'])), y = history['loss'] ,xlabel = \"epochs\", ylabel = \"loss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5f6ecf0357e95e30953d0cf08844b8b26fdbdf1f780a6e218131c917612a57e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
