{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trains Deep Learning Models to predict labels in ePodium dataset.\n",
    "\n",
    "TODO: More explanation notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, MeanSquaredError\n",
    "from tensorflow.keras.metrics import Precision, BinaryAccuracy, Recall\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import local_paths\n",
    "from functions import processing, display_helper, data_io\n",
    "from functions.epodium import Epodium\n",
    "epodium = Epodium()\n",
    "\n",
    "from models import transformer\n",
    "from models.dnn import fully_connected_model\n",
    "from models.hfawaz import cnn, encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='1'></a>\n",
    "## 1. Prepare Dataset\n",
    "\n",
    "__input dimensions__: \n",
    "+ x (batches, timesteps, channels)\n",
    "+ y (batches, labels)\n",
    "\n",
    "__labels__: \n",
    "+ Binary: Sex, At risk of dyslexia, Group a/b\n",
    "+ Regressive: Age, Vocabulary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"epodium\" # \"ddp\"\n",
    "\n",
    "if dataset_name == \"epodium\":\n",
    "    dataset = Epodium()\n",
    "    epochs_directory = local_paths.ePod_epochs\n",
    "    event_directory = local_paths.ePod_epochs_events\n",
    "    \n",
    "    epod_children, epod_cdi, epod_parents, epod_codes = \\\n",
    "        data_io.load_metadata(local_paths.ePod_metadata, epodium.metadata_filenames)\n",
    "    \n",
    "# TODO OUTPUT LABELS metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split processed epochs* into train and test sequence.\n",
    "\n",
    "*In the context of electroencephalography (EEG), *epochs* are EEG segments in which an event occurs. During processing, the epochs are chosen to be 1 second in which the event occurs at 0.2s. In the context of deep learning, *epochs* are iterations over the entire training dataset.\n",
    "\n",
    "First choose which processed data to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed: 229, bad: 43\n",
      "186 experiments have enough epochs for analysis.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Epodium' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m experiment_list \u001b[38;5;241m=\u001b[39m processing\u001b[38;5;241m.\u001b[39mvalid_experiments(dataset, event_directory, min_standards\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m180\u001b[39m, min_deviants\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m train, test \u001b[38;5;241m=\u001b[39m \u001b[43mepodium\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_train_test_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eegyolk/floris_files/functions/epodium.py:110\u001b[0m, in \u001b[0;36mEpodium.split_train_test_datasets\u001b[0;34m(experiment_list, test_size)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03mEach participant that exceeds the minimum epochs is put into a test or train set.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03mBoth the train and test sets have the same proportion of participants that did either a, b, or both experiments\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m    \n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Initialise same proportion of participants that did either a, b, or both experiments \u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m a_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(exp[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m exp \u001b[38;5;129;01min\u001b[39;00m experiment_list \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m exp)\n\u001b[1;32m    111\u001b[0m b_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(exp[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m exp \u001b[38;5;129;01min\u001b[39;00m experiment_list \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m exp)\n\u001b[1;32m    113\u001b[0m experiments_a_and_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(a_set\u001b[38;5;241m.\u001b[39mintersection(b_set))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Epodium' object is not iterable"
     ]
    }
   ],
   "source": [
    "experiment_list = processing.valid_experiments(dataset, event_directory, min_standards=180, min_deviants=80)\n",
    "train, test = epodium.split_train_test_datasets(experiment_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing data iterator (Sequence) as input to the deep learning models.\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence = epodium.EvokedDataIterator(train, , gaussian_noise=1e-6)\n",
    "test_sequence = epodium.EvokedDataIterator(test, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise data instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = test_sequence.__getitem__(6)\n",
    "print(f\"The shape of one data instance is {x[0].shape}\")\n",
    "\n",
    "index = 15 # 0 to 63\n",
    "epodium.plot_array_as_evoked(x[index][:32], frequency=128)\n",
    "epodium.plot_array_as_evoked(x[index][32:], frequency=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "The data is an *evoked* or *ERP* from a participant in the ePodium experiment. 60 EEG signals were averaged from -0.2 to +0.8 seconds after onset of an event. This is done for each of the 12 event types seperately.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"encoder_age_128_3\"\n",
    "model = encoder((64,128), 1)\n",
    "epochs = 300\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# Paths to save model info\n",
    "base_path = os.path.join(local_paths.models, model_name)\n",
    "\n",
    "path_history = os.path.join(base_path, \"history.npy\")\n",
    "path_model = os.path.join(base_path, \"model\")\n",
    "path_testset = os.path.join(base_path, \"testset.txt\")\n",
    "path_weights = os.path.join(base_path, \"weights.h5\")\n",
    "\n",
    "if os.path.exists(path_model):\n",
    "    print(f\"Model: '{model_name}' already exist. Delete the existing model first or rename this model.\")    \n",
    "else:\n",
    "    print(f\"Create model: {model_name}\")\n",
    "    if not os.path.exists(base_path):\n",
    "        os.mkdir(base_path)\n",
    "\n",
    "    # Save validation-set for future testing\n",
    "    with open(path_testset, 'w') as f:\n",
    "        for participant in test:\n",
    "            f.write(participant + '\\n')\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=MeanSquaredError()) # , metrics=[Precision(), BinaryAccuracy(), Recall()]\n",
    "\n",
    "    # Fit model\n",
    "    checkpointer = ModelCheckpoint(filepath=path_weights, monitor='val_loss', verbose=1, save_weights_only=True, save_best_only=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=10, factor=0.7, verbose=1) # add to callbacks if uncomment\n",
    "    history = model.fit(x=train_sequence, validation_data=test_sequence, epochs=epochs, callbacks=[checkpointer])\n",
    "\n",
    "    np.save(path_history, history.history)\n",
    "    model.save(path_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_helper.show_plot(x=range(len(history.history['loss'])), y=history.history['loss'], xlabel=\"epochs\", ylabel=\"validation loss\", title=f\"Loss during training ({model_name})\")\n",
    "display_helper.show_plot(x=range(len(history.history['loss'])), y=history.history['val_loss'], xlabel=\"epochs\", ylabel=\"validation loss\", title=f\"Validation loss during training ({model_name})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5f6ecf0357e95e30953d0cf08844b8b26fdbdf1f780a6e218131c917612a57e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
