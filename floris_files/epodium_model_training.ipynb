{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trains Deep Learning Models to predict labels in ePodium dataset.\n",
    "\n",
    "__input dimensions__: \n",
    "+ x (batches, timesteps, channels)\n",
    "+ y (batches, labels)\n",
    "\n",
    "__labels__: \n",
    "+ Binary: Sex, At risk of dyslexia, Group a/b\n",
    "+ Regressive: Age, Vocabulary\n",
    "\n",
    "TODO: More explanation notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 21:13:35.812642: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-28 21:13:35.812693: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, MeanSquaredError\n",
    "from tensorflow.keras.metrics import Precision, BinaryAccuracy, Recall\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import local_paths\n",
    "from functions import epodium, epodium_deep_learning, display_helper\n",
    "\n",
    "from models import transformer\n",
    "from models.dnn import fully_connected_model\n",
    "from models.hfawaz import cnn, encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Input Preparation\n",
    "\n",
    "\n",
    "#### Split processed epochs* into train and test sequence.\n",
    "\n",
    "*In the context of electroencephalography (EEG), *epochs* are EEG segments in which an event occurs. During processing, the epochs are chosen to be 1 second in which the event occurs at 0.2s. In the context of deep learning, *epochs* are iterations over the entire training dataset.\n",
    "\n",
    "First choose which processed data to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'local_paths' has no attribute 'clean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m processing_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautoreject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m experiment_list \u001b[38;5;241m=\u001b[39m \u001b[43mepodium_deep_learning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclean_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessing_method\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m train, test \u001b[38;5;241m=\u001b[39m epodium_deep_learning\u001b[38;5;241m.\u001b[39msplit_train_test_datasets(experiment_list)\n",
      "File \u001b[0;32m~/eegyolk/floris_files/functions/epodium_deep_learning.py:32\u001b[0m, in \u001b[0;36mclean_experiments\u001b[0;34m(cleaning_method, min_standards, min_deviants, min_firststandards)\u001b[0m\n\u001b[1;32m     29\u001b[0m metadata_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(local_paths\u001b[38;5;241m.\u001b[39mePod_metadata, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchildren.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m metadata \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_table(metadata_path)\n\u001b[0;32m---> 32\u001b[0m path_events \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mlocal_paths\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclean\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mePod_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m cleaning_method, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevents\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.txt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path_event \u001b[38;5;129;01min\u001b[39;00m path_events:\n\u001b[1;32m     34\u001b[0m     file_event \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(path_event)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'local_paths' has no attribute 'clean'"
     ]
    }
   ],
   "source": [
    "processing_method = \"autoreject\"\n",
    "experiment_list = epodium_deep_learning.clean_experiments(processing_method)\n",
    "train, test = epodium_deep_learning.split_train_test_datasets(experiment_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing data iterator (Sequence) as input to the deep learning models.\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence = epodium_deep_learning.EvokedDataIterator(train, \"autoreject_128hz\", gaussian_noise=1e-6)\n",
    "test_sequence = epodium_deep_learning.EvokedDataIterator(test, \"autoreject_128hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise data instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = test_sequence.__getitem__(6)\n",
    "print(f\"The shape of one data instance is {x[0].shape}\")\n",
    "\n",
    "index = 15 # 0 to 63\n",
    "epodium.plot_array_as_evoked(x[index][:32], frequency=128)\n",
    "epodium.plot_array_as_evoked(x[index][32:], frequency=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "The data is an *evoked* or *ERP* from a participant in the ePodium experiment. 60 EEG signals were averaged from -0.2 to +0.8 seconds after onset of an event. This is done for each of the 12 event types seperately.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"encoder_age_128_3\"\n",
    "model = encoder((64,128), 1)\n",
    "epochs = 300\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# Paths to save model info\n",
    "base_path = os.path.join(local_paths.models, model_name)\n",
    "\n",
    "path_history = os.path.join(base_path, \"history.npy\")\n",
    "path_model = os.path.join(base_path, \"model\")\n",
    "path_testset = os.path.join(base_path, \"testset.txt\")\n",
    "path_weights = os.path.join(base_path, \"weights.h5\")\n",
    "\n",
    "if os.path.exists(path_model):\n",
    "    print(f\"Model: '{model_name}' already exist. Delete the existing model first or rename this model.\")    \n",
    "else:\n",
    "    print(f\"Create model: {model_name}\")\n",
    "    if not os.path.exists(base_path):\n",
    "        os.mkdir(base_path)\n",
    "\n",
    "    # Save validation-set for future testing\n",
    "    with open(path_testset, 'w') as f:\n",
    "        for participant in test:\n",
    "            f.write(participant + '\\n')\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=MeanSquaredError()) # , metrics=[Precision(), BinaryAccuracy(), Recall()]\n",
    "\n",
    "    # Fit model\n",
    "    checkpointer = ModelCheckpoint(filepath=path_weights, monitor='val_loss', verbose=1, save_weights_only=True, save_best_only=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=10, factor=0.7, verbose=1) # add to callbacks if uncomment\n",
    "    history = model.fit(x=train_sequence, validation_data=test_sequence, epochs=epochs, callbacks=[checkpointer])\n",
    "\n",
    "    np.save(path_history, history.history)\n",
    "    model.save(path_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_helper.show_plot(x=range(len(history.history['loss'])), y=history.history['loss'], xlabel=\"epochs\", ylabel=\"validation loss\", title=f\"Loss during training ({model_name})\")\n",
    "display_helper.show_plot(x=range(len(history.history['loss'])), y=history.history['val_loss'], xlabel=\"epochs\", ylabel=\"validation loss\", title=f\"Validation loss during training ({model_name})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5f6ecf0357e95e30953d0cf08844b8b26fdbdf1f780a6e218131c917612a57e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
