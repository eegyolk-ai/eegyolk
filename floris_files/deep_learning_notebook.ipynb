{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Learning\n",
    "\n",
    "+ A model processes data and lables \n",
    "+ The model optimizes through a training loop\n",
    "+ New dummy data is generated for each training loop\n",
    "+ Able to use multiple models (standard is FNN)\n",
    "+ Introduce Transformer model\n",
    "\n",
    "TODO:\n",
    "+ Show the confidence of a prediction (with softmax probability between 0 and 1) \n",
    "+ Compare multiple models on dummy data (ML, FNN, CNN, RNN, Transformer(Encoder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as sk\n",
    "import numpy as np   \n",
    "import matplotlib.pyplot as plt\n",
    "import os              \n",
    "import sys\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from IPython.display import clear_output\n",
    "\n",
    "main_path = os.path.dirname(os.getcwd())\n",
    "eegyolk_path = os.path.join(main_path, 'eegyolk')\n",
    "sys.path.insert(0, eegyolk_path)\n",
    "from eegyolk import dummy_data_functions as dummy\n",
    "from eegyolk import display_helper as disp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialise Model\n",
    "\n",
    "Feedforward neural network (FNN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(1024,)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='sgd', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "planck_distribution = dummy.generate_frequency_distribution(\"planck\")\n",
    "const_distribution = dummy.generate_frequency_distribution(\"constant\")\n",
    "\n",
    "def create_batch(batch_size):\n",
    "  X = []\n",
    "  Y = np.zeros(batch_size)\n",
    "\n",
    "  for i in range(batch_size):\n",
    "      if random.random() < 0.5:\n",
    "          X.append(dummy.generate_epoch(const_distribution))\n",
    "      else:\n",
    "          X.append(dummy.generate_epoch(planck_distribution))\n",
    "          Y[i] = 1\n",
    "\n",
    "  return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.7484 - precision_2: 0.6119 - binary_accuracy: 0.5625 - recall_2: 0.5775\n",
      "Epoch 2/4\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.8894 - precision_2: 0.5763 - binary_accuracy: 0.5156 - recall_2: 0.4789\n",
      "Epoch 3/4\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.8324 - precision_2: 0.6418 - binary_accuracy: 0.5938 - recall_2: 0.6056\n",
      "Epoch 4/4\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.9115 - precision_2: 0.5493 - binary_accuracy: 0.5000 - recall_2: 0.5493\n",
      "4/4 - 0s - loss: 0.5410 - precision_2: 0.6863 - binary_accuracy: 0.7422 - recall_2: 0.9859 - 49ms/epoch - 12ms/step\n"
     ]
    }
   ],
   "source": [
    "for j in range(10):\n",
    "  X_train, Y_train = create_batch(batch_size)\n",
    "  model.fit(np.array(X_train), Y_train, epochs=4, batch_size=10)\n",
    "\n",
    "  X_test, Y_test = create_batch(batch_size)\n",
    "  model.evaluate(np.array(X_test),  Y_test, verbose=2)\n",
    "\n",
    "  clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer model:\n",
    "\n",
    "Run *training loop* again after this cell, to use this transformer model instead of the feedforward NN.\n",
    "\n",
    "The model is originally from:\n",
    "+ Author: Bruce Shuyue Jia\n",
    "+ Source: https://github.com/SuperBruceJia/EEG-DL/blob/master/Models/main-Transformer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "  X_train, Y_train = create_batch(batch_size)\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.5):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential([layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim), ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out = self.layernorm2(out1 + ffn_output)\n",
    "        return out\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = tf.reshape(x, [-1, maxlen, embed_dim])\n",
    "        out = x + positions\n",
    "        return out\n",
    "\n",
    "maxlen = 16      # Only consider 3 input time points\n",
    "embed_dim = 64  # Features of each time point\n",
    "num_heads = 8   # Number of attention heads\n",
    "ff_dim = 64     # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "# Input Time-series\n",
    "inputs = layers.Input(shape=(maxlen*embed_dim,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "\n",
    "# Encoder Architecture\n",
    "transformer_block_1 = TransformerBlock(embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim)\n",
    "transformer_block_2 = TransformerBlock(embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim)\n",
    "x = transformer_block_1(x)\n",
    "x = transformer_block_2(x)\n",
    "\n",
    "# Output\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[tf.keras.metrics.Precision(), tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.Recall()])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5f6ecf0357e95e30953d0cf08844b8b26fdbdf1f780a6e218131c917612a57e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('VENV')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
