{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "In this notebook the *Dutch Dyslexia Programme (DDP)* and *ePodium* datasets are used to train a deep neural network model.\n",
    "\n",
    "The model is trained to predict the age and risk of dyslexia from brain signals. This input data consists of the average of multiple epochs of the EEG data, called an *Event Related Potential* (ERP).\n",
    "\n",
    "+ In section 1. [Prepare Dataset](#1mt) the ePodium and DDP dataset are prepared for input in the deep learning model. The metadata containing the participant info is loaded and the dataset is split into a train, test and validation set. The data is loaded with the *Sequencer* class from TensorFlow, which iterates over the participants in the sets when the data is needed by the model.\n",
    "+ In section 2. [Deep Learning](#2mt) a deep neural network is trained to predict the age and risk of dyslexia from the ERPs from toddlers. The model types that are used are the *encoder*, *resnet* models. The models, training history, and subset contents are saved in *local_paths.models*.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "    It is recommended to run this notebook with CUDA enabled with a dedicated graphics card to speed-up the model training.\n",
    "    \n",
    "    In the context of electroencephalography, 'epochs' are EEG intervals in which an event occurs. In this notebook 'epochs' are used in the context of deep learning, in which epochs are iterations over the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Local\n",
    "import local_paths\n",
    "from functions import processing, display_helper, data_io\n",
    "from functions.epodium import Epodium\n",
    "from functions.ddp import DDP\n",
    "from functions.sequences import EpodiumSequence, DDPSequence\n",
    "\n",
    "# Models\n",
    "from models.dl_4_tsc import encoder_model, fully_convolutional_model, resnet_model\n",
    "from models.eeg_dl import transformer_model\n",
    "\n",
    "# Tensorflow dependencies\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import BinaryCrossentropy, MeanSquaredError\n",
    "from keras.metrics import Precision, BinaryAccuracy, Recall\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1mt'></a>\n",
    "## 1. Prepare Data\n",
    "\n",
    "#### Choose dataset\n",
    "\n",
    "This notebook works with both the ePodium and the DDP dataset. Choose which dataset to use by changing the variable: *dataset_name*. \n",
    "\n",
    "The *dataset* variable contains information and functions about the specific dataset. The directories to the data and the labels of the selected dataset are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose between datasets: \"epodium\" \"ddp\"\n",
    "dataset_name = \"epodium\"\n",
    "\n",
    "if dataset_name == \"epodium\":\n",
    "    dataset = Epodium()    \n",
    "    event_directory = local_paths.ePod_epochs_events    \n",
    "    epod_labels = dataset.create_labels(local_paths.ePod_metadata)\n",
    "    print(f\"The available labels are:\\n {list(epod_labels.columns)}\")\n",
    "\n",
    "elif dataset_name == \"ddp\":\n",
    "    dataset = DDP()\n",
    "    event_directory = local_paths.DDP_epochs_events    \n",
    "    directory_age_metadata = os.path.join(local_paths.DDP_metadata, \"ages\")\n",
    "    ddp_labels = dataset.create_labels(local_paths.DDP_dataset, directory_age_metadata)\n",
    "    print(f\"The available labels are:\\n {list(ddp_labels.columns)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Split valid experiments into train, validation, test set.\n",
    "\n",
    "1. Get only the valid experiments from each participants. The minimum amount of *standard* and *deviant* trials that need to be in each experiment can be selected.\n",
    "\n",
    "2. The data is split up into a train, validation, and test set. Each participant can have multiple experiments. The data is split up according to participants, so that no two experiments from the same participant are in multiple sets. The ratio *r* of the subsets is set to 70% test, 15% validation and 15% test set size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_list = processing.valid_experiments(dataset, event_directory, min_standards=180, min_deviants=80)\n",
    "\n",
    "# In the DDP dataset some experiments are ignored due to incorrect channels \n",
    "if dataset_name == \"ddp\":\n",
    "    experiment_list = list(set(experiment_list)-set(dataset.wrong_channels_experiments))\n",
    "    print(f\"{len(dataset.wrong_channels_experiments)} experiments have incorrect channels. \"\n",
    "          f\"{len(experiment_list)} experiments remain\")\n",
    "\n",
    "# [train / test / validation] ratio\n",
    "r = np.array([0.7, 0.15, 0.15])\n",
    "experiments_train_val, experiments_test = dataset.split_dataset(experiment_list, (r[0]+r[2])/r.sum())\n",
    "experiments_train, experiments_val = dataset.split_dataset(experiments_train_val, r[0]/(r[0]+r[2]))\n",
    "\n",
    "print(f\"\\nThe dataset is split up into {len(experiments_train)} train, \"\n",
    "      f\"{len(experiments_test)} test, and {len(experiments_val)} validation experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing a 'Sequence' as input to the deep learning models.\n",
    "\n",
    "In this deep learning notebook, the model iterates over a sequence of data. Each data instance in this sequence is only loaded when it is used, and unloaded when the model is done using the data instance. In Tensorflow, such a sequence is implemented with the *Sequence* class:\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence\n",
    "\n",
    "This notebook uses an extended version of this *Sequence* class for both the ePodium and the DDP dataset: *EpodiumSequence*, *DDPSequence*. When instantiating, some of the variables can be varied without breaking the code. These variables are discussed below.\n",
    "\n",
    "+ *n_trials_averaged*: The number of trials that are averaged together to get the ERP that is the input to the model. A lower number means more data-points, while a higher number of averaged trials reduces the noise.\n",
    "+ *gaussian_noise*: Noise can be artificially added to the data to reduce overfitting on the training set. The value of this parameter indicates the variation of the noise that is added to each individual time-step of each channel. \n",
    "+ *batch_size*: The number of experiments that are put into a single batch. In deep learning, a model is updated after processing a batch. A lower batch size means more updates, while a larger batch size has the advantage of less variations in the updates.\n",
    "\n",
    "\n",
    "The *train_sequence* is used for training the model, and the *val_sequence* is used to measure the actual performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == \"epodium\":\n",
    "    train_sequence = EpodiumSequence(experiments_train, epod_labels, local_paths.ePod_epochs_ddp_dims, \n",
    "                                     batch_size=2, n_trials_averaged=30,\n",
    "                                     input_type=\"standard\", label='age', standardise=True)\n",
    "    val_sequence = EpodiumSequence(experiments_val, epod_labels, local_paths.ePod_epochs_ddp_dims,\n",
    "                                   batch_size=2, n_trials_averaged=30,\n",
    "                                   input_type=\"standard\", label='age', standardise=True)\n",
    "    \n",
    "if dataset_name == \"ddp\":\n",
    "    train_sequence = DDPSequence(experiments_train, ddp_labels, local_paths.DDP_epochs, \n",
    "                                 batch_size=8, n_trials_averaged=30, gaussian_noise=1e-6, standardise=True)\n",
    "    val_sequence = DDPSequence(experiments_val, ddp_labels, local_paths.DDP_epochs, \n",
    "                               batch_size=8, n_trials_averaged=30, standardise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "<a id='2mt'></a>\n",
    "## 2. Deep Learning\n",
    "\n",
    "Now that the data is set-up, the training model created. The model dimensions are set to the dimensions of the input data and predicted output label(s): \n",
    "\n",
    "+ Input x has dimensions: *(batches, timesteps, channels)*\n",
    "+ Output y has dimensions: *(batches, labels)*\n",
    "\n",
    "An ERP consists of 2-dimensional input data. One of the dimensions represents the time where *n_timesteps* contains the number of timesteps. The other dimension represents the channels, i.e. the sensor locations on the scalp. The variable *n_channels* signifies the number of channels in the ERP.\n",
    "\n",
    "The model is trained to predict the output y from an input x. When *y_dimension* is set to 1, the model outputs a single floating point number from the ERP data input. This number can represent a regressive label like *age* and the *risk of dyslexia*.\n",
    "\n",
    "#### Choose model\n",
    "\n",
    "Multiple model types can be trained. To pick a model, change the *model_name* variable to contain the desired model. The model options are: **encoder** / **transformer** / **resnet**. Of course, it is also possible to import and use other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"epod_encoder_std_transfer_1\"\n",
    "n_channels = 26 # 32/26\n",
    "n_timesteps = 501 # 2049/501\n",
    "\n",
    "# Model dimensions\n",
    "x_dimension = (n_channels, n_timesteps)\n",
    "y_dimension = 1\n",
    "\n",
    "if \"transfer\" in model_name:\n",
    "    trained_model = \"ddp_encoder_age_std1_3\"\n",
    "    base_path = os.path.join(local_paths.models, trained_model)\n",
    "    path_model = os.path.join(base_path, \"model\")\n",
    "    path_weights = os.path.join(base_path, \"weights.h5\")\n",
    "    model = tf.keras.models.load_model(path_model)\n",
    "    model.load_weights(path_weights)\n",
    "    \n",
    "elif \"encoder\" in model_name:\n",
    "    model = encoder_model(x_dimension, y_dimension, final_activation=None)\n",
    "elif \"resnet\" in model_name:\n",
    "    model = resnet_model(x_dimension, y_dimension)\n",
    "else:\n",
    "    print(\"No model found. Add a model to the model name.\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train model\n",
    "\n",
    "The models, training history, and subset contents are saved in the folder *model_name* inside *local_paths.models*. \n",
    "\n",
    "Multiple hyperparameters can be adjusted:\n",
    "+ *epochs*: number of iterations over the dataset\n",
    "+ *learning_rate*: step size in optimizing the model parameters at each update\n",
    "\n",
    "*Adam* is used as an optimizer, since this optimizer performs well in a wide variety of cases. The loss of the optimizer is set to *MeanSquaredError*. This is a commonly used loss function in regression analysis where a 2x increase in error corresponds to a 4x increase in the loss.\n",
    "\n",
    "The model weights are saved when the validation loss is improved with the *ModelCheckpoint* callback. If however the validation loss is not improved, the learning rate is reduced with the *ReduceLROnPlateau* callback. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 50\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Paths for saving model data\n",
    "base_dir = os.path.join(local_paths.models, model_name)\n",
    "model_dir = os.path.join(base_dir, \"model\")\n",
    "subsets_dir = os.path.join(base_dir, \"subsets\")\n",
    "path_history = os.path.join(base_dir, \"history.npy\")\n",
    "path_weights = os.path.join(base_dir, \"weights.h5\")\n",
    "\n",
    "if os.path.exists(model_dir):\n",
    "    print(f\"Model: '{model_name}' already exist. Delete the existing model first or rename this model.\")    \n",
    "else:\n",
    "    print(f\"Create model: {model_name}\")\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.mkdir(base_dir)\n",
    "    if not os.path.exists(subsets_dir):\n",
    "        os.mkdir(subsets_dir)\n",
    "\n",
    "    # Save train / test / validation sets for future testing\n",
    "    data_io.save_experiment_names(experiments_train, os.path.join(subsets_dir, \"train_set.txt\"))\n",
    "    data_io.save_experiment_names(experiments_test, os.path.join(subsets_dir, \"test_set.txt\"))\n",
    "    data_io.save_experiment_names(experiments_val, os.path.join(subsets_dir, \"validation_set.txt\"))\n",
    "\n",
    "    # Model configurations\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=MeanSquaredError())\n",
    "    checkpointer = ModelCheckpoint(filepath=path_weights, monitor='val_loss', verbose=1, save_weights_only=True, save_best_only=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=1, factor=0.9, verbose=1)\n",
    "    \n",
    "    # Fit model\n",
    "    history = model.fit(x=train_sequence, validation_data=val_sequence, epochs=epochs, callbacks=[checkpointer, reduce_lr])\n",
    "    \n",
    "    # Save model and training history\n",
    "    model.save(model_dir)\n",
    "    np.save(path_history, history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_helper.show_plot(x=range(len(history.history['loss'])), \n",
    "                         y=[history.history['loss'], history.history['val_loss']], \n",
    "                         legend=[\"loss\",\"validation loss\"], \n",
    "                         xlabel=\"epochs\", ylabel=\"validation loss (MSE)\", \n",
    "                         title=f\"Loss during training ({model_name})\",\n",
    "                         xlim=[0,20], ylim=[0,0.1])\n",
    "                         # xlim=[0,100], ylim=[0,150000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5f6ecf0357e95e30953d0cf08844b8b26fdbdf1f780a6e218131c917612a57e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
