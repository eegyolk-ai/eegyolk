{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "In this notebook the *Dutch Dyslexia Programme (DDP)* and *ePodium* datasets are used to train a deep neural network model.\n",
    "\n",
    "The model is trained to predict the age and risk of dyslexia from brain signals. This input data consists of the average of multiple epochs of the EEG data, called an *Event Related Potential* (ERP).\n",
    "\n",
    "+ In section 1. [Prepare Dataset](#1mt) the ePodium and DDP dataset are prepared for input in the deep learning model. The metadata containing the participant info is loaded and the dataset is split into a train, test and validation set. The data is loaded with the *Sequencer* class from TensorFlow, which iterates over the participants in the sets when the data is needed by the model.\n",
    "+ In section 2. [Deep Learning](#2mt) a deep neural network is trained to predict the age and risk of dyslexia from the ERPs from toddlers. The model types that are used are the *encoder*, *resnet* models. The models, training history, and subset contents are saved in *local_paths.models*.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "    It is recommended to run this notebook with CUDA enabled with a dedicated graphics card to speed-up the model training.\n",
    "    \n",
    "    In the context of electroencephalography, 'epochs' are EEG intervals in which an event occurs. In this notebook 'epochs' are used in the context of deep learning, in which epochs are iterations over the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 12:26:44.631015: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-18 12:26:45.429615: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-18 12:26:45.809666: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-18 12:26:47.821267: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-18 12:26:47.821467: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-18 12:26:47.821473: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Local\n",
    "import local_paths\n",
    "from functions import processing, display_helper, data_io\n",
    "from functions.epodium import Epodium\n",
    "from functions.ddp import DDP\n",
    "from functions.sequences import EpodiumSequence, DDPSequence\n",
    "\n",
    "# Models\n",
    "from models.dl_4_tsc import encoder_model, fully_convolutional_model, resnet_model\n",
    "from models.eeg_dl import transformer_model\n",
    "\n",
    "# Tensorflow dependencies\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import BinaryCrossentropy, MeanSquaredError\n",
    "from keras.metrics import Precision, BinaryAccuracy, Recall\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1mt'></a>\n",
    "## 1. Prepare Data\n",
    "\n",
    "#### Choose dataset\n",
    "\n",
    "This notebook works with both the ePodium and the DDP dataset. Choose which dataset to use by changing the variable: *dataset_name*. \n",
    "\n",
    "The *dataset* variable contains information and functions about the specific dataset. The directories to the data and the labels of the selected dataset are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The available labels are:\n",
      " ['Participant', 'Age_days_a', 'Age_days_b', 'Risk_of_dyslexia', 'Dyslexia_score']\n"
     ]
    }
   ],
   "source": [
    "# Choose between datasets: \"epodium\" \"ddp\"\n",
    "dataset_name = \"epodium\"\n",
    "\n",
    "if dataset_name == \"epodium\":\n",
    "    dataset = Epodium()    \n",
    "    event_directory = local_paths.ePod_epochs_events    \n",
    "    epod_labels = dataset.create_labels(local_paths.ePod_metadata)\n",
    "    print(f\"The available labels are:\\n {list(epod_labels.columns)}\")\n",
    "\n",
    "elif dataset_name == \"ddp\":\n",
    "    dataset = DDP()\n",
    "    event_directory = local_paths.DDP_epochs_events    \n",
    "    directory_age_metadata = os.path.join(local_paths.DDP_metadata, \"ages\")\n",
    "    ddp_labels = dataset.create_labels(local_paths.DDP_dataset, directory_age_metadata)\n",
    "    print(f\"The available labels are:\\n {list(ddp_labels.columns)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Split valid experiments into train, validation, test set.\n",
    "\n",
    "1. Get only the valid experiments from each participants. The minimum amount of *standard* and *deviant* trials that need to be in each experiment can be selected.\n",
    "\n",
    "2. The data is split up into a train, validation, and test set. Each participant can have multiple experiments. The data is split up according to participants, so that no two experiments from the same participant are in multiple sets. The ratio *r* of the subsets is set to 70% test, 15% validation and 15% test set size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed: 248 bad: 62\n",
      "186 experiments have enough epochs for analysis.\n",
      "\n",
      "The dataset is split up into 127 train, 30 test, and 29 validation experiments\n"
     ]
    }
   ],
   "source": [
    "experiment_list = processing.valid_experiments(dataset, event_directory, min_standards=180, min_deviants=80)\n",
    "\n",
    "# In the DDP dataset some experiments are ignored due to incorrect channels \n",
    "if dataset_name == \"ddp\":\n",
    "    experiment_list = list(set(experiment_list)-set(dataset.wrong_channels_experiments))\n",
    "    print(f\"{len(dataset.wrong_channels_experiments)} experiments have incorrect channels. \"\n",
    "          f\"{len(experiment_list)} experiments remain\")\n",
    "\n",
    "# [train / test / validation] ratio\n",
    "r = np.array([0.7, 0.15, 0.15])\n",
    "experiments_train_val, experiments_test = dataset.split_dataset(experiment_list, (r[0]+r[2])/r.sum())\n",
    "experiments_train, experiments_val = dataset.split_dataset(experiments_train_val, r[0]/(r[0]+r[2]))\n",
    "\n",
    "print(f\"\\nThe dataset is split up into {len(experiments_train)} train, \"\n",
    "      f\"{len(experiments_test)} test, and {len(experiments_val)} validation experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing a 'Sequence' as input to the deep learning models.\n",
    "\n",
    "In this deep learning notebook, the model iterates over a sequence of data. Each data instance in this sequence is only loaded when it is used, and unloaded when the model is done using the data instance. In Tensorflow, such a sequence is implemented with the *Sequence* class:\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence\n",
    "\n",
    "This notebook uses an extended version of this *Sequence* class for both the ePodium and the DDP dataset: *EpodiumSequence*, *DDPSequence*. When instantiating, some of the variables can be varied without breaking the code. These variables are discussed below.\n",
    "\n",
    "+ *n_trials_averaged*: The number of trials that are averaged together to get the ERP that is the input to the model. A lower number means more data-points, while a higher number of averaged trials reduces the noise.\n",
    "+ *gaussian_noise*: Noise can be artificially added to the data to reduce overfitting on the training set. The value of this parameter indicates the variation of the noise that is added to each individual time-step of each channel. \n",
    "+ *batch_size*: The number of experiments that are put into a single batch. In deep learning, a model is updated after processing a batch. A lower batch size means more updates, while a larger batch size has the advantage of less variations in the updates.\n",
    "\n",
    "\n",
    "The *train_sequence* is used for training the model, and the *val_sequence* is used to measure the actual performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == \"epodium\":\n",
    "    train_sequence = EpodiumSequence(experiments_train, epod_labels, local_paths.ePod_epochs_ddp_dims, \n",
    "                                     batch_size=2, n_trials_averaged=30,\n",
    "                                     input_type=\"standard\", label='age', standardise=True)\n",
    "    val_sequence = EpodiumSequence(experiments_val, epod_labels, local_paths.ePod_epochs_ddp_dims,\n",
    "                                   batch_size=2, n_trials_averaged=30,\n",
    "                                   input_type=\"standard\", label='age', standardise=True)\n",
    "    \n",
    "if dataset_name == \"ddp\":\n",
    "    train_sequence = DDPSequence(experiments_train, ddp_labels, local_paths.DDP_epochs, \n",
    "                                 batch_size=8, n_trials_averaged=30, gaussian_noise=1e-6, standardise=True)\n",
    "    val_sequence = DDPSequence(experiments_val, ddp_labels, local_paths.DDP_epochs, \n",
    "                               batch_size=8, n_trials_averaged=30, standardise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "<a id='2mt'></a>\n",
    "## 2. Deep Learning\n",
    "\n",
    "Now that the data is set-up, the training model created. The model dimensions are set to the dimensions of the input data and predicted output label(s): \n",
    "\n",
    "+ Input x has dimensions: *(batches, timesteps, channels)*\n",
    "+ Output y has dimensions: *(batches, labels)*\n",
    "\n",
    "An ERP consists of 2-dimensional input data. One of the dimensions represents the time where *n_timesteps* contains the number of timesteps. The other dimension represents the channels, i.e. the sensor locations on the scalp. The variable *n_channels* signifies the number of channels in the ERP.\n",
    "\n",
    "The model is trained to predict the output y from an input x. When *y_dimension* is set to 1, the model outputs a single floating point number from the ERP data input. This number can represent a regressive label like *age* and the *risk of dyslexia*.\n",
    "\n",
    "#### Choose model\n",
    "\n",
    "Multiple model types can be trained. To pick a model, change the *model_name* variable to contain the desired model. The model options are: **encoder** / **transformer** / **resnet**. Of course, it is also possible to import and use other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 12:26:50.933284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-18 12:26:51.123594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-18 12:26:51.125777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-18 12:26:51.129690: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-18 12:26:51.130157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-18 12:26:51.131661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-18 12:26:51.133162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-18 12:26:52.844968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-18 12:26:52.847011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-18 12:26:52.847593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-18 12:26:52.848159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20639 MB memory:  -> device: 0, name: NVIDIA A10, pci bus id: 0000:00:05.0, compute capability: 8.6\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"epod_encoder_std_transfer_1\"\n",
    "n_channels = 26 # 32/26\n",
    "n_timesteps = 501 # 2049/501\n",
    "\n",
    "# Model dimensions\n",
    "x_dimension = (n_channels, n_timesteps)\n",
    "y_dimension = 1\n",
    "\n",
    "if \"transfer\" in model_name:\n",
    "    trained_model = \"ddp_encoder_age_std1_3\"\n",
    "    base_path = os.path.join(local_paths.models, trained_model)\n",
    "    path_model = os.path.join(base_path, \"model\")\n",
    "    path_weights = os.path.join(base_path, \"weights.h5\")\n",
    "    model = tf.keras.models.load_model(path_model)\n",
    "    model.load_weights(path_weights)\n",
    "    \n",
    "elif \"encoder\" in model_name:\n",
    "    model = encoder_model(x_dimension, y_dimension, final_activation=None)\n",
    "elif \"resnet\" in model_name:\n",
    "    model = resnet_model(x_dimension, y_dimension)\n",
    "else:\n",
    "    print(\"No model found. Add a model to the model name.\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train model\n",
    "\n",
    "The models, training history, and subset contents are saved in the folder *model_name* inside *local_paths.models*. \n",
    "\n",
    "Multiple hyperparameters can be adjusted:\n",
    "+ *epochs*: number of iterations over the dataset\n",
    "+ *learning_rate*: step size in optimizing the model parameters at each update\n",
    "\n",
    "*Adam* is used as an optimizer, since this optimizer performs well in a wide variety of cases. The loss of the optimizer is set to *MeanSquaredError*. This is a commonly used loss function in regression analysis where a 2x increase in error corresponds to a 4x increase in the loss.\n",
    "\n",
    "The model weights are saved when the validation loss is improved with the *ModelCheckpoint* callback. If however the validation loss is not improved, the learning rate is reduced with the *ReduceLROnPlateau* callback. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create model: epod_encoder_std_transfer_1\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 12:27:01.080486: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8600\n",
      "2022-11-18 12:27:03.922905: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-11-18 12:27:04.131664: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - ETA: 0s - loss: 7951.5981\n",
      "Epoch 1: val_loss improved from inf to 4521.36426, saving model to /home/fpauwels/eegyolk/floris_thesis_code/models/trained_models/epod_encoder_std_transfer_1/weights.h5\n",
      "64/64 [==============================] - 96s 1s/step - loss: 7951.5981 - val_loss: 4521.3643 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4671.2930\n",
      "Epoch 2: val_loss improved from 4521.36426 to 4256.85596, saving model to /home/fpauwels/eegyolk/floris_thesis_code/models/trained_models/epod_encoder_std_transfer_1/weights.h5\n",
      "64/64 [==============================] - 85s 1s/step - loss: 4671.2930 - val_loss: 4256.8560 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4793.7876\n",
      "Epoch 3: val_loss did not improve from 4256.85596\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "64/64 [==============================] - 86s 1s/step - loss: 4793.7876 - val_loss: 4261.7871 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4276.8442\n",
      "Epoch 4: val_loss improved from 4256.85596 to 4241.99023, saving model to /home/fpauwels/eegyolk/floris_thesis_code/models/trained_models/epod_encoder_std_transfer_1/weights.h5\n",
      "64/64 [==============================] - 89s 1s/step - loss: 4276.8442 - val_loss: 4241.9902 - lr: 9.0000e-04\n",
      "Epoch 5/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4546.9331\n",
      "Epoch 5: val_loss improved from 4241.99023 to 4173.69092, saving model to /home/fpauwels/eegyolk/floris_thesis_code/models/trained_models/epod_encoder_std_transfer_1/weights.h5\n",
      "64/64 [==============================] - 88s 1s/step - loss: 4546.9331 - val_loss: 4173.6909 - lr: 9.0000e-04\n",
      "Epoch 6/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4383.2368\n",
      "Epoch 6: val_loss improved from 4173.69092 to 3785.76025, saving model to /home/fpauwels/eegyolk/floris_thesis_code/models/trained_models/epod_encoder_std_transfer_1/weights.h5\n",
      "64/64 [==============================] - 88s 1s/step - loss: 4383.2368 - val_loss: 3785.7603 - lr: 9.0000e-04\n",
      "Epoch 7/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4497.6655\n",
      "Epoch 7: val_loss improved from 3785.76025 to 3775.77075, saving model to /home/fpauwels/eegyolk/floris_thesis_code/models/trained_models/epod_encoder_std_transfer_1/weights.h5\n",
      "64/64 [==============================] - 95s 1s/step - loss: 4497.6655 - val_loss: 3775.7708 - lr: 9.0000e-04\n",
      "Epoch 8/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4382.0693\n",
      "Epoch 8: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0008100000384729356.\n",
      "64/64 [==============================] - 94s 1s/step - loss: 4382.0693 - val_loss: 4070.3979 - lr: 9.0000e-04\n",
      "Epoch 9/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4365.8174\n",
      "Epoch 9: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0007290000503417104.\n",
      "64/64 [==============================] - 92s 1s/step - loss: 4365.8174 - val_loss: 4204.0723 - lr: 8.1000e-04\n",
      "Epoch 10/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4398.3018\n",
      "Epoch 10: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0006561000715009868.\n",
      "64/64 [==============================] - 94s 1s/step - loss: 4398.3018 - val_loss: 4068.7725 - lr: 7.2900e-04\n",
      "Epoch 11/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4466.8574\n",
      "Epoch 11: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005904900433961303.\n",
      "64/64 [==============================] - 87s 1s/step - loss: 4466.8574 - val_loss: 3846.7261 - lr: 6.5610e-04\n",
      "Epoch 12/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4399.9399\n",
      "Epoch 12: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005314410547725857.\n",
      "64/64 [==============================] - 96s 1s/step - loss: 4399.9399 - val_loss: 3908.1150 - lr: 5.9049e-04\n",
      "Epoch 13/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4282.6729\n",
      "Epoch 13: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00047829695977270604.\n",
      "64/64 [==============================] - 89s 1s/step - loss: 4282.6729 - val_loss: 3945.3635 - lr: 5.3144e-04\n",
      "Epoch 14/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4203.3838\n",
      "Epoch 14: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0004304672533180565.\n",
      "64/64 [==============================] - 94s 1s/step - loss: 4203.3838 - val_loss: 4139.5854 - lr: 4.7830e-04\n",
      "Epoch 15/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4134.6562\n",
      "Epoch 15: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00038742052274756136.\n",
      "64/64 [==============================] - 90s 1s/step - loss: 4134.6562 - val_loss: 4078.7632 - lr: 4.3047e-04\n",
      "Epoch 16/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4280.6938\n",
      "Epoch 16: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0003486784757114947.\n",
      "64/64 [==============================] - 90s 1s/step - loss: 4280.6938 - val_loss: 4129.6636 - lr: 3.8742e-04\n",
      "Epoch 17/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4252.8237\n",
      "Epoch 17: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.00031381062290165574.\n",
      "64/64 [==============================] - 89s 1s/step - loss: 4252.8237 - val_loss: 4008.1138 - lr: 3.4868e-04\n",
      "Epoch 18/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4226.9541\n",
      "Epoch 18: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0002824295632308349.\n",
      "64/64 [==============================] - 94s 1s/step - loss: 4226.9541 - val_loss: 3959.7524 - lr: 3.1381e-04\n",
      "Epoch 19/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4223.2339\n",
      "Epoch 19: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.00025418660952709616.\n",
      "64/64 [==============================] - 89s 1s/step - loss: 4223.2339 - val_loss: 4036.2295 - lr: 2.8243e-04\n",
      "Epoch 20/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4074.0225\n",
      "Epoch 20: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.00022876793809700757.\n",
      "64/64 [==============================] - 88s 1s/step - loss: 4074.0225 - val_loss: 4184.2700 - lr: 2.5419e-04\n",
      "Epoch 21/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4275.2183\n",
      "Epoch 21: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00020589114428730683.\n",
      "64/64 [==============================] - 86s 1s/step - loss: 4275.2183 - val_loss: 3932.0000 - lr: 2.2877e-04\n",
      "Epoch 22/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4314.9048\n",
      "Epoch 22: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.00018530203378759326.\n",
      "64/64 [==============================] - 95s 1s/step - loss: 4314.9048 - val_loss: 3960.4631 - lr: 2.0589e-04\n",
      "Epoch 23/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4159.2212\n",
      "Epoch 23: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.00016677183302817866.\n",
      "64/64 [==============================] - 90s 1s/step - loss: 4159.2212 - val_loss: 3883.3010 - lr: 1.8530e-04\n",
      "Epoch 24/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4137.3491\n",
      "Epoch 24: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.00015009464841568844.\n",
      "64/64 [==============================] - 91s 1s/step - loss: 4137.3491 - val_loss: 3935.8938 - lr: 1.6677e-04\n",
      "Epoch 25/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4111.9722\n",
      "Epoch 25: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0001350851875031367.\n",
      "64/64 [==============================] - 89s 1s/step - loss: 4111.9722 - val_loss: 4131.1426 - lr: 1.5009e-04\n",
      "Epoch 26/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4241.4907\n",
      "Epoch 26: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.00012157666351413355.\n",
      "64/64 [==============================] - 83s 1s/step - loss: 4241.4907 - val_loss: 4024.4431 - lr: 1.3509e-04\n",
      "Epoch 27/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4139.4224\n",
      "Epoch 27: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.00010941899454337544.\n",
      "64/64 [==============================] - 93s 1s/step - loss: 4139.4224 - val_loss: 4156.1821 - lr: 1.2158e-04\n",
      "Epoch 28/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4103.9268\n",
      "Epoch 28: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 9.847709443420172e-05.\n",
      "64/64 [==============================] - 97s 2s/step - loss: 4103.9268 - val_loss: 3956.5952 - lr: 1.0942e-04\n",
      "Epoch 29/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4192.4888\n",
      "Epoch 29: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 8.862938630045391e-05.\n",
      "64/64 [==============================] - 90s 1s/step - loss: 4192.4888 - val_loss: 3851.5364 - lr: 9.8477e-05\n",
      "Epoch 30/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4223.4199\n",
      "Epoch 30: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 7.976644701557234e-05.\n",
      "64/64 [==============================] - 98s 2s/step - loss: 4223.4199 - val_loss: 4045.8135 - lr: 8.8629e-05\n",
      "Epoch 31/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4169.9663\n",
      "Epoch 31: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 7.178980231401511e-05.\n",
      "64/64 [==============================] - 89s 1s/step - loss: 4169.9663 - val_loss: 4068.2874 - lr: 7.9766e-05\n",
      "Epoch 32/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4173.6396\n",
      "Epoch 32: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 6.461082011810504e-05.\n",
      "64/64 [==============================] - 95s 1s/step - loss: 4173.6396 - val_loss: 3886.1592 - lr: 7.1790e-05\n",
      "Epoch 33/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4157.3877\n",
      "Epoch 33: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 5.8149741380475466e-05.\n",
      "64/64 [==============================] - 88s 1s/step - loss: 4157.3877 - val_loss: 4183.0776 - lr: 6.4611e-05\n",
      "Epoch 34/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4133.0630\n",
      "Epoch 34: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 5.233476658759173e-05.\n",
      "64/64 [==============================] - 97s 2s/step - loss: 4133.0630 - val_loss: 3933.2642 - lr: 5.8150e-05\n",
      "Epoch 35/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 3967.9768\n",
      "Epoch 35: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 4.7101289601414466e-05.\n",
      "64/64 [==============================] - 96s 2s/step - loss: 3967.9768 - val_loss: 4026.9688 - lr: 5.2335e-05\n",
      "Epoch 36/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4045.5864\n",
      "Epoch 36: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 4.239116096869111e-05.\n",
      "64/64 [==============================] - 93s 1s/step - loss: 4045.5864 - val_loss: 3961.6863 - lr: 4.7101e-05\n",
      "Epoch 37/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4117.9272\n",
      "Epoch 37: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 3.815204618149437e-05.\n",
      "64/64 [==============================] - 96s 2s/step - loss: 4117.9272 - val_loss: 4010.2471 - lr: 4.2391e-05\n",
      "Epoch 38/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4087.5413\n",
      "Epoch 38: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 3.4336842873017304e-05.\n",
      "64/64 [==============================] - 94s 1s/step - loss: 4087.5413 - val_loss: 4111.0405 - lr: 3.8152e-05\n",
      "Epoch 39/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4116.5288\n",
      "Epoch 39: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 3.0903160222806036e-05.\n",
      "64/64 [==============================] - 100s 2s/step - loss: 4116.5288 - val_loss: 4033.5601 - lr: 3.4337e-05\n",
      "Epoch 40/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 3999.6130\n",
      "Epoch 40: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 2.7812844200525434e-05.\n",
      "64/64 [==============================] - 88s 1s/step - loss: 3999.6130 - val_loss: 4138.8872 - lr: 3.0903e-05\n",
      "Epoch 41/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4020.5442\n",
      "Epoch 41: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 2.5031560107890984e-05.\n",
      "64/64 [==============================] - 90s 1s/step - loss: 4020.5442 - val_loss: 4040.9583 - lr: 2.7813e-05\n",
      "Epoch 42/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4278.7520\n",
      "Epoch 42: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 2.2528404588229024e-05.\n",
      "64/64 [==============================] - 87s 1s/step - loss: 4278.7520 - val_loss: 3868.9546 - lr: 2.5032e-05\n",
      "Epoch 43/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4013.7041\n",
      "Epoch 43: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 2.0275563474569936e-05.\n",
      "64/64 [==============================] - 85s 1s/step - loss: 4013.7041 - val_loss: 4049.8779 - lr: 2.2528e-05\n",
      "Epoch 44/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4201.0127\n",
      "Epoch 44: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 1.8248007290821987e-05.\n",
      "64/64 [==============================] - 89s 1s/step - loss: 4201.0127 - val_loss: 4038.5283 - lr: 2.0276e-05\n",
      "Epoch 45/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4017.3508\n",
      "Epoch 45: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 1.6423206398030745e-05.\n",
      "64/64 [==============================] - 88s 1s/step - loss: 4017.3508 - val_loss: 4137.8994 - lr: 1.8248e-05\n",
      "Epoch 46/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4107.8501\n",
      "Epoch 46: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 1.4780885430809576e-05.\n",
      "64/64 [==============================] - 89s 1s/step - loss: 4107.8501 - val_loss: 3964.7666 - lr: 1.6423e-05\n",
      "Epoch 47/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4065.8091\n",
      "Epoch 47: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 1.3302796560310526e-05.\n",
      "64/64 [==============================] - 90s 1s/step - loss: 4065.8091 - val_loss: 3830.0586 - lr: 1.4781e-05\n",
      "Epoch 48/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4024.4131\n",
      "Epoch 48: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 1.1972517313552089e-05.\n",
      "64/64 [==============================] - 88s 1s/step - loss: 4024.4131 - val_loss: 4011.3752 - lr: 1.3303e-05\n",
      "Epoch 49/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4136.5669\n",
      "Epoch 49: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 1.077526558219688e-05.\n",
      "64/64 [==============================] - 85s 1s/step - loss: 4136.5669 - val_loss: 3870.2183 - lr: 1.1973e-05\n",
      "Epoch 50/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 4107.4053\n",
      "Epoch 50: val_loss did not improve from 3775.77075\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 9.697739187686238e-06.\n",
      "64/64 [==============================] - 85s 1s/step - loss: 4107.4053 - val_loss: 4127.9224 - lr: 1.0775e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/fpauwels/eegyolk/floris_thesis_code/models/trained_models/epod_encoder_std_transfer_1/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/fpauwels/eegyolk/floris_thesis_code/models/trained_models/epod_encoder_std_transfer_1/model/assets\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "epochs = 50\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Paths for saving model data\n",
    "base_dir = os.path.join(local_paths.models, model_name)\n",
    "model_dir = os.path.join(base_dir, \"model\")\n",
    "subsets_dir = os.path.join(base_dir, \"subsets\")\n",
    "path_history = os.path.join(base_dir, \"history.npy\")\n",
    "path_weights = os.path.join(base_dir, \"weights.h5\")\n",
    "\n",
    "if os.path.exists(model_dir):\n",
    "    print(f\"Model: '{model_name}' already exist. Delete the existing model first or rename this model.\")    \n",
    "else:\n",
    "    print(f\"Create model: {model_name}\")\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.mkdir(base_dir)\n",
    "    if not os.path.exists(subsets_dir):\n",
    "        os.mkdir(subsets_dir)\n",
    "\n",
    "    # Save train / test / validation sets for future testing\n",
    "    data_io.save_experiment_names(experiments_train, os.path.join(subsets_dir, \"train_set.txt\"))\n",
    "    data_io.save_experiment_names(experiments_test, os.path.join(subsets_dir, \"test_set.txt\"))\n",
    "    data_io.save_experiment_names(experiments_val, os.path.join(subsets_dir, \"validation_set.txt\"))\n",
    "\n",
    "    # Model configurations\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=MeanSquaredError())\n",
    "    checkpointer = ModelCheckpoint(filepath=path_weights, monitor='val_loss', verbose=1, save_weights_only=True, save_best_only=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=1, factor=0.9, verbose=1)\n",
    "    \n",
    "    # Fit model\n",
    "    history = model.fit(x=train_sequence, validation_data=val_sequence, epochs=epochs, callbacks=[checkpointer, reduce_lr])\n",
    "    \n",
    "    # Save model and training history\n",
    "    model.save(model_dir)\n",
    "    np.save(path_history, history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_helper.show_plot(x=range(len(history.history['loss'])), \n",
    "                         y=[history.history['loss'], history.history['val_loss']], \n",
    "                         legend=[\"loss\",\"validation loss\"], \n",
    "                         xlabel=\"epochs\", ylabel=\"validation loss (MSE)\", \n",
    "                         title=f\"Loss during training ({model_name})\",\n",
    "                         xlim=[0,20], ylim=[0,0.1])\n",
    "                         # xlim=[0,100], ylim=[0,150000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5f6ecf0357e95e30953d0cf08844b8b26fdbdf1f780a6e218131c917612a57e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
