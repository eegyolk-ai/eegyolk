{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can be used to create a 'reduced' data set. In this data set, we reduce the original DL data set by a factor 10. This was necessary to load the data set into memory on a external cluster which was used for hyperparameter search. The reduced data set was only used for hyperparameter search and NOT for training the final models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import sys, os, fnmatch, csv\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import zarr\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import PATH_RAW_DATA, PATH_METADATA, PATH_DATA_PROCESSED_DL, PATH_DATA_PROCESSED_DL_REDUCED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load preprocessed data (non-reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.04 s, sys: 346 ms, total: 4.38 s\n",
      "Wall time: 5.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load all the metadata\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Get all the files in the output folder\n",
    "file_names = os.listdir(PATH_DATA_PROCESSED_DL)\n",
    "\n",
    "# Step 2: Get the full paths of the files (without extensions)\n",
    "files = [os.path.splitext(os.path.join(PATH_DATA_PROCESSED_DL, file_name))[0] for file_name in fnmatch.filter(file_names, \"*.zarr\")]\n",
    "\n",
    "# Step 3: Load all the metadata\n",
    "frames = []\n",
    "\n",
    "for idx, feature_file in enumerate(files):\n",
    "    df_metadata = pd.read_csv(feature_file.replace(\"processed_raw_\", \"processed_metadata_\") + \".csv\")\n",
    "    frames.append(df_metadata)\n",
    "\n",
    "df_metadata = pd.concat(frames) \n",
    "\n",
    "# Step 4: Add missing age information based on the age group the subject is in\n",
    "df_metadata['age_months'].fillna(df_metadata['age_group'], inplace=True)\n",
    "df_metadata['age_days'].fillna(df_metadata['age_group']*30, inplace=True)\n",
    "df_metadata['age_years'].fillna(df_metadata['age_group']/12, inplace=True)\n",
    "\n",
    "# Step 5: List all the unique subject IDs\n",
    "subject_ids = sorted(list(set(df_metadata[\"code\"].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>cnt_path</th>\n",
       "      <th>cnt_file</th>\n",
       "      <th>age_group</th>\n",
       "      <th>age_days</th>\n",
       "      <th>age_months</th>\n",
       "      <th>age_years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>/Volumes/Seagate Expansion Drive/ePodium/Data/...</td>\n",
       "      <td>023_35_mc_mmn36</td>\n",
       "      <td>35</td>\n",
       "      <td>1052.0</td>\n",
       "      <td>35.066667</td>\n",
       "      <td>2.922222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>/Volumes/Seagate Expansion Drive/ePodium/Data/...</td>\n",
       "      <td>337_23_jc_mmn_36_wk</td>\n",
       "      <td>23</td>\n",
       "      <td>692.0</td>\n",
       "      <td>23.066667</td>\n",
       "      <td>1.922222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>456</td>\n",
       "      <td>/Volumes/Seagate Expansion Drive/ePodium/Data/...</td>\n",
       "      <td>456_23_md_mmn36_wk</td>\n",
       "      <td>23</td>\n",
       "      <td>691.0</td>\n",
       "      <td>23.033333</td>\n",
       "      <td>1.919444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>328</td>\n",
       "      <td>/Volumes/Seagate Expansion Drive/ePodium/Data/...</td>\n",
       "      <td>328_23_jc_mmn36_wk</td>\n",
       "      <td>23</td>\n",
       "      <td>699.0</td>\n",
       "      <td>23.300000</td>\n",
       "      <td>1.941667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>314</td>\n",
       "      <td>/Volumes/Seagate Expansion Drive/ePodium/Data/...</td>\n",
       "      <td>314_29_mmn_36_wk</td>\n",
       "      <td>29</td>\n",
       "      <td>877.0</td>\n",
       "      <td>29.233333</td>\n",
       "      <td>2.436111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>348</td>\n",
       "      <td>/Volumes/Seagate Expansion Drive/ePodium/Data/...</td>\n",
       "      <td>348_29_jc_mmn25_wk</td>\n",
       "      <td>29</td>\n",
       "      <td>858.0</td>\n",
       "      <td>28.600000</td>\n",
       "      <td>2.383333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>/Volumes/Seagate Expansion Drive/ePodium/Data/...</td>\n",
       "      <td>009_23_jc_mmn58</td>\n",
       "      <td>23</td>\n",
       "      <td>692.0</td>\n",
       "      <td>23.066667</td>\n",
       "      <td>1.922222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>751</td>\n",
       "      <td>/Volumes/Seagate Expansion Drive/ePodium/Data/...</td>\n",
       "      <td>751-452-29m-jr-mmn36</td>\n",
       "      <td>29</td>\n",
       "      <td>869.0</td>\n",
       "      <td>28.966667</td>\n",
       "      <td>2.413889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>348</td>\n",
       "      <td>/Volumes/Seagate Expansion Drive/ePodium/Data/...</td>\n",
       "      <td>348_17_jc_mmn25_wk</td>\n",
       "      <td>17</td>\n",
       "      <td>512.0</td>\n",
       "      <td>17.066667</td>\n",
       "      <td>1.422222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>/Volumes/Seagate Expansion Drive/ePodium/Data/...</td>\n",
       "      <td>033_23_mc_mmn25_slp</td>\n",
       "      <td>23</td>\n",
       "      <td>690.0</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.916667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2096 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    code                                           cnt_path  \\\n",
       "0     23  /Volumes/Seagate Expansion Drive/ePodium/Data/...   \n",
       "0    337  /Volumes/Seagate Expansion Drive/ePodium/Data/...   \n",
       "0    456  /Volumes/Seagate Expansion Drive/ePodium/Data/...   \n",
       "0    328  /Volumes/Seagate Expansion Drive/ePodium/Data/...   \n",
       "0    314  /Volumes/Seagate Expansion Drive/ePodium/Data/...   \n",
       "..   ...                                                ...   \n",
       "0    348  /Volumes/Seagate Expansion Drive/ePodium/Data/...   \n",
       "0      9  /Volumes/Seagate Expansion Drive/ePodium/Data/...   \n",
       "0    751  /Volumes/Seagate Expansion Drive/ePodium/Data/...   \n",
       "0    348  /Volumes/Seagate Expansion Drive/ePodium/Data/...   \n",
       "0     33  /Volumes/Seagate Expansion Drive/ePodium/Data/...   \n",
       "\n",
       "                cnt_file  age_group  age_days  age_months  age_years  \n",
       "0        023_35_mc_mmn36         35    1052.0   35.066667   2.922222  \n",
       "0    337_23_jc_mmn_36_wk         23     692.0   23.066667   1.922222  \n",
       "0     456_23_md_mmn36_wk         23     691.0   23.033333   1.919444  \n",
       "0     328_23_jc_mmn36_wk         23     699.0   23.300000   1.941667  \n",
       "0       314_29_mmn_36_wk         29     877.0   29.233333   2.436111  \n",
       "..                   ...        ...       ...         ...        ...  \n",
       "0     348_29_jc_mmn25_wk         29     858.0   28.600000   2.383333  \n",
       "0        009_23_jc_mmn58         23     692.0   23.066667   1.922222  \n",
       "0   751-452-29m-jr-mmn36         29     869.0   28.966667   2.413889  \n",
       "0     348_17_jc_mmn25_wk         17     512.0   17.066667   1.422222  \n",
       "0    033_23_mc_mmn25_slp         23     690.0   23.000000   1.916667  \n",
       "\n",
       "[2096 rows x 7 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from list.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "        \n",
    "def average_epochs():\n",
    "    IDs = sorted(list(set(df_metadata[\"code\"].tolist())))\n",
    "    # Step 1: Iterate over subjects\n",
    "    for ID in IDs:\n",
    "        \n",
    "        # Step 2: Find all files of a subject\n",
    "        df_temp = df_metadata[df_metadata['code'] == ID]\n",
    "    \n",
    "        # Step 3: Find all the age groups the subject was found in\n",
    "        ages_subject = sorted(list(set(df_temp['age_group'].tolist())))\n",
    "        \n",
    "        # Step 4: Loop over all the age groups the subject is in\n",
    "        for age_group in ages_subject:            \n",
    "            X_data = np.zeros((0, 30, 501))\n",
    "            X_averaged_subsets = np.zeros((0, 30, 501))\n",
    "            \n",
    "            # Step 5: Concatenate data of files in the same age group\n",
    "            for i, metadata_file in df_temp[df_temp['age_group'] == age_group].iterrows():\n",
    "                filename = os.path.join(PATH_DATA_PROCESSED_DL, 'processed_raw_' + metadata_file['cnt_file'] + '.zarr')\n",
    "                data_signal = zarr.open(os.path.join(filename), mode='r')\n",
    "                X_data = np.concatenate((X_data, data_signal), axis=0)\n",
    "            \n",
    "            np.random.shuffle(X_data) # Shuffle data for randomly picking epochs without replacement\n",
    "            \n",
    "            for subset in chunks(X_data, 10):\n",
    "                X_data_mean = np.mean(subset[:,:,:], axis=0) # Average all epochs in subset\n",
    "                X_data_mean = np.expand_dims(X_data_mean, axis=0)\n",
    "                X_averaged_subsets = np.concatenate((X_averaged_subsets, X_data_mean), axis=0)\n",
    "            \n",
    "            file_name = f\"{str(ID).zfill(3)}_{age_group}\"\n",
    "            zarr_name = file_name + \".zarr\"\n",
    "            csv_name = file_name + \".csv\"\n",
    "            \n",
    "            metadata_file['cnt_file'] = file_name\n",
    "            path_metadata = os.path.join(PATH_DATA_PROCESSED_DL_REDUCED, csv_name)\n",
    "            \n",
    "            pd.DataFrame(metadata_file).transpose().to_csv(path_metadata, sep=',', index=False, header=True)     \n",
    "            z_file =  zarr.open(os.path.join(PATH_DATA_PROCESSED_DL_REDUCED, zarr_name), \n",
    "                                mode='w', \n",
    "                                shape=X_averaged_subsets.shape, \n",
    "                                chunks=(1, X_averaged_subsets.shape[1], X_averaged_subsets.shape[2]))\n",
    "            z_file[:] = X_averaged_subsets\n",
    "        \n",
    "average_epochs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load preprocessed data (reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load all the metadata\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Get all the files in the output folder\n",
    "file_names = os.listdir(PATH_DATA_PROCESSED_DL_REDUCED)\n",
    "\n",
    "# Step 2: Get the full paths of the files (without extensions)\n",
    "files = [os.path.splitext(os.path.join(PATH_DATA_PROCESSED_DL_REDUCED, file_name))[0] for file_name in fnmatch.filter(file_names, \"*.zarr\")]\n",
    "\n",
    "# Step 3: Load all the metadata\n",
    "frames = []\n",
    "\n",
    "for idx, feature_file in enumerate(files):\n",
    "    df_metadata = pd.read_csv(feature_file + \".csv\")\n",
    "    frames.append(df_metadata)\n",
    "\n",
    "df_metadata = pd.concat(frames) \n",
    "\n",
    "# Step 4: Add missing age information based on the age group the subject is in\n",
    "df_metadata['age_months'].fillna(df_metadata['age_group'], inplace=True)\n",
    "df_metadata['age_days'].fillna(df_metadata['age_group']*30, inplace=True)\n",
    "df_metadata['age_years'].fillna(df_metadata['age_group']/12, inplace=True)\n",
    "\n",
    "# Step 5: List all the unique subject IDs\n",
    "subject_ids = sorted(list(set(df_metadata[\"code\"].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>cnt_path</th>\n",
       "      <th>cnt_file</th>\n",
       "      <th>age_group</th>\n",
       "      <th>age_days</th>\n",
       "      <th>age_months</th>\n",
       "      <th>age_years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>/Volumes/Seagate Expansion Drive/ePodium/Data/...</td>\n",
       "      <td>001_29</td>\n",
       "      <td>29</td>\n",
       "      <td>842.0</td>\n",
       "      <td>28.066667</td>\n",
       "      <td>2.338889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>/Volumes/Seagate Expansion Drive/ePodium/Data/...</td>\n",
       "      <td>001_23</td>\n",
       "      <td>23</td>\n",
       "      <td>691.0</td>\n",
       "      <td>23.033333</td>\n",
       "      <td>1.919444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>/Volumes/Seagate Expansion Drive/ePodium/Data/...</td>\n",
       "      <td>001_35</td>\n",
       "      <td>35</td>\n",
       "      <td>1047.0</td>\n",
       "      <td>34.900000</td>\n",
       "      <td>2.908333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>/Volumes/Seagate Expansion Drive/ePodium/Data/...</td>\n",
       "      <td>001_17</td>\n",
       "      <td>17</td>\n",
       "      <td>510.0</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>/Volumes/Seagate Expansion Drive/ePodium/Data/...</td>\n",
       "      <td>001_41</td>\n",
       "      <td>41</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>3.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>/Volumes/Seagate Expansion Drive/ePodium/Data/...</td>\n",
       "      <td>001_11</td>\n",
       "      <td>11</td>\n",
       "      <td>329.0</td>\n",
       "      <td>10.966667</td>\n",
       "      <td>0.913889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>/Volumes/Seagate Expansion Drive/ePodium/Data/...</td>\n",
       "      <td>001_47</td>\n",
       "      <td>47</td>\n",
       "      <td>1403.0</td>\n",
       "      <td>46.766667</td>\n",
       "      <td>3.897222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   code                                           cnt_path cnt_file  \\\n",
       "0     1  /Volumes/Seagate Expansion Drive/ePodium/Data/...   001_29   \n",
       "0     1  /Volumes/Seagate Expansion Drive/ePodium/Data/...   001_23   \n",
       "0     1  /Volumes/Seagate Expansion Drive/ePodium/Data/...   001_35   \n",
       "0     1  /Volumes/Seagate Expansion Drive/ePodium/Data/...   001_17   \n",
       "0     1  /Volumes/Seagate Expansion Drive/ePodium/Data/...   001_41   \n",
       "0     1  /Volumes/Seagate Expansion Drive/ePodium/Data/...   001_11   \n",
       "0     1  /Volumes/Seagate Expansion Drive/ePodium/Data/...   001_47   \n",
       "\n",
       "   age_group  age_days  age_months  age_years  \n",
       "0         29     842.0   28.066667   2.338889  \n",
       "0         23     691.0   23.033333   1.919444  \n",
       "0         35    1047.0   34.900000   2.908333  \n",
       "0         17     510.0   17.000000   1.416667  \n",
       "0         41    1230.0   41.000000   3.416667  \n",
       "0         11     329.0   10.966667   0.913889  \n",
       "0         47    1403.0   46.766667   3.897222  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
