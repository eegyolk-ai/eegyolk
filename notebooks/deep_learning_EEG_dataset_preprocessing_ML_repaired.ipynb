{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A notebook mofdified from the work of Bjorn in the new enviroment, with code updates...in progress March 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare EEG data for training of machine-learning models\n",
    "\n",
    "In this notebook, the preprocessing for machine learning purposes is done. Also, some exploration and visualization is done to better understand the data at hand.\n",
    "\n",
    "+ Import data.\n",
    "+ Apply filters (bandpass).\n",
    "+ Detect potential bad channels and replace them by interpolation.\n",
    "+ Detect potential bad epochs and remove them.\n",
    "+ Extract features\n",
    "+ Select features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages & links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\makeda\\anaconda3\\envs\\mne-march2\\lib\\site-packages\\mne\\fixes.py:321: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(scipy.__version__) >= '1.1':\n",
      "C:\\Users\\makeda\\anaconda3\\envs\\mne-march2\\lib\\site-packages\\mne\\fixes.py:1134: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(numba.__version__) < LooseVersion('0.40'):\n",
      "C:\\Users\\makeda\\anaconda3\\envs\\mne-march2\\lib\\site-packages\\mne\\fixes.py:1134: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(numba.__version__) < LooseVersion('0.40'):\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mne\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "%matplotlib inline\n",
    "# %matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import PATH_RAW_DATA, PATH_METADATA, PATH_DATA_PROCESSED_ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import helper_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search all *.cnt files from the Bjorn set and get paths, code, and age group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_names = {11: '11mnd mmn',\n",
    "             17: '17mnd mmn',\n",
    "             23: '23mnd mmn',\n",
    "             29: '29mnd mmn',\n",
    "             35: '35mnd mmn',\n",
    "             41: '41mnd mmn',\n",
    "             47: '47mnd mmn'}\n",
    "    \n",
    "df_list = []\n",
    "\n",
    "for age_group, directory in dir_names.items(): # Go into every age group folder        \n",
    "    dir_path = os.path.join('C:/Projects/EEG_explorer/Data', directory)\n",
    "    file_names = os.listdir(dir_path)\n",
    "    cnt_paths = [os.path.join(dir_path, file_name) for file_name in fnmatch.filter(file_names, \"*.cnt\")]\n",
    "    cnt_files = [os.path.basename(x)[:-4] for x in cnt_paths]\n",
    "    codes = [int(re.search(r'\\d+', x).group()) for x in cnt_files]\n",
    "    df = pd.DataFrame(list(zip(codes, cnt_paths, cnt_files)), columns=['code', 'cnt_path','cnt_file']) \n",
    "    \n",
    "    df['age_group'] = age_group\n",
    "    df_list.append(df)\n",
    "\n",
    "cnt_files = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>cnt_path</th>\n",
       "      <th>cnt_file</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>704</td>\n",
       "      <td>C:/Projects/EEG_explorer/Data\\47mnd mmn\\704-03...</td>\n",
       "      <td>704-032-47m-jr-mmn36</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>705</td>\n",
       "      <td>C:/Projects/EEG_explorer/Data\\47mnd mmn\\705-05...</td>\n",
       "      <td>705-050-47m-jr-mmn36</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>709</td>\n",
       "      <td>C:/Projects/EEG_explorer/Data\\47mnd mmn\\709-07...</td>\n",
       "      <td>709-078-47m-jr-mmn36</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>710</td>\n",
       "      <td>C:/Projects/EEG_explorer/Data\\47mnd mmn\\710-07...</td>\n",
       "      <td>710-078-47m-jr-mmn36</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>711</td>\n",
       "      <td>C:/Projects/EEG_explorer/Data\\47mnd mmn\\711-08...</td>\n",
       "      <td>711-085-47m-jr-mmn36</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    code                                           cnt_path  \\\n",
       "39   704  C:/Projects/EEG_explorer/Data\\47mnd mmn\\704-03...   \n",
       "40   705  C:/Projects/EEG_explorer/Data\\47mnd mmn\\705-05...   \n",
       "41   709  C:/Projects/EEG_explorer/Data\\47mnd mmn\\709-07...   \n",
       "42   710  C:/Projects/EEG_explorer/Data\\47mnd mmn\\710-07...   \n",
       "43   711  C:/Projects/EEG_explorer/Data\\47mnd mmn\\711-08...   \n",
       "\n",
       "                cnt_file  age_group  \n",
       "39  704-032-47m-jr-mmn36         47  \n",
       "40  705-050-47m-jr-mmn36         47  \n",
       "41  709-078-47m-jr-mmn36         47  \n",
       "42  710-078-47m-jr-mmn36         47  \n",
       "43  711-085-47m-jr-mmn36         47  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_files.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a hash to the cnt_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now do the new old files\n",
    "    \n",
    "df_list = []\n",
    "\n",
    "#for age_group, directory in dir_names.items(): # Go into every age group folder        \n",
    "dir_path = os.path.join(PATH_RAW_DATA)\n",
    "file_names = os.listdir(dir_path)\n",
    "cnt_paths = [os.path.join(dir_path, file_name) for file_name in fnmatch.filter(file_names, \"*.cnt\")]\n",
    "cnt_files = [os.path.basename(x)[:-4] for x in cnt_paths]\n",
    "codes = [int(re.search(r'\\d+', x).group()) for x in cnt_files]\n",
    "df2 = pd.DataFrame(list(zip(codes, cnt_paths, cnt_files)), columns=['code', 'cnt_path','cnt_file']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>cnt_path</th>\n",
       "      <th>cnt_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>C:/Projects\\EEG_explorer\\Data_Old\\015_thomas_m...</td>\n",
       "      <td>015_thomas_mmn36w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34</td>\n",
       "      <td>C:/Projects\\EEG_explorer\\Data_Old\\034_17_mc_mm...</td>\n",
       "      <td>034_17_mc_mmn36_wk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36</td>\n",
       "      <td>C:/Projects\\EEG_explorer\\Data_Old\\036_17_mc_mm...</td>\n",
       "      <td>036_17_mc_mmn36_wk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36</td>\n",
       "      <td>C:/Projects\\EEG_explorer\\Data_Old\\036_29_mc_mm...</td>\n",
       "      <td>036_29_mc_mmn36_wk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>119</td>\n",
       "      <td>C:/Projects\\EEG_explorer\\Data_Old\\119_17_jr_mm...</td>\n",
       "      <td>119_17_jr_mmn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>755</td>\n",
       "      <td>C:/Projects\\EEG_explorer\\Data_Old\\755-471-17m-...</td>\n",
       "      <td>755-471-17m-mr-mmn36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>755</td>\n",
       "      <td>C:/Projects\\EEG_explorer\\Data_Old\\755-471-29m-...</td>\n",
       "      <td>755-471-29m-mr-mmn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>757</td>\n",
       "      <td>C:/Projects\\EEG_explorer\\Data_Old\\757-487-17m-...</td>\n",
       "      <td>757-487-17m-jr-mmn36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>758</td>\n",
       "      <td>C:/Projects\\EEG_explorer\\Data_Old\\758-465-17m-...</td>\n",
       "      <td>758-465-17m-mr-mmn36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>758</td>\n",
       "      <td>C:/Projects\\EEG_explorer\\Data_Old\\758-465-29m-...</td>\n",
       "      <td>758-465-29m-mr-mmn36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     code                                           cnt_path  \\\n",
       "0      15  C:/Projects\\EEG_explorer\\Data_Old\\015_thomas_m...   \n",
       "1      34  C:/Projects\\EEG_explorer\\Data_Old\\034_17_mc_mm...   \n",
       "2      36  C:/Projects\\EEG_explorer\\Data_Old\\036_17_mc_mm...   \n",
       "3      36  C:/Projects\\EEG_explorer\\Data_Old\\036_29_mc_mm...   \n",
       "4     119  C:/Projects\\EEG_explorer\\Data_Old\\119_17_jr_mm...   \n",
       "..    ...                                                ...   \n",
       "187   755  C:/Projects\\EEG_explorer\\Data_Old\\755-471-17m-...   \n",
       "188   755  C:/Projects\\EEG_explorer\\Data_Old\\755-471-29m-...   \n",
       "189   757  C:/Projects\\EEG_explorer\\Data_Old\\757-487-17m-...   \n",
       "190   758  C:/Projects\\EEG_explorer\\Data_Old\\758-465-17m-...   \n",
       "191   758  C:/Projects\\EEG_explorer\\Data_Old\\758-465-29m-...   \n",
       "\n",
       "                 cnt_file  \n",
       "0       015_thomas_mmn36w  \n",
       "1      034_17_mc_mmn36_wk  \n",
       "2      036_17_mc_mmn36_wk  \n",
       "3      036_29_mc_mmn36_wk  \n",
       "4           119_17_jr_mmn  \n",
       "..                    ...  \n",
       "187  755-471-17m-mr-mmn36  \n",
       "188    755-471-29m-mr-mmn  \n",
       "189  757-487-17m-jr-mmn36  \n",
       "190  758-465-17m-mr-mmn36  \n",
       "191  758-465-29m-mr-mmn36  \n",
       "\n",
       "[192 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only cnt files where file is in the old set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['age_group'] = age_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.age_group.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list.append(df)\n",
    "\n",
    "cnt_files = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.basename('Projects\\\\EEG_explorer\\\\Data\\\\11mnd mmn\\\\001_11_jc_mmn36_wk_mmn25_wk_mmn47_wk_mmn58_wk')[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_files.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PATH_RAW_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_files.cnt_path.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_files = cnt_files.reset_index()\n",
    "cnt_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnt_files['old_index'] = cnt_files['index']\n",
    "# cnt_files\n",
    "\n",
    "# cnt_files = cnt_files.drop('index', axis=1, inplace=True)\n",
    "#drop('column_name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_files.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for all age files and create DataFrame containing all ages per subject code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "age_files = {11: 'ages_11mnths.txt',\n",
    "             17: 'ages_17mnths.txt',\n",
    "             23: 'ages_23mnths.txt',\n",
    "             29: 'ages_29mnths.txt',\n",
    "             35: 'ages_35mnths.txt',\n",
    "             41: 'ages_41mnths.txt',\n",
    "             47: 'ages_47mnths.txt'}\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for age_group, age_file in age_files.items():\n",
    "    df = pd.read_csv(os.path.join(PATH_METADATA, 'ages', age_file), sep=\"\\t\")\n",
    "    df['age_group'] = age_group\n",
    "    df_list.append(df)\n",
    "\n",
    "age_df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_df.code.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_files.code.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the .cnt files with the age information we have on the subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_files['cnt_file'] = cnt_files['cnt_file'].astype(str)\n",
    "cnt_files['cnt_path'] = cnt_files['cnt_path'].astype(str)\n",
    "#MergeDat['Motor'] = MergeDat['Motor'].astype(str)\n",
    "type(cnt_files.cnt_path[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(age_df, cnt_files, how='left', on=['age_group','code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merged_df), len(cnt_files['cnt_path']), len(age_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.age_months.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(~merged_df['cnt_path'].isna()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the age ranges within groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_11 = merged_df.loc[merged_df['age_group'] == 11]\n",
    "data_17 = merged_df.loc[merged_df['age_group'] == 17]\n",
    "data_23 = merged_df.loc[merged_df['age_group'] == 23]\n",
    "data_29 = merged_df.loc[merged_df['age_group'] == 29]\n",
    "data_35 = merged_df.loc[merged_df['age_group'] == 35]\n",
    "data_41 = merged_df.loc[merged_df['age_group'] == 41]\n",
    "data_47 = merged_df.loc[merged_df['age_group'] == 47]\n",
    "data = [data_11, data_17, data_23, data_29, data_35, data_41, data_47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "bins = 20\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(16,16))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i+1 > len(data):\n",
    "        ax.remove()\n",
    "    else:\n",
    "        ax.hist(data[i]['age_months'], bins=bins)\n",
    "        ax.set_xlabel('Age (months)')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.set_title(f'Frequency histogram, bins={bins}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(16,16))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i+1 > len(data):\n",
    "        ax.remove()\n",
    "    else:\n",
    "        sns.swarmplot(ax=ax, x=\"age_group\", y=\"age_months\", data=data[i])\n",
    "        ax.set_xlabel('Age group')\n",
    "        ax.set_ylabel('Age (months)')\n",
    "        ax.set_title('Chronological age vs. age group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(16,16))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i+1 > len(data):\n",
    "        ax.remove()\n",
    "    else:    \n",
    "        sns.boxplot(ax=ax, x=\"age_group\", y=\"age_months\", data=data[i], showmeans=True, \n",
    "                meanprops={\"marker\":\"o\",\n",
    "                           \"markerfacecolor\":\"white\", \n",
    "                           \"markeredgecolor\":\"black\",\n",
    "                           \"markersize\":\"6\"})\n",
    "        ax.set_xlabel('Age group')\n",
    "        ax.set_ylabel('Age (months)')\n",
    "        ax.set_title('Chronological age vs. age group')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check files with no label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.loc[merged_df['age_days'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in the missing age data based on the age group the subject is in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know the age group (i.e. 11, 17, 23, .. months etc) of all the subjects, based on the folder the files are in and based on the file name. We have got the exact ages (in days) of most subjects seperately, which we have added to the DataFrame above. For some of the subjects, we don't have the exact age and therefore we set this equal to the age group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['age_months'].fillna(merged_df['age_group'], inplace=True)\n",
    "merged_df['age_days'].fillna(merged_df['age_group']*30, inplace=True)\n",
    "merged_df['age_years'].fillna(merged_df['age_group']/12, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below should now return an empty dataframe, because all empty fields have been filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.loc[merged_df['age_days'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore EEG data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import EEG data (from .cnt files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_df.loc[50]\n",
    "merged_df['cnt_path'][50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a file\n",
    "file = merged_df['cnt_path'][50] # just a random file number\n",
    "\n",
    "# Import file \n",
    "data_raw = mne.io.read_raw_cnt(file, eog='auto', preload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data type: {}\\n\\n{}\\n'.format(type(data_raw), data_raw))\n",
    "\n",
    "# Get the sample rate\n",
    "print('Sample rate:', data_raw.info['sfreq'], 'Hz')\n",
    "\n",
    "# Get the size of the matrix\n",
    "print('Size of the matrix: {}\\n'.format(data_raw.get_data().shape))\n",
    "\n",
    "# The mne.info class can be used to learn more about the data.\n",
    "print(data_raw.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show data as pandas dataframe (i.e. as a table).\n",
    "The raw data itself is just an array dimensions are no. of channels and timepoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = data_raw.to_data_frame()\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.info['bads']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Band-pass filter (between 1 and 40 Hz. was 0.5 to 30Hz in Stober 2016)\n",
    "data_raw.filter(1, 40, fir_design='firwin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from the first 5 channels, from 1 s to 10 s.\n",
    "sfreq = data_raw.info['sfreq']\n",
    "data, times = data_raw[:5, int(sfreq * 1):int(sfreq * 10)]\n",
    "\n",
    "fig = plt.subplots(figsize=(10,8))\n",
    "plt.plot(times, data.T)\n",
    "plt.xlabel('Seconds')\n",
    "plt.ylabel('$\\mu V$')\n",
    "plt.title('Channels: 1-5')\n",
    "plt.legend(data_raw.ch_names[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mne plots\n",
    "There are many nice plotting options included in mne. They are, however, not always interactive and fully functional in Jupyter notebooks... so better try them out from a python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.plot(duration=10, block=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at events "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the events\n",
    "events, event_id = mne.events_from_annotations(data_raw)\n",
    "print(events[:10,:])\n",
    "print(event_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which unique event indentifiers there are\n",
    "unique_event_types = set(events[:,2])\n",
    "print(unique_event_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for most common event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin = -0.2\n",
    "tmax = 0.8\n",
    "\n",
    "baseline = (None, 0)  # means from the first instant to t = 0\n",
    "counts = pd.Series()\n",
    "\n",
    "for i, path in enumerate(merged_df['cnt_path']):        \n",
    "    # Import file \n",
    "    try:\n",
    "        raw = mne.io.read_raw_cnt(path, eog='auto', preload=True, verbose=False)\n",
    "    except: \n",
    "        continue\n",
    "    \n",
    "    # Load events\n",
    "    events_count, event_id_count = mne.events_from_annotations(raw, verbose=False)    \n",
    "    event_id_count = {y:x for x,y in event_id_count.items()}\n",
    "    \n",
    "    temp_df = pd.DataFrame(events_count) \n",
    "    temp_df[2].replace(event_id_count, inplace=True)\n",
    "    counts = counts.add(temp_df[2].value_counts(),fill_value=0)\n",
    "\n",
    "print(counts)\n",
    "\n",
    "# 0        3567.0\n",
    "# 1       13263.0\n",
    "# 10         18.0\n",
    "# 104         1.0\n",
    "# 11         12.0\n",
    "# 112         2.0\n",
    "# 12      44920.0\n",
    "# 127       304.0\n",
    "# 13     120790.0\n",
    "# 14       8649.0\n",
    "# 15       6600.0\n",
    "# 16         13.0\n",
    "# 17         13.0\n",
    "# 18         93.0\n",
    "# 19         75.0\n",
    "# 2      357282.0\n",
    "# 20         15.0\n",
    "# 201         2.0\n",
    "# 202         2.0\n",
    "# 203         2.0\n",
    "# 204         2.0\n",
    "# 205         2.0\n",
    "# 206         2.0\n",
    "# 208         2.0\n",
    "# 209         2.0\n",
    "# 21         10.0\n",
    "# 210         2.0\n",
    "# 211         2.0\n",
    "# 255         2.0\n",
    "# 26          1.0\n",
    "# 27          1.0\n",
    "# 3      878840.0\n",
    "# 31          1.0\n",
    "# 33        614.0\n",
    "# 35          1.0\n",
    "# 36          2.0\n",
    "# 4       72045.0\n",
    "# 40         12.0\n",
    "# 41          4.0\n",
    "# 45          1.0\n",
    "# 48         18.0\n",
    "# 49         10.0\n",
    "# 5       55476.0\n",
    "# 55      44602.0\n",
    "# 58          5.0\n",
    "# 6          12.0\n",
    "# 60          9.0\n",
    "# 65          5.0\n",
    "# 66      91848.0\n",
    "# 7          12.0\n",
    "# 72          4.0\n",
    "# 73          7.0\n",
    "# 77       8969.0\n",
    "# 8          13.0\n",
    "# 80         62.0\n",
    "# 81         31.0\n",
    "# 82          8.0\n",
    "# 88       6894.0\n",
    "# 9          12.0\n",
    "# 99      28749.0\n",
    "# dtype: float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display signal around one type of event\n",
    "Selects signal for specific event ID and plots time window from tmin to tmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event_id = [1, 2, 3, 4] # select events for the given event IDs\n",
    "tmin = -0.2  # start of each epoch (200ms before the trigger)\n",
    "tmax = 0.5  # end of each epoch (500ms after the trigger)\n",
    "\n",
    "baseline = (None, 0)  # means from the first instant to t = 0\n",
    "picks = mne.pick_types(data_raw.info, meg=False, eeg=True, stim=False, eog=False, exclude='bads')\n",
    "\n",
    "print(picks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = mne.Epochs(data_raw, events, event_id, tmin, tmax, proj=True, picks=picks,\n",
    "                    baseline=baseline, preload=True, verbose=True)\n",
    "\n",
    "print(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data in tabular structure as a pandas DataFrame.\n",
    "epochs_df = epochs.to_data_frame()\n",
    "epochs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evoked = epochs['5'].average()\n",
    "evoked.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial plot:\n",
    "evoked.plot_topomap(times=[0.1], size=3., title=\"Topo plot\", time_unit='s');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot topomaps for different time points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evoked.plot_topomap(times=np.array([0, 0.016, 0.030, 0.060, 0.070, 0.1, 0.2, 0.5]), time_unit='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test other event type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evoked = epochs['4'].average()\n",
    "evoked.plot()\n",
    "evoked.plot_topomap(times=[0.1], size=3., title=\"Topo plot\", time_unit='s');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evoked.plot_topomap(times=np.array([0, 0.016, 0.030, 0.060, 0.070, 0.1, 0.2, 0.5]), time_unit='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a montage to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montages specify the exact electrode placement on the scalp of the subject. This contains coordinates relative to a point on the scalp. Often this data is included in the EEG data (.cnt file). Unfortunately for us, we don't have this information. The electrode placement information can be used to fix broken channels by using the channels surrounding this channel. Even though we don't have the exact locations, we do know the electrode placement system used: 10-20. We can use this to approximate the locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When looking at the maps above, the electrode placement seems to be incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax2d = fig.add_subplot(121)\n",
    "ax3d = fig.add_subplot(122, projection='3d')\n",
    "data_raw.plot_sensors(ch_type='eeg', axes=ax2d)\n",
    "data_raw.plot_sensors(ch_type='eeg', axes=ax3d, kind='3d')\n",
    "ax3d.view_init(azim=70, elev=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_from_raw = mne.channels.make_eeg_layout(data_raw.info)\n",
    "layout_from_raw.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unfortunately, we don't have the exact sensor locations. Therefore, we try to approximate them with a standard montage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard montages come with the mne package. They're based on well known and often used electrode placement systems (10-20 in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_raw.ch_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_raw.ch_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "montage = mne.channels.make_standard_montage('standard_1020')\n",
    "montage.ch_names = [ch_name.upper() for ch_name in montage.ch_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "montage.plot(kind='topomap', show_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw_1020 = data_raw.copy().set_montage(montage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, after setting the 1020 montage, the maps look different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax2d = fig.add_subplot(121)\n",
    "ax3d = fig.add_subplot(122, projection='3d')\n",
    "data_raw_1020.plot_sensors(ch_type='eeg', axes=ax2d)\n",
    "data_raw_1020.plot_sensors(ch_type='eeg', axes=ax3d, kind='3d')\n",
    "ax3d.view_init(azim=70, elev=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_from_raw = mne.channels.make_eeg_layout(data_raw_1020.info)\n",
    "layout_from_raw.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom cnt-file import function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cnt_file(file,\n",
    "                  label_group,\n",
    "                  event_idx = [2, 3, 4, 5, 12, 13, 14, 15],\n",
    "                  channel_set = \"30\",\n",
    "                  tmin = -0.2,\n",
    "                  tmax = 0.8,\n",
    "                  lpass = 0.5, \n",
    "                  hpass = 40, \n",
    "                  threshold = 5, \n",
    "                  max_bad_fraction = 0.2,\n",
    "                  max_bad_channels = 2):\n",
    "    \"\"\" Function to read cnt file. Run bandpass filter. \n",
    "    Then detect and correct/remove bad channels and bad epochs.\n",
    "    Store resulting epochs as arrays.\n",
    "    \n",
    "    Args:\n",
    "    --------\n",
    "    file: str\n",
    "        Name of file to import.\n",
    "    label_group: int\n",
    "        Unique ID of specific group (must be >0).\n",
    "    channel_set: str\n",
    "        Select among pre-defined channel sets. Here: \"30\" or \"62\"\n",
    "    \"\"\"\n",
    "    \n",
    "    if channel_set == \"30\":\n",
    "        channel_set = ['O2', 'O1', 'OZ', 'PZ', 'P4', 'CP4', 'P8', 'C4', 'TP8', 'T8', 'P7', \n",
    "                       'P3', 'CP3', 'CPZ', 'CZ', 'FC4', 'FT8', 'TP7', 'C3', 'FCZ', 'FZ', \n",
    "                       'F4', 'F8', 'T7', 'FT7', 'FC3', 'F3', 'FP2', 'F7', 'FP1']\n",
    "    elif channel_set == \"62\":\n",
    "        channel_set = ['O2', 'O1', 'OZ', 'PZ', 'P4', 'CP4', 'P8', 'C4', 'TP8', 'T8', 'P7', \n",
    "                       'P3', 'CP3', 'CPZ', 'CZ', 'FC4', 'FT8', 'TP7', 'C3', 'FCZ', 'FZ', \n",
    "                       'F4', 'F8', 'T7', 'FT7', 'FC3', 'F3', 'FP2', 'F7', 'FP1', 'AFZ', 'PO3', \n",
    "                       'P1', 'POZ', 'P2', 'PO4', 'CP2', 'P6', 'M1', 'CP6', 'C6', 'PO8', 'PO7', \n",
    "                       'P5', 'CP5', 'CP1', 'C1', 'C2', 'FC2', 'FC6', 'C5', 'FC1', 'F2', 'F6', \n",
    "                       'FC5', 'F1', 'AF4', 'AF8', 'F5', 'AF7', 'AF3', 'FPZ']\n",
    "    else:\n",
    "        print(\"Predefined channel set given by 'channel_set' not known...\")\n",
    "        \n",
    "    \n",
    "    # Initialize array\n",
    "    signal_collection = np.zeros((0,len(channel_set),501))\n",
    "    label_collection = [] #np.zeros((0))\n",
    "    channel_names_collection = []\n",
    "    \n",
    "    # Import file\n",
    "    try:\n",
    "        data_raw = mne.io.read_raw_cnt(file, eog='auto', preload=True, verbose=False)\n",
    "    except ValueError:\n",
    "        print(\"ValueError\")\n",
    "        print(\"Could not load file:\", file)\n",
    "        return None, None, None\n",
    "    \n",
    "    # Band-pass filter (between 0.5 and 40 Hz. was 0.5 to 30Hz in Stober 2016)\n",
    "    data_raw.filter(0.5, 40, fir_design='firwin')\n",
    "\n",
    "    # Get events from annotations in the data\n",
    "    events_from_annot, event_dict = mne.events_from_annotations(data_raw)\n",
    "    \n",
    "    # Set baseline:\n",
    "    baseline = (None, 0)  # means from the first instant to t = 0\n",
    "\n",
    "    # Select channels to exclude (if any)\n",
    "    channels_exclude = [x for x in data_raw.ch_names if x not in channel_set]\n",
    "    channels_exclude = [x for x in channels_exclude if x not in ['HEOG', 'VEOG']]\n",
    "    \n",
    "    for event_id in event_idx:\n",
    "        if str(event_id) in event_dict:\n",
    "            # Pick EEG channels\n",
    "            picks = mne.pick_types(data_raw.info, meg=False, eeg=True, stim=False, eog=False,\n",
    "                               #exclude=data_exclude)#'bads'])\n",
    "                                   include=channel_set, exclude=channels_exclude)#'bads'])\n",
    "\n",
    "            epochs = mne.Epochs(data_raw, events=events_from_annot, event_id=event_dict,\n",
    "                                tmin=tmin, tmax=tmax, proj=True, picks=picks,\n",
    "                                baseline=baseline, preload=True, event_repeated='merge', verbose=False)\n",
    "\n",
    "            # Detect potential bad channels and epochs\n",
    "            bad_channels, bad_epochs = helper_functions.select_bad_epochs(epochs,\n",
    "                                                                          event_id,\n",
    "                                                                          threshold = threshold,\n",
    "                                                                          max_bad_fraction = max_bad_fraction)\n",
    "\n",
    "            # Interpolate bad channels\n",
    "            # ------------------------------------------------------------------\n",
    "            if len(bad_channels) > 0:\n",
    "                if len(bad_channels) > max_bad_channels:\n",
    "                    print(20*'--')\n",
    "                    print(\"Found too many bad channels (\" + str(len(bad_channels)) + \")\")\n",
    "                    return None, None, None\n",
    "                else:\n",
    "                    # MARK: Setting the montage is not verified yet (choice of standard montage)\n",
    "                    montage = mne.channels.make_standard_montage('standard_1020')\n",
    "                    montage.ch_names = [ch_name.upper() for ch_name in montage.ch_names]\n",
    "                    data_raw.set_montage(montage)\n",
    "                    \n",
    "                    # TODO: Think about using all channels before removing (62 -> 30), to enable for better interpolation\n",
    "                    \n",
    "                    # Mark bad channels:\n",
    "                    data_raw.info['bads'] = bad_channels\n",
    "                    # Pick EEG channels:\n",
    "                    picks = mne.pick_types(data_raw.info, meg=False, eeg=True, stim=False, eog=False,\n",
    "                                       #exclude=data_exclude)#'bads'])\n",
    "                                       include=channel_set, exclude=channels_exclude)#'bads'])\n",
    "                    epochs = mne.Epochs(data_raw, events=events_from_annot, event_id=event_dict,\n",
    "                                        tmin=tmin, tmax=tmax, proj=True, picks=picks,\n",
    "                                        baseline=baseline, preload=True, verbose=False)\n",
    "                    \n",
    "                    # Interpolate bad channels using functionality of 'mne'\n",
    "                    epochs.interpolate_bads()\n",
    "                    \n",
    "\n",
    "            # Get signals as array and add to total collection\n",
    "            channel_names_collection.append(epochs.ch_names)\n",
    "            signals_cleaned = epochs[str(event_id)].drop(bad_epochs).get_data()\n",
    "            signal_collection = np.concatenate((signal_collection, signals_cleaned), axis=0)\n",
    "            label_collection += [event_id + label_group] * signals_cleaned.shape[0]\n",
    "\n",
    "    return signal_collection, label_collection, channel_names_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_collect, label_collect, channel_names_collection = read_cnt_file(merged_df['cnt_path'][20], \n",
    "                                                                        merged_df['age_months'][0],\n",
    "                                                                        event_idx = [2, 3, 4, 5, 12, 13, 14, 15],\n",
    "                                                                        channel_set = \"30\",\n",
    "                                                                        tmin = -0.2,\n",
    "                                                                        tmax = 0.8,\n",
    "                                                                        lpass = 0.5, \n",
    "                                                                        hpass = 40, \n",
    "                                                                        threshold = 5, \n",
    "                                                                        max_bad_fraction = 0.2,\n",
    "                                                                        max_bad_channels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_collect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label_collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs investigation\n",
    "signal_collection.shape, label_collection.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs investigation- not defined anywhere\n",
    "metadata_collection[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction and save processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine how to store the processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we're determining what the best method is to extract and save the features. At the end, we combine all the parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, file in merged_df.head(10).iterrows():\n",
    "    \n",
    "    # Import data and labels\n",
    "    signal_collect, label_collect, ch_names = read_cnt_file(file['cnt_path'],\n",
    "                                                            file['age_months'],\n",
    "                                                            event_idx = [2, 3, 4, 5, 12, 13, 14, 15],\n",
    "                                                            channel_set = \"30\",\n",
    "                                                            tmin = -0.2,\n",
    "                                                            tmax = 0.8,\n",
    "                                                            lpass = 0.5, \n",
    "                                                            hpass = 40, \n",
    "                                                            threshold = 5, \n",
    "                                                            max_bad_fraction = 0.2,\n",
    "                                                            max_bad_channels = 2)    \n",
    "        \n",
    "    # Save data and labels\n",
    "    # ---------------------------------------------------------\n",
    "    if signal_collect is None:\n",
    "        continue\n",
    "\n",
    "    # Firstly, save the metadata which will be the same for all files derived from this .cnt file\n",
    "    filename_metadata = os.path.join(PATH_DATA_PROCESSED_ML, \"processed_data_\" + file['cnt_file'] + \".csv\")\n",
    "    pd.DataFrame(file).transpose().to_csv(filename_metadata, sep=',', index=False, header=True)        \n",
    "    \n",
    "    # Save all the epochs to separate files\n",
    "    filename_signal = os.path.join(PATH_DATA_PROCESSED_ML, \"processed_data_\" + file['cnt_file'] + \".npy\")\n",
    "    np.save(filename_signal, signal_collect)\n",
    "    \n",
    "    break # MARK: Remove this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features that can be used for machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne_features.feature_extraction import extract_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features from the raw data to reduce dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User defined functions\n",
    "\n",
    "def compute_rms(data):\n",
    "    \"\"\"Root-mean squared value of the data (per channel).\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : ndarray, shape (n_channels, n_times)\n",
    "    Returns\n",
    "    -------\n",
    "    output : ndarray, shape (n_channels,)\n",
    "    Notes\n",
    "    -----\n",
    "    Alias of the feature function: *rms*\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean(np.power(data, 2), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import mne_features\n",
    "# Select features from the raw data for machine learning\n",
    "selected_features = {'mean', ('root_mean_squared', compute_rms), 'hjorth_mobility', 'hjorth_complexity', 'variance', 'std', 'kurtosis', 'skewness', 'app_entropy', 'zero_crossings', 'energy_freq_bands', 'spect_edge_freq', 'ptp_amp'}\n",
    "\n",
    "X_new = extract_features(signal_collect, 500.0, selected_features, return_as_df=1)\n",
    "# pro\n",
    "#mne_features.feature_extraction.extract_features(X, sfreq, selected_funcs, funcs_params=None, n_jobs=1, ch_names=None, return_as_df=Fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data had a shape of (1917, 30, 501) - the extracted features data is almost 30 times smaller (before feature selection):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for highly correlated features and remove one of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features are often highly correlated and therefore don't add a lot of additional information to the model. To further reduce dimensionality, one of the two highly correlated features can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_1 = X_new.iloc[:, X_new.columns.get_level_values(1)=='1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the first channel for all features\n",
    "\n",
    "X_new_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation = X_new_1.corr()\n",
    "correlation = X_new.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation.ge(0.9)\n",
    "\n",
    "\"\"\"\n",
    "Highly correlated (>0.90), channel 0:\n",
    "- std & rms (0.926)\n",
    "- std & ptp_amp (0.9688)\n",
    "- std & variance (0.952)\n",
    "- ptp_amp & variance (0.900)\n",
    "\n",
    "Highly correlated (>0.90), channel 1:\n",
    "std & rms\n",
    "std & ptp_amp\n",
    "std & variance\n",
    "rms & ptp_amp\n",
    "ptp_amp & variance\n",
    "\n",
    "Highly correlated (>0.90), channel 2:\n",
    "std & rms\n",
    "std & ptp_amp\n",
    "std & variance\n",
    "rms & ptp_amp\n",
    "ptp_amp & variance\n",
    "\n",
    "Highly correlated (>0.90), channel 3:\n",
    "std & rms\n",
    "std & ptp_amp\n",
    "std & variance\n",
    "ptp_amp & variance\n",
    "\n",
    "Highly correlated (>0.90), channel 3:\n",
    "std & rms\n",
    "std & ptp_amp\n",
    "std & variance\n",
    "ptp_amp & variance\n",
    "\n",
    "Highly correlated (>0.90), channel 4:\n",
    "std & rms\n",
    "std & ptp_amp\n",
    "std & variance\n",
    "ptp_amp & variance\n",
    "\n",
    "Highly correlated (>0.90), channel 5:\n",
    "std & rms\n",
    "std & ptp_amp\n",
    "std & variance\n",
    "rms & ptp_amp\n",
    "ptp_amp & variance\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inspecting a few channels and the correlation between the features, the features 'std' and 'ptp_amp' can be removed, because they have a high correlation with eachother, 'rms' and 'variance'. Removing these two features will reduce the dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction after selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed features: std, ptp_amp\n",
    "selected_features_selection = {'mean', ('root_mean_squared', compute_rms), 'hjorth_mobility', 'hjorth_complexity', 'variance', 'kurtosis', 'skewness', 'app_entropy', 'zero_crossings', 'energy_freq_bands', 'spect_edge_freq'}\n",
    "\n",
    "X_new_selection = extract_features(signal_collect, 500.0, selected_features_selection, return_as_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_new_selection.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different channels aren't identifiable by the current naming method. Map the numbers to the actual channel name and flatten the MultiIndex column dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the column names\n",
    "\n",
    "import re\n",
    "\n",
    "cols = []\n",
    "\n",
    "for i, j in X_new_selection.columns:\n",
    "    try:\n",
    "        cols.append(f'{i}_{channel_names_collection[0][int(j)]}')\n",
    "    except ValueError:\n",
    "        # energy_freq_bands features\n",
    "        str_parts = j.split('_')        \n",
    "        ch_num = re.findall(\"\\d+\", str_parts[0])[0]\n",
    "        cols.append(f'{i}_{str_parts[1]}_{channel_names_collection[0][int(ch_num)]}')\n",
    "        \n",
    "X_new_selection.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in X_new_selection.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting the feature extraction, selection and saving steps together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "1. Check if files don't already exist\n",
    "2. Load EEG data\n",
    "3. Extract features\n",
    "4. Save metadata, save features dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from itertools import islice\n",
    "\n",
    "# TODO: Check the band ranges used\n",
    "selected_features = {'mean', ('root_mean_squared', compute_rms), 'hjorth_mobility', 'hjorth_complexity', 'variance', 'kurtosis', 'skewness', 'app_entropy', 'zero_crossings', 'energy_freq_bands', 'spect_edge_freq'}\n",
    "\n",
    "for i, file in islice(merged_df.iterrows(), 0, None): # Enables you to set starting row of DataFrame\n",
    "    print(f\"Checking out file: {file['cnt_file']}\")\n",
    "        \n",
    "    filename_extracted_features = os.path.join(PATH_DATA_PROCESSED_ML, \"extracted_features_\" + file['cnt_file'] + \".h5\")\n",
    "    filename_metadata = os.path.join(PATH_DATA_PROCESSED_ML, \"processed_data_\" + file['cnt_file'] + \".csv\")\n",
    "    \n",
    "    # Step 1: Check if file don't already exist, else skip\n",
    "    if os.path.exists(filename_extracted_features) and os.path.exists(filename_metadata):\n",
    "        print(f\"Skipping because {filename_extracted_features} and {filename_metadata} already exist.\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    # Step 2: Import data and labels \n",
    "    signal_collect, label_collect, ch_names = read_cnt_file(file['cnt_path'],\n",
    "                                                            file['age_months'],\n",
    "                                                            event_idx = [2, 3, 4, 5, 12, 13, 14, 15],\n",
    "                                                            channel_set = \"30\",\n",
    "                                                            tmin = -0.2,\n",
    "                                                            tmax = 0.8,\n",
    "                                                            lpass = 0.5, \n",
    "                                                            hpass = 40, \n",
    "                                                            threshold = 5, \n",
    "                                                            max_bad_fraction = 0.2,\n",
    "                                                            max_bad_channels = 2)    \n",
    "    \n",
    "    if signal_collect is None:\n",
    "        continue\n",
    "    \n",
    "    # Step 3: Extract the features from the raw data\n",
    "    try:\n",
    "        extracted_features = extract_features(signal_collect, 500.0, selected_features, return_as_df=1)\n",
    "    except:\n",
    "        print(f\"Skipping because {file['cnt_file']} causes error when extracting features.\")\n",
    "        continue\n",
    "    \n",
    "    # Change the DataFrame from hierarchical column structure ('mean' -> ch1, ch2, ch3) \n",
    "    # to flat column structure ('mean-ch1', 'mean-ch2', etc..)\n",
    "    col_names = []\n",
    "\n",
    "    for i, j in extracted_features.columns:\n",
    "        try:\n",
    "            col_names.append(f'{i}_{channel_names_collection[0][int(j)]}')\n",
    "        except ValueError:\n",
    "            # energy_freq_bands feature has different naming\n",
    "            str_parts = j.split('_')        \n",
    "            ch_num = re.findall(\"\\d+\", str_parts[0])[0]\n",
    "            col_names.append(f'{i}_{str_parts[1]}_{channel_names_collection[0][int(ch_num)]}')\n",
    "        \n",
    "    extracted_features.columns = col_names\n",
    "    \n",
    "    # Step 4: Save metadata (same for all files derived from this .cnt file), save the extracted features\n",
    "    pd.DataFrame(file).transpose().to_csv(filename_metadata, sep=',', index=False, header=True)     \n",
    "    extracted_features.to_hdf(filename_extracted_features, key='df', mode='w')\n",
    "    # TODO: 1. Try saving 2. Retry after n seconds if fails 3. If fails m times, continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check: test loading the saved file from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_load = pd.read_hdf(os.path.join(PATH_DATA_PROCESSED_ML,\"extracted_features_009_11_jc_mmn36_wk_mmn25_wk.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
